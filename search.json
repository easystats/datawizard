[{"path":[]},{"path":"/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement patilindrajeet.science@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.0, available https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines inspired Mozilla’s code conduct enforcement ladder. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to datawizard","title":"Contributing to datawizard","text":"outlines propose change datawizard.","code":""},{"path":"/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to datawizard","text":"Small typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. want fix typos documentation, please edit related .R file R/ folder. edit .Rd file man/.","code":""},{"path":"/CONTRIBUTING.html","id":"filing-an-issue","dir":"","previous_headings":"","what":"Filing an issue","title":"Contributing to datawizard","text":"easiest way propose change new feature file issue. ’ve found bug, may also create associated issue. possible, try illustrate proposal bug minimal reproducible example.","code":""},{"path":"/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"","what":"Pull requests","title":"Contributing to datawizard","text":"Please create Git branch pull request (PR). contributed code roughly follow R style guide, particular easystats convention code-style. datawizard uses roxygen2, Markdown syntax, documentation. datawizard uses testthat. Adding tests PR makes easier merge PR code base. PR user-visible change, may add bullet top NEWS.md describing changes made. may optionally add GitHub username, links relevant issue(s)/PR(s).","code":""},{"path":"/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to datawizard","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"/articles/demean.html","id":"sample-data-used-in-this-vignette","dir":"Articles","previous_headings":"","what":"Sample data used in this vignette","title":"Analysing Longitudinal or Panel Data","text":"Variables: QoL : Response (quality life patient) phq4 : Patient Health Questionnaire, time-varying variable hospital : Location treatment, time-invariant variable, co-variate education: Educational level, time-invariant variable, co-variate ID : patient ID time : time-point measurement","code":"library(parameters) data(\"qol_cancer\")"},{"path":"/articles/demean.html","id":"heterogeneity-bias","dir":"Articles","previous_headings":"","what":"Heterogeneity bias","title":"Analysing Longitudinal or Panel Data","text":"Heterogeneity bias occurs group-level predictors vary within across groups, hence fixed effects may correlate group (random) effects. typical situation analyzing longitudinal panel data: Due repeated measurements persons, “person” (subject-ID) now level-2 variable. Predictors level-1 (“fixed effects”), e.g. self-rated health income, now effect level-1 (“within”-effect) higher-level units (level-2, subject-level, “”-effect) (see also posting). inevitably leads correlating fixed effects error terms - , turn, results biased estimates, within- -effect captured one estimate. can check model may suffer heterogeneity bias using check_heterogeneity_bias() function:","code":"library(performance) check_heterogeneity_bias(qol_cancer, select = c(\"phq4\", \"education\"), group = \"ID\") #> Possible heterogeneity bias due to following predictors: phq4"},{"path":"/articles/demean.html","id":"adressing-heterogeneity-bias-the-fixed-effects-regression-fe-approach","dir":"Articles","previous_headings":"","what":"Adressing heterogeneity bias: the Fixed Effects Regression (FE) approach","title":"Analysing Longitudinal or Panel Data","text":"Fixed effects regression models (FE) popular approach panel data analysis particular econometrics considered gold standard. avoid problem heterogeneity bias, FE higher-level variance (thus, -effects), “controlled using higher-level entities , included model dummy variables” (Bell Jones 2015). consequence, FE models able estimate within-effects. remove -effects model within-effects, data needs preparation: de-meaning. De-meaning, person-mean centering, centering within clusters, takes away higher-level mean regression equation, , FE avoids estimating parameter higher-level unit.","code":""},{"path":"/articles/demean.html","id":"computing-the-de-meaned-and-group-meaned-variables","dir":"Articles","previous_headings":"Adressing heterogeneity bias: the Fixed Effects Regression (FE) approach","what":"Computing the de-meaned and group-meaned variables","title":"Analysing Longitudinal or Panel Data","text":"Now : phq4_between: time-varying variable mean phq4 across time-points, patient (ID). phq4_within: de-meaned time-varying variable phq4. FE model classical linear model, Intercept removed time-invariant predictors allowed included group-level factor included predictor time-varying predictors de-meaned (“person-mean centered”, indicating “within-subject” effect) can see, within-effect PHQ-4 -3.66, hence mean change average individual case sample (, “net” effect), -3.66. -effect? people higher PHQ-4 score differ people lower PHQ-4 score? educational inequalities? higher educated people higher PHQ-4 score lower educated people? question answered FE regression. : “Can one fit multilevel model varying intercepts (coefficients) units predictors correlate? answer yes. solution simple.” (Bafumi Gelman 2006)","code":"qol_cancer <- cbind(   qol_cancer,   datawizard::demean(qol_cancer, select = c(\"phq4\", \"QoL\"), group = \"ID\") ) fe_model1 <- lm(   QoL ~ 0 + time + phq4_within + ID,   data = qol_cancer ) # we use only the first two rows, because the remaining rows are # the estimates for \"ID\", which is not of interest here... model_parameters(fe_model1)[1:2, ] #> Parameter   | Coefficient |   SE |         95% CI | t(374) |      p #> ------------------------------------------------------------------- #> time        |        1.09 | 0.64 | [-0.17,  2.34] |   1.70 | 0.089  #> phq4 within |       -3.66 | 0.41 | [-4.46, -2.86] |  -8.95 | < .001   # instead of removing the intercept, we could also use the # de-meaned response... fe_model2 <- lm(   QoL_within ~ time + phq4_within + ID,   data = qol_cancer ) model_parameters(fe_model2)[2:3, ] #> Parameter   | Coefficient |   SE |         95% CI | t(374) |      p #> ------------------------------------------------------------------- #> time        |        1.09 | 0.64 | [-0.17,  2.34] |   1.70 | 0.089  #> phq4 within |       -3.66 | 0.41 | [-4.46, -2.86] |  -8.95 | < .001  # we compare the results with those from the \"lfe\"-package for panel data library(lfe) fe_model3 <- felm(   QoL ~ time + phq4 | ID,    data = qol_cancer ) model_parameters(fe_model3) #> # Fixed Effects #>  #> Parameter | Coefficient |   SE |         95% CI | t(374) |      p #> ----------------------------------------------------------------- #> time      |        1.09 | 0.64 | [-0.17,  2.34] |   1.70 | 0.089  #> phq4      |       -3.66 | 0.41 | [-4.46, -2.86] |  -8.95 | < .001"},{"path":"/articles/demean.html","id":"adressing-heterogeneity-bias-the-mixed-model-approach","dir":"Articles","previous_headings":"","what":"Adressing heterogeneity bias: the Mixed Model approach","title":"Analysing Longitudinal or Panel Data","text":"Mixed models include different levels sources variability (.e. error terms level). Predictors used level-1 varying across higher-level units thus residual errors level-1 higher-level units. “covariates contain two parts: one specific higher-level entity vary occasions, one represents difference occasions, within higher-level entities” (Bell Jones 2015). Hence, error terms correlated covariate, violates one assumptions mixed models (iid, independent identically distributed error terms) - also known described heterogeneity bias. can issue addressed outside FE framework? several ways address using mixed models approach: Correlated group factors predictors problem anyway, partial pooling allows estimates units o borrow strength whole sample shrink toward common mean (Shor et al. (2007)). predictor group factors correlate, one can remove correlation group-meaning (“mean within clusters,” Bafumi Gelman 2006; Gelman Hill 2007, chap. 12.6.). time-varying predictors “decomposed” time-varying time-invariant components (demeaning), mixed models can model within- -subject effects (Bell, Fairbrother, Jones 2019) - approach essentially development long-known recommendation Mundlak (Mundlak 1978). now, follow last recommendation use within- -version phq4. can see, estimates standard errors identical. argument use mixed models, .e. using mixed models panel data yield biased estimates standard errors, based incorrect model specification (Mundlak 1978). , (mixed) model properly specified, estimator mixed model identical ‘within’ (.e. FE) estimator. consequence, use specified mixed model panel data, can even specify complex models including within-effects, -effects random effects variation. mixed models approach can model causes endogeneity explicitly including (separated) within- -effects time-varying fixed effects including time-constant fixed effects. complex models, within-effects naturally change slightly longer identical simpler FE models. “bias”, rather result building complex models: FE models lack information variation group-effects -subject effects. Furthermore, FE models include random slopes, means fixed effects regressions neglecting “cross-cluster differences effects lower-level controls () reduces precision estimated context effects, resulting (…) low statistical power” (Heisig, Schaeffer, Giesecke 2017).","code":"library(lme4) mixed_1 <- lmer(   QoL ~ time + phq4_within + phq4_between + (1 | ID),   data = qol_cancer ) model_parameters(mixed_1) #> # Fixed Effects #>  #> Parameter    | Coefficient |   SE |         95% CI | t(558) |      p #> -------------------------------------------------------------------- #> (Intercept)  |       71.53 | 1.56 | [68.48, 74.59] |  45.98 | < .001 #> time         |        1.09 | 0.64 | [-0.17,  2.34] |   1.70 | 0.089  #> phq4 within  |       -3.66 | 0.41 | [-4.46, -2.86] |  -8.95 | < .001 #> phq4 between |       -6.28 | 0.50 | [-7.27, -5.30] | -12.53 | < .001 #>  #> # Random Effects #>  #> Parameter          | Coefficient #> -------------------------------- #> SD (Intercept: ID) |        9.88 #> SD (Residual)      |       12.37  # compare to FE-model model_parameters(fe_model1)[1:2, ] #> Parameter   | Coefficient |   SE |         95% CI | t(374) |      p #> ------------------------------------------------------------------- #> time        |        1.09 | 0.64 | [-0.17,  2.34] |   1.70 | 0.089  #> phq4 within |       -3.66 | 0.41 | [-4.46, -2.86] |  -8.95 | < .001 mixed_2 <- lmer(   QoL ~ time + phq4_within + phq4_between + education + (1 + time | ID),   data = qol_cancer ) # effects = \"fixed\" will not display random effects, but split the # fixed effects into its between- and within-effects components. model_parameters(mixed_2, effects = \"fixed\") #> Parameter        | Coefficient |   SE |         95% CI | t(554) |      p #> ------------------------------------------------------------------------ #> (Intercept)      |       67.36 | 2.48 | [62.48, 72.23] |  27.15 | < .001 #> time             |        1.09 | 0.66 | [-0.21,  2.39] |   1.65 | 0.099  #> education [mid]  |        5.01 | 2.35 | [ 0.40,  9.62] |   2.14 | 0.033  #> education [high] |        5.52 | 2.75 | [ 0.11, 10.93] |   2.00 | 0.046  #>  #> # Within-Effects #>  #> Parameter   | Coefficient |   SE |         95% CI | t(554) |      p #> ------------------------------------------------------------------- #> phq4 within |       -3.72 | 0.41 | [-4.52, -2.92] |  -9.10 | < .001 #>  #> # Between-Effects #>  #> Parameter    | Coefficient |   SE |         95% CI | t(554) |      p #> -------------------------------------------------------------------- #> phq4 between |       -6.13 | 0.52 | [-7.14, -5.11] | -11.84 | < .001"},{"path":"/articles/demean.html","id":"conclusion-complex-random-effects-within-between-models","dir":"Articles","previous_headings":"","what":"Conclusion: Complex Random Effects Within-Between Models","title":"Analysing Longitudinal or Panel Data","text":"Depending structure data, best approach analyzing panel data called “complex random effects within-” model (Bell, Fairbrother, Jones 2019): yit = β0 + β1W (xit - ͞xi) + β2B ͞xi + β3 zi + υi0 + υi1 (xit - ͞xi) + εit xit - ͞xi de-meaned predictor, phq4_within ͞xi group-meaned predictor, phq4_between β1W coefficient phq4_within (within-subject) β2B coefficient phq4_between (bewteen-subject) β3 coefficient time-constant predictors, hospital education (bewteen-subject) R-code, model written like : time-constant predictors? demeaning time-varying predictors, “higher level, mean term longer constrained Level 1 effects, free account higher-level variance associated variable” (Bell Jones 2015). Thus, time-constant categorical predictors, -effect, can simply included fixed effects predictor (since ’re constrained level-1 effects). Time-constant continuous group-level predictors (instance, GDP countries) group-meaned, proper “”-effect (Gelman Hill 2007, chap. 12.6.). benefit kind model information within-, - time-constant (.e. ) effects group-level predictors… … can also model variation (group) effects across time (probably space), can even include higher-level units (e.g. nested design cross-classified design two levels): imbalanced groups, .e. large differences N per group? See little example visual example…","code":"rewb <- lmer(   QoL ~ time + phq4_within + phq4_between + education +     (1 + time | ID) + (1 + phq4_within | ID),   data = qol_cancer ) model_parameters(rewb, effects = \"fixed\") #> Parameter        | Coefficient |   SE |         95% CI | t(551) |      p #> ------------------------------------------------------------------------ #> (Intercept)      |       67.18 | 2.39 | [62.49, 71.87] |  28.13 | < .001 #> time             |        1.18 | 0.60 | [-0.01,  2.37] |   1.95 | 0.051  #> education [mid]  |        4.95 | 2.35 | [ 0.34,  9.56] |   2.11 | 0.035  #> education [high] |        5.62 | 2.76 | [ 0.20, 11.04] |   2.04 | 0.042  #>  #> # Within-Effects #>  #> Parameter   | Coefficient |   SE |         95% CI | t(551) |      p #> ------------------------------------------------------------------- #> phq4 within |       -4.50 | 0.58 | [-5.64, -3.36] |  -7.78 | < .001 #>  #> # Between-Effects #>  #> Parameter    | Coefficient |   SE |         95% CI | t(551) |      p #> -------------------------------------------------------------------- #> phq4 between |       -6.11 | 0.52 | [-7.13, -5.10] | -11.81 | < .001 random_parameters(rewb) #> # Random Effects #>  #> Within-Group Variance              119.47 (10.93) #> Between-Group Variance #>   Random Intercept (ID)             107.5 (10.37) #>   Random Intercept (ID.1)           25.76  (5.08) #>   Random Slope (ID.time)             0.49   (0.7) #>   Random Slope (ID.1.phq4_within)   14.37  (3.79) #> Correlations #>   ID.time                           -0.99 #>   ID.phq4_within                     0.44 #> N (groups per factor) #>   ID                                  188 #> Observations                          564"},{"path":"/articles/demean.html","id":"a-visual-example","dir":"Articles","previous_headings":"","what":"A visual example","title":"Analysing Longitudinal or Panel Data","text":"First, generate fake data implies linear relationship outcome independent variable. objective amount typing errors depends fast (typing speed) can type, however, typing experience , faster can type. Thus, outcome measure “amount typing errors”, predictor “typing speed”. Furthermore, repeated measurements people different “typing experience levels”. results show two sources variation: Overall, experienced typists make less mistakes (group-level pattern). typing faster, typists make mistakes (individual-level pattern). Let’s look raw data…","code":"library(ggplot2) library(poorman) library(see)  set.seed(123) n <- 5 b <- seq(1, 1.5, length.out = 5) x <- seq(2, 2 * n, 2)  d <- do.call(rbind, lapply(1:n, function(i) {   data.frame(     x = seq(1, n, by = .2),     y = 2 * x[i] + b[i] * seq(1, n, by = .2) + rnorm(21),     grp = as.factor(2 * i)   ) }))  d <- d %>%   group_by(grp) %>%   mutate(x = rev(15 - (x + 1.5 * as.numeric(grp)))) %>%   ungroup()  labs <- c(\"very slow\", \"slow\", \"average\", \"fast\", \"very fast\") levels(d$grp) <- rev(labs)  d <- cbind(d, datawizard::demean(d, c(\"x\", \"y\"), group = \"grp\"))"},{"path":"/articles/demean.html","id":"model-1-linear-relationship-between-typing-errors-and-typing-speed","dir":"Articles","previous_headings":"A visual example","what":"Model 1: Linear relationship between typing errors and typing speed","title":"Analysing Longitudinal or Panel Data","text":"can now assume (linear) relationship typing errors typing speed.  Looking coefficients, following model coefficient -1.92. However, ignored clustered structure data, example due repeated measurements.","code":"m1 <- lm(y ~ x, data = d) model_parameters(m1) #> Parameter   | Coefficient |   SE |         95% CI | t(103) |      p #> ------------------------------------------------------------------- #> (Intercept) |       30.20 | 1.42 | [27.39, 33.00] |  21.34 | < .001 #> x           |       -1.92 | 0.18 | [-2.27, -1.56] | -10.69 | < .001"},{"path":"/articles/demean.html","id":"model-2-within-subject-effect-of-typing-speed","dir":"Articles","previous_headings":"A visual example","what":"Model 2: Within-subject effect of typing speed","title":"Analysing Longitudinal or Panel Data","text":"fixed effects regression (FE-regression) now remove -effects include within-effects well group-level indicator.  returns coefficient “within”-effect, 1.2, standard error 0.07. Note FE-model take variation subjects account, thus resulting (possibly) biased estimates, biased standard errors.","code":"m2 <- lm(y ~ 0 + x_within + grp, data = d) model_parameters(m2)[1, ] #> Parameter | Coefficient |   SE |       95% CI | t(99) |      p #> -------------------------------------------------------------- #> x within  |        1.20 | 0.07 | [1.06, 1.35] | 16.08 | < .001"},{"path":"/articles/demean.html","id":"model-3-between-subject-effect-of-typing-speed","dir":"Articles","previous_headings":"A visual example","what":"Model 3: Between-subject effect of typing speed","title":"Analysing Longitudinal or Panel Data","text":"understand, model 1 (m1) returns biased estimate, “weighted average” within- -effects, let us look -effect now.  can see, -effect -2.93, different -1.92 estimated model m1.","code":"m3 <- lm(y ~ x_between, data = d) model_parameters(m3) #> Parameter   | Coefficient |   SE |         95% CI | t(103) |      p #> ------------------------------------------------------------------- #> (Intercept) |       37.83 | 0.62 | [36.59, 39.06] |  60.79 | < .001 #> x between   |       -2.93 | 0.08 | [-3.09, -2.78] | -36.76 | < .001"},{"path":"/articles/demean.html","id":"model-4-mixed-model-with-within--and-between-subjects","dir":"Articles","previous_headings":"A visual example","what":"Model 4: Mixed model with within- and between-subjects","title":"Analysing Longitudinal or Panel Data","text":"Since FE-models can model within-effects, now use mixed model within- -effects.  see, estimate within-effects biased. Furthermore, get correct -effect well (standard errors differ, variance grouping structure accurately taken account).","code":"m4 <- lmer(y ~ x_between + x_within + (1 | grp), data = d) model_parameters(m4) #> # Fixed Effects #>  #> Parameter   | Coefficient |   SE |         95% CI | t(100) |      p #> ------------------------------------------------------------------- #> (Intercept) |       37.83 | 0.33 | [37.17, 38.48] | 114.46 | < .001 #> x between   |       -2.93 | 0.04 | [-3.02, -2.85] | -69.22 | < .001 #> x within    |        1.20 | 0.07 | [ 1.06,  1.35] |  16.22 | < .001 #>  #> # Random Effects #>  #> Parameter           | Coefficient #> --------------------------------- #> SD (Intercept: grp) |        0.00 #> SD (Residual)       |        0.92"},{"path":"/articles/demean.html","id":"model-5-complex-random-effects-within-between-model","dir":"Articles","previous_headings":"A visual example","what":"Model 5: Complex Random-Effects Within-Between Model","title":"Analysing Longitudinal or Panel Data","text":"Finally, can also take variation subjects account adding random slope. model can called complex “REWB” (random-effects within-) model. Due variation subjects, get larger standard errors within-effect.","code":"m5 <- lmer(y ~ x_between + x_within + (1 + x_within | grp), data = d) model_parameters(m5) #> # Fixed Effects #>  #> Parameter   | Coefficient |   SE |         95% CI |  t(98) |      p #> ------------------------------------------------------------------- #> (Intercept) |       37.95 | 0.34 | [37.28, 38.63] | 111.15 | < .001 #> x between   |       -2.95 | 0.04 | [-3.04, -2.87] | -67.57 | < .001 #> x within    |        1.20 | 0.10 | [ 1.01,  1.40] |  12.16 | < .001 #>  #> # Random Effects #>  #> Parameter                     | Coefficient #> ------------------------------------------- #> SD (Intercept: grp)           |        0.09 #> SD (x_within: grp)            |        0.15 #> Cor (Intercept~x_within: grp) |       -1.00 #> SD (Residual)                 |        0.90"},{"path":"/articles/demean.html","id":"balanced-versus-imbalanced-groups","dir":"Articles","previous_headings":"","what":"Balanced versus imbalanced groups","title":"Analysing Longitudinal or Panel Data","text":"“simple” linear slope -effect (also within-effect) (almost) identical “classical” linear regression compared linear mixed models groups balanced, .e. number observation per group similar . Whenever group size imbalanced, “simple” linear slope adjusted. leads different estimates -effects classical mixed models regressions due shrinkage - .e. larger variation group sizes find stronger regularization estimates. Hence, mixed models larger differences number observation per random effects group, -effect differ -effect calculated “classical” regression models. However, shrinkage desired property mixed models usually improves estimates.","code":"set.seed(123) n <- 5 b <- seq(1, 1.5, length.out = 5) x <- seq(2, 2 * n, 2)  d <- do.call(rbind, lapply(1:n, function(i) {   data.frame(     x = seq(1, n, by = .2),     y = 2 * x[i] + b[i] * seq(1, n, by = .2) + rnorm(21),     grp = as.factor(2 * i)   ) }))  # create imbalanced groups d$grp[sample(which(d$grp == 8), 10)] <- 6 d$grp[sample(which(d$grp == 4), 8)] <- 2 d$grp[sample(which(d$grp == 10), 9)] <- 6  d <- d %>%   group_by(grp) %>%   mutate(x = rev(15 - (x + 1.5 * as.numeric(grp)))) %>%   ungroup()  labs <- c(\"very slow\", \"slow\", \"average\", \"fast\", \"very fast\") levels(d$grp) <- rev(labs)  d <- cbind(d, datawizard::demean(d, c(\"x\", \"y\"), group = \"grp\"))  # Between-subject effect of typing speed m1 <- lm(y ~ x_between, data = d) model_parameters(m1) #> Parameter   | Coefficient |   SE |         95% CI | t(103) |      p #> ------------------------------------------------------------------- #> (Intercept) |       38.32 | 1.33 | [35.69, 40.95] |  28.87 | < .001 #> x between   |       -2.81 | 0.16 | [-3.13, -2.49] | -17.47 | < .001  # Between-subject effect of typing speed, accounting for group structure m2 <- lmer(y ~ x_between + (1 | grp), data = d) model_parameters(m2) #> # Fixed Effects #>  #> Parameter   | Coefficient |   SE |         95% CI | t(101) |      p #> ------------------------------------------------------------------- #> (Intercept) |       37.02 | 2.73 | [31.59, 42.44] |  13.54 | < .001 #> x between   |       -2.71 | 0.35 | [-3.40, -2.02] |  -7.81 | < .001 #>  #> # Random Effects #>  #> Parameter           | Coefficient #> --------------------------------- #> SD (Intercept: grp) |        1.54 #> SD (Residual)       |        2.98"},{"path":[]},{"path":"/articles/standardize_data.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Data Standardization","text":"make sense data effects, scientists might want standardize (Z-score) variables. makes data unitless, expressed terms deviation index centrality (e.g., mean median). However, aside benefits, standardization also comes challenges issues, scientist aware .","code":""},{"path":"/articles/standardize_data.html","id":"methods-of-standardization","dir":"Articles","previous_headings":"Introduction","what":"Methods of Standardization","title":"Data Standardization","text":"datawizard package offers two methods standardization via standardize() function: Normal standardization: center around mean, SD units (default). Robust standardization: center around median, MAD (median absolute deviation) units (robust = TRUE). Let’s look following example: can see different methods give different central variation values: standardize() can also used standardize full data frame - numeric variable standardized separately: Weighted standardization also supported via weights argument, factors can also standardized (’re kind thing) setting force = TRUE, converts factors treatment-coded dummy variables standardizing.","code":"library(datawizard) library(effectsize) # for data  # let's have a look at what the data look like data(\"hardlyworking\", package = \"effectsize\") head(hardlyworking) >     salary xtra_hours n_comps age seniority > 1 19744.65   4.161812       1  32         3 > 2 11301.95   1.621868       0  34         3 > 3 20635.62   1.190859       3  33         5 > 4 23047.16   7.190997       1  35         3 > 5 27342.15  11.261399       0  33         4 > 6 25656.63   3.633346       2  30         5 # let's use both methods of standardization hardlyworking$xtra_hours_z <- standardize(hardlyworking$xtra_hours) hardlyworking$xtra_hours_zr <- standardize(hardlyworking$xtra_hours, robust = TRUE) library(dplyr) library(tidyr)  hardlyworking %>%   select(starts_with(\"xtra_hours\")) %>%   pivot_longer(everything()) %>%   group_by(name) %>%   summarise(     mean = mean(value),     sd = sd(value),     median = median(value),     mad = mad(value)   ) hardlyworking_z <- standardize(hardlyworking) hardlyworking_z %>%   select(-xtra_hours_z, -xtra_hours_zr) %>%   pivot_longer(everything()) %>%   group_by(name) %>%   summarise(     mean = mean(value),     sd = sd(value),     median = median(value),     mad = mad(value)   )"},{"path":"/articles/standardize_data.html","id":"variable-wise-vs--participant-wise","dir":"Articles","previous_headings":"Introduction","what":"Variable-wise vs. Participant-wise","title":"Data Standardization","text":"Standardization important step extra caution required repeated-measures designs, three ways standardizing data: Variable-wise: common method. simple scaling column. Participant-wise: Variables standardized “within” participant, .e., participant, participant’s mean SD. Full: Participant-wise first re-standardizing variable-wise. Unfortunately, method used often explicitly stated. issue methods can generate important discrepancies (can turn contribute reproducibility crisis). Let’s investigate 3 methods.","code":""},{"path":"/articles/standardize_data.html","id":"the-data","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"The Data","title":"Data Standardization","text":"take emotion dataset participants exposed negative pictures rate emotions (valence) amount memories associated picture (autobiographical link). One make hypothesis young participants context war violence, negative pictures (mutilations) less related memories less negative pictures (involving example car crashes sick people). words, expect positive relationship valence (high values corresponding less negativity) autobiographical link. Let’s look data, averaged participants: can see means SDs, lot variability participants means individual within-participant SD.","code":"# Download the 'emotion' dataset load(url(\"https://raw.github.com/neuropsychology/psycho.R/master/data/emotion.rda\"))  # Discard neutral pictures (keep only negative) emotion <- emotion %>% filter(Emotion_Condition == \"Negative\")  # Summary emotion %>%   drop_na(Subjective_Valence, Autobiographical_Link) %>%   group_by(Participant_ID) %>%   summarise(     n_Trials = n(),     Valence_Mean = mean(Subjective_Valence),     Valence_SD = sd(Subjective_Valence)   ) >  [38;5;246m# A tibble: 19 × 4 [39m >  [38;5;246m# Groups:   Participant_ID [19] [39m >    Participant_ID n_Trials Valence_Mean Valence_SD >     [3m [38;5;246m<fct> [39m [23m              [3m [38;5;246m<int> [39m [23m         [3m [38;5;246m<dbl> [39m [23m       [3m [38;5;246m<dbl> [39m [23m >  [38;5;250m 1 [39m 10S                  24       - [31m58 [39m [31m. [39m [31m1 [39m       42.6  >  [38;5;250m 2 [39m 11S                  24       - [31m73 [39m [31m. [39m [31m2 [39m       37.0  >  [38;5;250m 3 [39m 12S                  24       - [31m57 [39m [31m. [39m [31m5 [39m       26.6  >  [38;5;250m 4 [39m 13S                  24       - [31m63 [39m [31m. [39m [31m2 [39m       23.7  >  [38;5;250m 5 [39m 14S                  24       - [31m56 [39m [31m. [39m [31m6 [39m       26.5  >  [38;5;250m 6 [39m 15S                  24       - [31m60 [39m [31m. [39m [31m6 [39m       33.7  >  [38;5;250m 7 [39m 16S                  24       - [31m46 [39m [31m. [39m [31m1 [39m       24.9  >  [38;5;250m 8 [39m 17S                  24        - [31m1 [39m [31m. [39m [31m54 [39m       4.98 >  [38;5;250m 9 [39m 18S                  24       - [31m67 [39m [31m. [39m [31m2 [39m       35.0  >  [38;5;250m10 [39m 19S                  24       - [31m59 [39m [31m. [39m [31m6 [39m       33.2  >  [38;5;250m11 [39m 1S                   24       - [31m53 [39m [31m. [39m [31m0 [39m       42.9  >  [38;5;250m12 [39m 2S                   23       - [31m43 [39m [31m. [39m [31m0 [39m       39.2  >  [38;5;250m13 [39m 3S                   24       - [31m64 [39m [31m. [39m [31m3 [39m       34.4  >  [38;5;250m14 [39m 4S                   24       - [31m81 [39m [31m. [39m [31m6 [39m       27.6  >  [38;5;250m15 [39m 5S                   24       - [31m58 [39m [31m. [39m [31m1 [39m       25.3  >  [38;5;250m16 [39m 6S                   24       - [31m74 [39m [31m. [39m [31m7 [39m       29.2  >  [38;5;250m17 [39m 7S                   24       - [31m62 [39m [31m. [39m [31m3 [39m       39.7  >  [38;5;250m18 [39m 8S                   24       - [31m56 [39m [31m. [39m [31m9 [39m       32.7  >  [38;5;250m19 [39m 9S                   24       - [31m31 [39m [31m. [39m [31m5 [39m       52.7"},{"path":"/articles/standardize_data.html","id":"effect-of-standardization","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"Effect of Standardization","title":"Data Standardization","text":"create three data frames standardized three techniques. Let’s see three standardization techniques affected Valence variable.","code":"Z_VariableWise <- emotion %>%   standardize()  Z_ParticipantWise <- emotion %>%   group_by(Participant_ID) %>%   standardize()  Z_Full <- emotion %>%   group_by(Participant_ID) %>%   standardize() %>%   ungroup() %>%   standardize()"},{"path":"/articles/standardize_data.html","id":"across-participants","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"Across Participants","title":"Data Standardization","text":"can calculate mean SD Valence across participants: means SD appear fairly similar (0 1)…  marginal distributions…","code":"# Create a convenient function to print summarise_Subjective_Valence <- function(data) {   df_name <- deparse(substitute(data))   data %>%     ungroup() %>%     summarise(       DF = df_name,       Mean = mean(Subjective_Valence),       SD = sd(Subjective_Valence)     ) } # Check the results rbind(   summarise_Subjective_Valence(Z_VariableWise),   summarise_Subjective_Valence(Z_ParticipantWise),   summarise_Subjective_Valence(Z_Full) ) library(see) library(ggplot2)  ggplot() +   geom_density(aes(Z_VariableWise$Subjective_Valence,     color = \"Z_VariableWise\"   ), size = 1) +   geom_density(aes(Z_ParticipantWise$Subjective_Valence,     color = \"Z_ParticipantWise\"   ), size = 1) +   geom_density(aes(Z_Full$Subjective_Valence,     color = \"Z_Full\"   ), size = 1) +   see::theme_modern() +   labs(color = \"\")"},{"path":"/articles/standardize_data.html","id":"at-the-participant-level","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"At the Participant Level","title":"Data Standardization","text":"However, can also look happens participant level. Let’s look first 5 participants: Seems like full participant-wise standardization give similar results, different ones variable-wise standardization.","code":"# Create convenient function print_participants <- function(data) {   df_name <- deparse(substitute(data))   data %>%     group_by(Participant_ID) %>%     summarise(       DF = df_name,       Mean = mean(Subjective_Valence),       SD = sd(Subjective_Valence)     ) %>%     head(5) %>%     select(DF, everything()) }  # Check the results rbind(   print_participants(Z_VariableWise),   print_participants(Z_ParticipantWise),   print_participants(Z_Full) )"},{"path":"/articles/standardize_data.html","id":"compare","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"Compare","title":"Data Standardization","text":"Let’s correlation variable-wise participant-wise methods.  three standardization methods roughly present characteristics general level (mean 0 SD 1) similar distribution, values exactly ! Let’s now answer original question investigating linear relationship valence autobiographical link. can running mixed-effects model participants entered random effects. can extract parameters interest model, find: can see, variable-wise standardization affects coefficient (expected, changes unit), test statistic statistical significance. However, using participant-wise standardization affect coefficient significance. method better justified, choice depends specific case, context, data goal.","code":"r <- cor.test(   Z_VariableWise$Subjective_Valence,   Z_ParticipantWise$Subjective_Valence )  data.frame(   Original = emotion$Subjective_Valence,   VariableWise = Z_VariableWise$Subjective_Valence,   ParticipantWise = Z_ParticipantWise$Subjective_Valence ) %>%   ggplot(aes(x = VariableWise, y = ParticipantWise, colour = Original)) +   geom_point(alpha = 0.75, shape = 16) +   geom_smooth(method = \"lm\", color = \"black\") +   scale_color_distiller(palette = 1) +   ggtitle(paste0(\"r = \", round(r$estimate, 2))) +   see::theme_modern() library(lme4) m_raw <- lmer(   formula = Subjective_Valence ~ Autobiographical_Link + (1 | Participant_ID),   data = emotion ) m_VariableWise <- update(m_raw, data = Z_VariableWise) m_ParticipantWise <- update(m_raw, data = Z_ParticipantWise) m_Full <- update(m_raw, data = Z_Full) # Convenient function get_par <- function(model) {   mod_name <- deparse(substitute(model))   parameters::model_parameters(model) %>%     mutate(Model = mod_name) %>%     select(-Parameter) %>%     select(Model, everything()) %>%     .[-1, ] }  # Run the model on all datasets rbind(   get_par(m_raw),   get_par(m_VariableWise),   get_par(m_ParticipantWise),   get_par(m_Full) ) > # Fixed Effects >  > Model             | Coefficient |   SE |        95% CI | t(451) |     p > ----------------------------------------------------------------------- > m_raw             |        0.09 | 0.07 | [-0.04, 0.22] |   1.36 | 0.174 > m_VariableWise    |        0.07 | 0.05 | [-0.03, 0.17] |   1.36 | 0.174 > m_ParticipantWise |        0.08 | 0.05 | [-0.01, 0.17] |   1.75 | 0.080 > m_Full            |        0.08 | 0.05 | [-0.01, 0.17] |   1.75 | 0.080 >  > # Random Effects: Participant_ID >  > Model             | Coefficient > ------------------------------- > m_raw             |       16.49 > m_VariableWise    |        0.45 > m_ParticipantWise |        0.00 > m_Full            |        0.00 >  > # Random Effects: Residual >  > Model             | Coefficient > ------------------------------- > m_raw             |       33.56 > m_VariableWise    |        0.91 > m_ParticipantWise |        0.98 > m_Full            |        1.00"},{"path":"/articles/standardize_data.html","id":"conclusion","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"Conclusion","title":"Data Standardization","text":"Standardization can useful cases justified. Variable Participant-wise standardization methods appear produce similar data. Variable Participant-wise standardization can lead different results. chosen method can strongly influence results therefore explicitly stated justified enhance reproducibility results. showed yet another way sneakily tweaking data can change results. prevent use bad practice, can highlight importance open data, open analysis/scripts, preregistration.","code":""},{"path":"/articles/standardize_data.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See also","title":"Data Standardization","text":"datawizard::demean(): https://easystats.github.io/datawizard/reference/demean.html standardize_parameters(method = \"pseudo\") mixed-effects models https://easystats.github.io/effectsize/reference/standardize_parameters.html","code":""},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dominique Makowski. Author.            @Dom_Makowski Daniel Lüdecke. Author.            @strengejacke Indrajeet Patil. Author, maintainer.            @patilindrajeets Mattan S. Ben-Shachar. Author. Brenton M. Wiernik. Author.            @bmwiernik","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Makowski, Lüdecke, Patil, Ben-Shachar, & Wiernik (2021). datawizard: Easy Data Wrangling. CRAN. Available https://easystats.github.io/datawizard/","code":"@Article{,   title = {datawizard: Easy Data Wrangling},   author = {Dominique Makowski and Daniel Lüdecke and Indrajeet Patil and Mattan S. Ben-Shachar and Brenton M. Wiernik},   journal = {CRAN},   year = {2021},   note = {R package},   url = {https://easystats.github.io/datawizard/}, }"},{"path":"/index.html","id":"datawizard-easy-data-wrangling-","dir":"","previous_headings":"","what":"Easy Data Wrangling","title":"Easy Data Wrangling","text":"Hockety pockety wockety wack, prepare data forth back ✨ datawizard lightweight package easily manipulate, clean, transform, prepare data analysis.","code":""},{"path":[]},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Easy Data Wrangling","text":"cite package, run following command:","code":"citation(\"datawizard\")  To cite datawizard in publications use:    Makowski, Lüdecke, Patil, Ben-Shachar, & Wiernik (2021). datawizard:   Easy Data Wrangling. CRAN. Available from   https://easystats.github.io/datawizard/  A BibTeX entry for LaTeX users is    @Article{,     title = {datawizard: Easy Data Wrangling},     author = {Dominique Makowski and Daniel Lüdecke and Indrajeet Patil and Mattan S. Ben-Shachar and Brenton M. Wiernik},     journal = {CRAN},     year = {2021},     note = {R package},     url = {https://easystats.github.io/datawizard/},   }"},{"path":[]},{"path":[]},{"path":"/index.html","id":"select-and-filter","dir":"","previous_headings":"Data wrangling","what":"Select and filter","title":"Easy Data Wrangling","text":"package provides helpers select columns filter rows meeting certain conditions: manipulations:","code":"matching_rows <- data_match(mtcars, data.frame(vs = 0, am = 1)) mtcars[matching_rows, ] #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 #> Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 #> Maserati Bora  15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8 head(data_addprefix(iris, \"NEW_\")) #>   NEW_Sepal.Length NEW_Sepal.Width NEW_Petal.Length NEW_Petal.Width NEW_Species #> 1              5.1             3.5              1.4             0.2      setosa #> 2              4.9             3.0              1.4             0.2      setosa #> 3              4.7             3.2              1.3             0.2      setosa #> 4              4.6             3.1              1.5             0.2      setosa #> 5              5.0             3.6              1.4             0.2      setosa #> 6              5.4             3.9              1.7             0.4      setosa"},{"path":"/index.html","id":"transform","dir":"","previous_headings":"Data wrangling","what":"Transform","title":"Easy Data Wrangling","text":"packages also contains multiple functions help transform data. example, standardize (z-score) data: winsorize data: grand-mean center data rank-transform data: rescale numeric variable new range:","code":"# before summary(swiss) #>    Fertility      Agriculture     Examination      Education     #>  Min.   :35.00   Min.   : 1.20   Min.   : 3.00   Min.   : 1.00   #>  1st Qu.:64.70   1st Qu.:35.90   1st Qu.:12.00   1st Qu.: 6.00   #>  Median :70.40   Median :54.10   Median :16.00   Median : 8.00   #>  Mean   :70.14   Mean   :50.66   Mean   :16.49   Mean   :10.98   #>  3rd Qu.:78.45   3rd Qu.:67.65   3rd Qu.:22.00   3rd Qu.:12.00   #>  Max.   :92.50   Max.   :89.70   Max.   :37.00   Max.   :53.00   #>     Catholic       Infant.Mortality #>  Min.   :  2.150   Min.   :10.80    #>  1st Qu.:  5.195   1st Qu.:18.15    #>  Median : 15.140   Median :20.00    #>  Mean   : 41.144   Mean   :19.94    #>  3rd Qu.: 93.125   3rd Qu.:21.70    #>  Max.   :100.000   Max.   :26.60  # after summary(standardize(swiss)) #>    Fertility         Agriculture       Examination         Education       #>  Min.   :-2.81327   Min.   :-2.1778   Min.   :-1.69084   Min.   :-1.0378   #>  1st Qu.:-0.43569   1st Qu.:-0.6499   1st Qu.:-0.56273   1st Qu.:-0.5178   #>  Median : 0.02061   Median : 0.1515   Median :-0.06134   Median :-0.3098   #>  Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   #>  3rd Qu.: 0.66504   3rd Qu.: 0.7481   3rd Qu.: 0.69074   3rd Qu.: 0.1062   #>  Max.   : 1.78978   Max.   : 1.7190   Max.   : 2.57094   Max.   : 4.3702   #>     Catholic       Infant.Mortality   #>  Min.   :-0.9350   Min.   :-3.13886   #>  1st Qu.:-0.8620   1st Qu.:-0.61543   #>  Median :-0.6235   Median : 0.01972   #>  Mean   : 0.0000   Mean   : 0.00000   #>  3rd Qu.: 1.2464   3rd Qu.: 0.60337   #>  Max.   : 1.4113   Max.   : 2.28566 # before anscombe #>    x1 x2 x3 x4    y1   y2    y3    y4 #> 1  10 10 10  8  8.04 9.14  7.46  6.58 #> 2   8  8  8  8  6.95 8.14  6.77  5.76 #> 3  13 13 13  8  7.58 8.74 12.74  7.71 #> 4   9  9  9  8  8.81 8.77  7.11  8.84 #> 5  11 11 11  8  8.33 9.26  7.81  8.47 #> 6  14 14 14  8  9.96 8.10  8.84  7.04 #> 7   6  6  6  8  7.24 6.13  6.08  5.25 #> 8   4  4  4 19  4.26 3.10  5.39 12.50 #> 9  12 12 12  8 10.84 9.13  8.15  5.56 #> 10  7  7  7  8  4.82 7.26  6.42  7.91 #> 11  5  5  5  8  5.68 4.74  5.73  6.89  # after winsorize(anscombe) #>       x1 x2 x3 x4   y1   y2   y3   y4 #>  [1,] 10 10 10  8 8.04 9.13 7.46 6.58 #>  [2,]  8  8  8  8 6.95 8.14 6.77 5.76 #>  [3,] 12 12 12  8 7.58 8.74 8.15 7.71 #>  [4,]  9  9  9  8 8.81 8.77 7.11 8.47 #>  [5,] 11 11 11  8 8.33 9.13 7.81 8.47 #>  [6,] 12 12 12  8 8.81 8.10 8.15 7.04 #>  [7,]  6  6  6  8 7.24 6.13 6.08 5.76 #>  [8,]  6  6  6  8 5.68 6.13 6.08 8.47 #>  [9,] 12 12 12  8 8.81 9.13 8.15 5.76 #> [10,]  7  7  7  8 5.68 7.26 6.42 7.91 #> [11,]  6  6  6  8 5.68 6.13 6.08 6.89 center(anscombe) #>    x1 x2 x3 x4          y1         y2    y3         y4 #> 1   1  1  1 -1  0.53909091  1.6390909 -0.04 -0.9209091 #> 2  -1 -1 -1 -1 -0.55090909  0.6390909 -0.73 -1.7409091 #> 3   4  4  4 -1  0.07909091  1.2390909  5.24  0.2090909 #> 4   0  0  0 -1  1.30909091  1.2690909 -0.39  1.3390909 #> 5   2  2  2 -1  0.82909091  1.7590909  0.31  0.9690909 #> 6   5  5  5 -1  2.45909091  0.5990909  1.34 -0.4609091 #> 7  -3 -3 -3 -1 -0.26090909 -1.3709091 -1.42 -2.2509091 #> 8  -5 -5 -5 10 -3.24090909 -4.4009091 -2.11  4.9990909 #> 9   3  3  3 -1  3.33909091  1.6290909  0.65 -1.9409091 #> 10 -2 -2 -2 -1 -2.68090909 -0.2409091 -1.08  0.4090909 #> 11 -4 -4 -4 -1 -1.82090909 -2.7609091 -1.77 -0.6109091 # before head(trees) #>   Girth Height Volume #> 1   8.3     70   10.3 #> 2   8.6     65   10.3 #> 3   8.8     63   10.2 #> 4  10.5     72   16.4 #> 5  10.7     81   18.8 #> 6  10.8     83   19.7  # after head(ranktransform(trees)) #>   Girth Height Volume #> 1     1    6.0    2.5 #> 2     2    3.0    2.5 #> 3     3    1.0    1.0 #> 4     4    8.5    5.0 #> 5     5   25.5    7.0 #> 6     6   28.0    9.0 change_scale(c(0, 1, 5, -5, -2)) #> [1]  50  60 100   0  30"},{"path":"/index.html","id":"reshape","dir":"","previous_headings":"Data wrangling","what":"Reshape","title":"Easy Data Wrangling","text":"common data wrangling task reshape data. Either go wide/Cartesian long/tidy format way","code":"library(datawizard)  wide_data <- data.frame(replicate(5, rnorm(10)))  data_to_long(wide_data) #>    Name       Value #> 1    X1 -0.08281164 #> 2    X2 -1.12490028 #> 3    X3 -0.70632036 #> 4    X4 -0.70278946 #> 5    X5  0.07633326 #> 6    X1  1.93468099 #> 7    X2 -0.87430362 #> 8    X3  0.96687656 #> 9    X4  0.29986416 #> 10   X5 -0.23035595 #> 11   X1 -2.05128979 #> 12   X2  0.04386162 #> 13   X3 -0.71016648 #> 14   X4  1.14946968 #> 15   X5  0.31746484 #> 16   X1  0.27773897 #> 17   X2 -0.58397514 #> 18   X3 -0.05917365 #> 19   X4 -0.30164149 #> 20   X5 -1.59268440 #> 21   X1 -1.52596060 #> 22   X2 -0.82329858 #> 23   X3 -0.23094342 #> 24   X4 -0.54733935 #> 25   X5 -0.18194062 #> 26   X1 -0.26916362 #> 27   X2  0.11059280 #> 28   X3  0.69200045 #> 29   X4 -0.38540415 #> 30   X5  1.75614174 #> 31   X1  1.23305388 #> 32   X2  0.36472778 #> 33   X3  1.35682290 #> 34   X4  0.27637203 #> 35   X5  0.11394932 #> 36   X1  0.63360774 #> 37   X2  0.05370100 #> 38   X3  1.78872284 #> 39   X4  0.15186081 #> 40   X5 -0.29216508 #> 41   X1  0.35271746 #> 42   X2  1.36867235 #> 43   X3  0.41071582 #> 44   X4 -0.43138079 #> 45   X5  1.75409316 #> 46   X1 -0.56048248 #> 47   X2 -0.38045724 #> 48   X3 -2.18785470 #> 49   X4 -1.87050006 #> 50   X5  1.80958455 long_data <- data_to_long(wide_data, rows_to = \"Row_ID\") # Save row number  data_to_wide(long_data,   colnames_from = \"Name\",   values_from = \"Value\",   rows_from = \"Row_ID\" ) #>    Row_ID    Value_X1    Value_X2    Value_X3   Value_X4    Value_X5 #> 1       1 -0.08281164 -1.12490028 -0.70632036 -0.7027895  0.07633326 #> 2       2  1.93468099 -0.87430362  0.96687656  0.2998642 -0.23035595 #> 3       3 -2.05128979  0.04386162 -0.71016648  1.1494697  0.31746484 #> 4       4  0.27773897 -0.58397514 -0.05917365 -0.3016415 -1.59268440 #> 5       5 -1.52596060 -0.82329858 -0.23094342 -0.5473394 -0.18194062 #> 6       6 -0.26916362  0.11059280  0.69200045 -0.3854041  1.75614174 #> 7       7  1.23305388  0.36472778  1.35682290  0.2763720  0.11394932 #> 8       8  0.63360774  0.05370100  1.78872284  0.1518608 -0.29216508 #> 9       9  0.35271746  1.36867235  0.41071582 -0.4313808  1.75409316 #> 10     10 -0.56048248 -0.38045724 -2.18785470 -1.8705001  1.80958455"},{"path":"/index.html","id":"data-proprties","dir":"","previous_headings":"","what":"Data proprties","title":"Easy Data Wrangling","text":"datawizard provides way provide comprehensive descriptive summary variables dataframe: even just variable also additional data properties can computed using package.","code":"data(iris) describe_distribution(iris) #> Variable     | Mean |   SD |  IQR | Min | Max | Skewness | Kurtosis |   n | n_Missing #> ------------------------------------------------------------------------------------- #> Sepal.Length |  5.8 | 0.83 | 1.30 | 4.3 | 7.9 |     0.31 |    -0.55 | 150 |         0 #> Sepal.Width  |  3.1 | 0.44 | 0.52 | 2.0 | 4.4 |     0.32 |     0.23 | 150 |         0 #> Petal.Length |  3.8 | 1.77 | 3.52 | 1.0 | 6.9 |    -0.27 |    -1.40 | 150 |         0 #> Petal.Width  |  1.2 | 0.76 | 1.50 | 0.1 | 2.5 |    -0.10 |    -1.34 | 150 |         0 describe_distribution(mtcars$wt) #> Mean |   SD | IQR | Min | Max | Skewness | Kurtosis |  n | n_Missing #> -------------------------------------------------------------------- #> 3.2  | 0.98 | 1.2 | 1.5 | 5.4 |     0.47 |     0.42 | 32 |         0 x <- (-10:10)^3 + rnorm(21, 0, 100) smoothness(x, method = \"diff\") #> [1] 1.821163 #> attr(,\"class\") #> [1] \"parameters_smoothness\" \"numeric\""},{"path":"/index.html","id":"contributing-and-support","dir":"","previous_headings":"","what":"Contributing and Support","title":"Easy Data Wrangling","text":"case want file issue contribute another way package, please follow guide. questions functionality, may either contact us via email also file issue.","code":""},{"path":"/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Easy Data Wrangling","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"/reference/adjust.html","id":null,"dir":"Reference","previous_headings":"","what":"Adjust data for the effect of other variable(s) — adjust","title":"Adjust data for the effect of other variable(s) — adjust","text":"function can used adjust data effect variables present dataset. based underlying fitting regressions models, allowing quite flexibility, including factors random effects mixed models (multilevel partialization), continuous variables smooth terms general additive models (non-linear partialization) /fitting models Bayesian framework. values returned function residuals regression models. Note regular correlation two \"adjusted\" variables equivalent partial correlation .","code":""},{"path":"/reference/adjust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adjust data for the effect of other variable(s) — adjust","text":"","code":"adjust(   data,   effect = NULL,   select = NULL,   exclude = NULL,   multilevel = FALSE,   additive = FALSE,   bayesian = FALSE,   keep_intercept = FALSE )  data_adjust(   data,   effect = NULL,   select = NULL,   exclude = NULL,   multilevel = FALSE,   additive = FALSE,   bayesian = FALSE,   keep_intercept = FALSE )"},{"path":"/reference/adjust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adjust data for the effect of other variable(s) — adjust","text":"data dataframe. effect Character vector column names adjusted (regressed ). NULL (default), variables selected. select Character vector column names. NULL (default), variables selected. exclude Character vector column names excluded selection. multilevel TRUE, factors included random factors. Else, FALSE (default), included fixed effects simple regression model. additive TRUE, continuous variables included smooth terms additive models. goal regress-potential non-linear effects. bayesian TRUE, models fitted Bayesian framework using rstanarm. keep_intercept FALSE (default), intercept model re-added. avoids centering around 0 happens default regressing another variable (see examples visual representation ).","code":""},{"path":"/reference/adjust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adjust data for the effect of other variable(s) — adjust","text":"data frame comparable data, adjusted variables.","code":""},{"path":"/reference/adjust.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adjust data for the effect of other variable(s) — adjust","text":"","code":"adjusted_all <- adjust(attitude) head(adjusted_all) #>        rating complaints privileges    learning     raises   critical #> 1  -8.1102953  5.5583770 -15.848949 -2.75102306  0.5742664  15.605502 #> 2   1.6472337  0.0646564  -1.422592 -3.06207012 -1.5567655  -2.315781 #> 3   1.0605589 -7.5116953  11.174609  5.59808033  4.8603132   8.061801 #> 4  -0.2268416  3.8345277  -4.567441  0.03866933 -7.1185324  13.002574 #> 5   6.5462010 -1.2420122  -3.051098  0.87312095 -2.7131349   6.500353 #> 6 -10.9418499  5.2030745   2.664156 -1.24552098  4.1370346 -21.678382 #>      advance #> 1  2.8684130 #> 2  5.3937097 #> 3 -6.4236221 #> 4 -0.3951046 #> 5  2.1988621 #> 6 -3.1912418 adjusted_one <- adjust(attitude, effect = \"complaints\", select = \"rating\") head(adjusted_one) #>        rating complaints privileges learning raises critical advance #> 1  -9.8614202         51         30       39     61       92      45 #> 2   0.3286522         64         51       54     63       73      47 #> 3   3.8009933         70         68       69     76       86      48 #> 4  -0.9167380         63         45       47     54       84      35 #> 5   7.7641147         78         56       66     71       83      47 #> 6 -12.8798594         55         49       44     54       49      34 # \\donttest{ adjust(attitude, effect = \"complaints\", select = \"rating\", bayesian = TRUE) #>          rating complaints privileges learning raises critical advance #> 1   -9.96168217         51         30       39     61       92      45 #> 2    0.22759085         64         51       54     63       73      47 #> 3    3.69956302         70         68       69     76       86      48 #> 4   -1.01773784         63         45       47     54       84      35 #> 5    7.66219257         78         56       66     71       83      47 #> 6  -12.98036739         55         49       44     54       49      34 #> 7   -7.03642306         67         42       56     66       68      35 #> 8   -0.07379351         75         50       55     70       66      41 #> 9   -4.35649265         82         72       67     71       83      31 #> 10   6.49160477         61         45       47     62       80      41 #> 11   9.52897522         53         53       58     58       67      34 #> 12   7.24627608         60         47       39     59       74      41 #> 13   7.73693346         62         57       42     55       63      25 #> 14  -9.11116396         83         83       45     59       77      35 #> 15   4.41686388         77         54       72     79       77      46 #> 16  -1.39386310         90         50       72     60       54      36 #> 17  -4.62050657         85         64       69     79       79      63 #> 18   5.24627608         60         65       75     55       80      60 #> 19  -2.30043698         70         46       57     75       85      46 #> 20  -8.24438131         58         68       54     64       78      52 #> 21   5.33970219         40         33       34     43       64      33 #> 22   3.49160477         61         52       62     66       80      41 #> 23 -11.28175176         66         52       50     63       80      37 #> 24  -2.39628389         37         42       58     50       57      49 #> 25   7.77430391         54         42       48     66       75      33 #> 26  -6.58313612         77         66       63     88       76      72 #> 27   6.92620649         75         58       74     80       78      49 #> 28  -9.48971001         57         44       45     51       83      38 #> 29   6.37949343         85         71       71     77       74      55 #> 30   5.64350735         82         39       59     64       78      39 adjust(attitude, effect = \"complaints\", select = \"rating\", additive = TRUE) #>          rating complaints privileges learning raises critical advance #> 1   -9.86142016         51         30       39     61       92      45 #> 2    0.32865220         64         51       54     63       73      47 #> 3    3.80099328         70         68       69     76       86      48 #> 4   -0.91673799         63         45       47     54       84      35 #> 5    7.76411473         78         56       66     71       83      47 #> 6  -12.87985944         55         49       44     54       49      34 #> 7   -6.93517726         67         42       56     66       68      35 #> 8    0.02794419         75         50       55     70       66      41 #> 9   -4.25432454         82         72       67     71       83      31 #> 10   6.59248165         61         45       47     62       80      41 #> 11   9.62936020         53         53       58     58       67      34 #> 12   7.34709147         60         47       39     59       74      41 #> 13   7.83787183         62         57       42     55       63      25 #> 14  -9.00893436         83         83       45     59       77      35 #> 15   4.51872455         77         54       72     79       77      46 #> 16  -1.29120309         90         50       72     60       54      36 #> 17  -4.51815400         85         64       69     79       79      63 #> 18   5.34709147         60         65       75     55       80      60 #> 19  -2.19900672         70         46       57     75       85      46 #> 20  -8.14368889         58         68       54     64       78      52 #> 21   5.43928784         40         33       34     43       64      33 #> 22   3.59248165         61         52       62     66       80      41 #> 23 -11.18056744         66         52       50     63       80      37 #> 24  -2.29688270         37         42       58     50       57      49 #> 25   7.87475038         54         42       48     66       75      33 #> 26  -6.48127545         77         66       63     88       76      72 #> 27   7.02794419         75         58       74     80       78      49 #> 28  -9.38907907         57         44       45     51       83      38 #> 29   6.48184600         85         71       71     77       74      55 #> 30   5.74567546         82         39       59     64       78      39 attitude$complaints_LMH <- cut(attitude$complaints, 3) adjust(attitude, effect = \"complaints_LMH\", select = \"rating\", multilevel = TRUE) #>         rating complaints privileges learning raises critical advance #> 1   -9.9809282         51         30       39     61       92      45 #> 2    2.6250549         64         51       54     63       73      47 #> 3   10.6250549         70         68       69     76       86      48 #> 4    0.6250549         63         45       47     54       84      35 #> 5    5.6503521         78         56       66     71       83      47 #> 6  -17.3749451         55         49       44     54       49      34 #> 7   -2.3749451         67         42       56     66       68      35 #> 8   -4.3496479         75         50       55     70       66      41 #> 9   -3.3496479         82         72       67     71       83      31 #> 10   6.6250549         61         45       47     62       80      41 #> 11  11.0190718         53         53       58     58       67      34 #> 12   6.6250549         60         47       39     59       74      41 #> 13   8.6250549         62         57       42     55       63      25 #> 14  -7.3496479         83         83       45     59       77      35 #> 15   1.6503521         77         54       72     79       77      46 #> 16   5.6503521         90         50       72     60       54      36 #> 17  -1.3496479         85         64       69     79       79      63 #> 18   4.6250549         60         65       75     55       80      60 #> 19   4.6250549         70         46       57     75       85      46 #> 20 -10.3749451         58         68       54     64       78      52 #> 21  -2.9809282         40         33       34     43       64      33 #> 22   3.6250549         61         52       62     66       80      41 #> 23  -7.3749451         66         52       50     63       80      37 #> 24 -12.9809282         37         42       58     50       57      49 #> 25  10.0190718         54         42       48     66       75      33 #> 26  -9.3496479         77         66       63     88       76      72 #> 27   2.6503521         75         58       74     80       78      49 #> 28 -12.3749451         57         44       45     51       83      38 #> 29   9.6503521         85         71       71     77       74      55 #> 30   6.6503521         82         39       59     64       78      39 #>    complaints_LMH #> 1     (36.9,54.7] #> 2     (54.7,72.3] #> 3     (54.7,72.3] #> 4     (54.7,72.3] #> 5     (72.3,90.1] #> 6     (54.7,72.3] #> 7     (54.7,72.3] #> 8     (72.3,90.1] #> 9     (72.3,90.1] #> 10    (54.7,72.3] #> 11    (36.9,54.7] #> 12    (54.7,72.3] #> 13    (54.7,72.3] #> 14    (72.3,90.1] #> 15    (72.3,90.1] #> 16    (72.3,90.1] #> 17    (72.3,90.1] #> 18    (54.7,72.3] #> 19    (54.7,72.3] #> 20    (54.7,72.3] #> 21    (36.9,54.7] #> 22    (54.7,72.3] #> 23    (54.7,72.3] #> 24    (36.9,54.7] #> 25    (36.9,54.7] #> 26    (72.3,90.1] #> 27    (72.3,90.1] #> 28    (54.7,72.3] #> 29    (72.3,90.1] #> 30    (72.3,90.1] # }  if (require(\"MASS\") && require(\"bayestestR\")) {   # Generate data   data <- simulate_correlation(n = 100, r = 0.7)   data$V2 <- (5 * data$V2) + 20 # Add intercept    # Adjust   adjusted <- adjust(data, effect = \"V1\", select = \"V2\")   adjusted_icpt <- adjust(data, effect = \"V1\", select = \"V2\", keep_intercept = TRUE)    # Visualize   plot(data$V1, data$V2,     pch = 19, col = \"blue\",     ylim = c(min(adjusted$V2), max(data$V2)),     main = \"Original (blue), adjusted (green), and adjusted - intercept kept (red) data\"   )   abline(lm(V2 ~ V1, data = data), col = \"blue\")   points(adjusted$V1, adjusted$V2, pch = 19, col = \"green\")   abline(lm(V2 ~ V1, data = adjusted), col = \"green\")   points(adjusted_icpt$V1, adjusted_icpt$V2, pch = 19, col = \"red\")   abline(lm(V2 ~ V1, data = adjusted_icpt), col = \"red\") } #> Loading required package: MASS #> Loading required package: bayestestR"},{"path":"/reference/center.html","id":null,"dir":"Reference","previous_headings":"","what":"Centering (Grand-Mean Centering) — center","title":"Centering (Grand-Mean Centering) — center","text":"Performs grand-mean centering data.","code":""},{"path":"/reference/center.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Centering (Grand-Mean Centering) — center","text":"","code":"center(x, ...)  centre(x, ...)  # S3 method for numeric center(   x,   robust = FALSE,   weights = NULL,   verbose = TRUE,   reference = NULL,   center = NULL,   ... )  # S3 method for data.frame center(   x,   robust = FALSE,   weights = NULL,   verbose = TRUE,   reference = NULL,   select = NULL,   exclude = NULL,   remove_na = c(\"none\", \"selected\", \"all\"),   force = FALSE,   append = FALSE,   center = NULL,   ... )"},{"path":"/reference/center.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Centering (Grand-Mean Centering) — center","text":"x data frame, (numeric character) vector factor. ... Currently used. robust Logical, TRUE, centering done subtracting median variables. FALSE, variables centered subtracting mean. weights Can NULL (weighting), : data frames: numeric vector weights, character name column data.frame contains weights. numeric vectors: numeric vector weights. verbose Toggle warnings messages. reference data frame variable centrality deviation computed instead input variable. Useful standardizing subset new data according another data frame. center Numeric value, can used alternative reference define reference centrality. center length 1, recycled match length selected variables centering. Else, center must length number selected variables. Values center matched selected variables provided order, unless named vector given. case, names matched names selected variables. select Character vector column names. NULL (default), variables selected. exclude Character vector column names excluded selection. remove_na missing values (NA) treated: \"none\" (default): column's standardization done separately, ignoring NAs. Else, rows NA columns selected select / exclude (\"selected\") columns (\"\") dropped standardization, resulting data frame include cases. force Logical, TRUE, forces centering factors well. Factors converted numerical values, lowest level value 1 (unless factor numeric levels, converted corresponding numeric value). append Logical string. TRUE, centered variables get new column names (suffix \"_c\") appended (column bind) x, thus returning original centered variables. FALSE, original variables x overwritten centered versions. character value, centered variables appended new column names (using defined suffix) original data frame.","code":""},{"path":"/reference/center.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Centering (Grand-Mean Centering) — center","text":"centered variables.","code":""},{"path":"/reference/center.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Centering (Grand-Mean Centering) — center","text":"Difference centering standardizing: Standardized variables computed subtracting mean variable dividing standard deviation, centering variables involves subtraction.","code":""},{"path":[]},{"path":"/reference/center.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Centering (Grand-Mean Centering) — center","text":"","code":"data(iris)  # entire dataframe or a vector head(iris$Sepal.Width) #> [1] 3.5 3.0 3.2 3.1 3.6 3.9 head(center(iris$Sepal.Width)) #> [1]  0.44266667 -0.05733333  0.14266667  0.04266667  0.54266667  0.84266667 head(center(iris)) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   -0.7433333  0.44266667       -2.358  -0.9993333  setosa #> 2   -0.9433333 -0.05733333       -2.358  -0.9993333  setosa #> 3   -1.1433333  0.14266667       -2.458  -0.9993333  setosa #> 4   -1.2433333  0.04266667       -2.258  -0.9993333  setosa #> 5   -0.8433333  0.54266667       -2.358  -0.9993333  setosa #> 6   -0.4433333  0.84266667       -2.058  -0.7993333  setosa head(center(iris, force = TRUE)) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   -0.7433333  0.44266667       -2.358  -0.9993333      -1 #> 2   -0.9433333 -0.05733333       -2.358  -0.9993333      -1 #> 3   -1.1433333  0.14266667       -2.458  -0.9993333      -1 #> 4   -1.2433333  0.04266667       -2.258  -0.9993333      -1 #> 5   -0.8433333  0.54266667       -2.358  -0.9993333      -1 #> 6   -0.4433333  0.84266667       -2.058  -0.7993333      -1  # only the selected columns from a dataframe center(anscombe, select = c(\"x1\", \"x3\")) #>    x1 x2 x3 x4    y1   y2    y3    y4 #> 1   1 10  1  8  8.04 9.14  7.46  6.58 #> 2  -1  8 -1  8  6.95 8.14  6.77  5.76 #> 3   4 13  4  8  7.58 8.74 12.74  7.71 #> 4   0  9  0  8  8.81 8.77  7.11  8.84 #> 5   2 11  2  8  8.33 9.26  7.81  8.47 #> 6   5 14  5  8  9.96 8.10  8.84  7.04 #> 7  -3  6 -3  8  7.24 6.13  6.08  5.25 #> 8  -5  4 -5 19  4.26 3.10  5.39 12.50 #> 9   3 12  3  8 10.84 9.13  8.15  5.56 #> 10 -2  7 -2  8  4.82 7.26  6.42  7.91 #> 11 -4  5 -4  8  5.68 4.74  5.73  6.89 center(anscombe, exclude = c(\"x1\", \"x3\")) #>    x1 x2 x3 x4          y1         y2    y3         y4 #> 1  10  1 10 -1  0.53909091  1.6390909 -0.04 -0.9209091 #> 2   8 -1  8 -1 -0.55090909  0.6390909 -0.73 -1.7409091 #> 3  13  4 13 -1  0.07909091  1.2390909  5.24  0.2090909 #> 4   9  0  9 -1  1.30909091  1.2690909 -0.39  1.3390909 #> 5  11  2 11 -1  0.82909091  1.7590909  0.31  0.9690909 #> 6  14  5 14 -1  2.45909091  0.5990909  1.34 -0.4609091 #> 7   6 -3  6 -1 -0.26090909 -1.3709091 -1.42 -2.2509091 #> 8   4 -5  4 10 -3.24090909 -4.4009091 -2.11  4.9990909 #> 9  12  3 12 -1  3.33909091  1.6290909  0.65 -1.9409091 #> 10  7 -2  7 -1 -2.68090909 -0.2409091 -1.08  0.4090909 #> 11  5 -4  5 -1 -1.82090909 -2.7609091 -1.77 -0.6109091  # centering with reference center and scale d <- data.frame(   a = c(-2, -1, 0, 1, 2),   b = c(3, 4, 5, 6, 7) )  # default centering at mean center(d) #>    a  b #> 1 -2 -2 #> 2 -1 -1 #> 3  0  0 #> 4  1  1 #> 5  2  2  # centering, using 0 as mean center(d, center = 0) #>    a b #> 1 -2 3 #> 2 -1 4 #> 3  0 5 #> 4  1 6 #> 5  2 7  # centering, using -5 as mean center(d, center = -5) #>   a  b #> 1 3  8 #> 2 4  9 #> 3 5 10 #> 4 6 11 #> 5 7 12"},{"path":"/reference/compact_character.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove empty strings from character — compact_character","title":"Remove empty strings from character — compact_character","text":"Remove empty strings character","code":""},{"path":"/reference/compact_character.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove empty strings from character — compact_character","text":"","code":"compact_character(x)"},{"path":"/reference/compact_character.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove empty strings from character — compact_character","text":"x single character vector characters.","code":""},{"path":"/reference/compact_character.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove empty strings from character — compact_character","text":"character character vector empty strings removed.","code":""},{"path":"/reference/compact_character.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove empty strings from character — compact_character","text":"","code":"compact_character(c(\"x\", \"y\", NA)) #> [1] \"x\" \"y\" compact_character(c(\"x\", \"NULL\", \"\", \"y\")) #> [1] \"x\" \"y\""},{"path":"/reference/compact_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove empty elements from lists — compact_list","title":"Remove empty elements from lists — compact_list","text":"Remove empty elements lists","code":""},{"path":"/reference/compact_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove empty elements from lists — compact_list","text":"","code":"compact_list(x, remove_na = FALSE)"},{"path":"/reference/compact_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove empty elements from lists — compact_list","text":"x list vector. remove_na Logical decide NAs removed.","code":""},{"path":"/reference/compact_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove empty elements from lists — compact_list","text":"","code":"compact_list(list(NULL, 1, c(NA, NA))) #> [[1]] #> [1] 1 #>  #> [[2]] #> [1] NA NA #>  compact_list(c(1, NA, NA)) #> [1]  1 NA NA compact_list(c(1, NA, NA), remove_na = TRUE) #> [1] 1"},{"path":"/reference/convert_data_to_numeric.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert data to numeric — convert_data_to_numeric","title":"Convert data to numeric — convert_data_to_numeric","text":"Convert data numeric converting characters factors factors either numeric levels dummy variables.","code":""},{"path":"/reference/convert_data_to_numeric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert data to numeric — convert_data_to_numeric","text":"","code":"convert_data_to_numeric(x, ...)  data_to_numeric(x, ...)  # S3 method for data.frame convert_data_to_numeric(x, dummy_factors = TRUE, ...)"},{"path":"/reference/convert_data_to_numeric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert data to numeric — convert_data_to_numeric","text":"x data frame vector. ... Arguments passed methods. dummy_factors Transform factors dummy factors (factor levels different columns filled binary 0-1 value).","code":""},{"path":"/reference/convert_data_to_numeric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert data to numeric — convert_data_to_numeric","text":"data frame numeric variables.","code":""},{"path":"/reference/convert_data_to_numeric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert data to numeric — convert_data_to_numeric","text":"","code":"convert_data_to_numeric(head(ToothGrowth)) #>    len supp.OJ supp.VC dose #> 1  4.2       0       1  0.5 #> 2 11.5       0       1  0.5 #> 3  7.3       0       1  0.5 #> 4  5.8       0       1  0.5 #> 5  6.4       0       1  0.5 #> 6 10.0       0       1  0.5 convert_data_to_numeric(head(ToothGrowth), dummy_factors = FALSE) #>    len supp dose #> 1  4.2    2  0.5 #> 2 11.5    2  0.5 #> 3  7.3    2  0.5 #> 4  5.8    2  0.5 #> 5  6.4    2  0.5 #> 6 10.0    2  0.5"},{"path":"/reference/convert_to_na.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert non-missing values in a variable into missing values. — convert_to_na","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"Convert non-missing values variable missing values.","code":""},{"path":"/reference/convert_to_na.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"","code":"convert_to_na(x, ...)  # S3 method for numeric convert_to_na(x, na = NULL, verbose = TRUE, ...)  # S3 method for data.frame convert_to_na(x, na = NULL, select = NULL, exclude = NULL, verbose = TRUE, ...)"},{"path":"/reference/convert_to_na.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"x vector, factor data frame. ... used. na Numeric character vector (list numeric character vectors) values converted NA. verbose Toggle warnings. select Character vector column names. NULL (default), variables selected. exclude Character vector column names excluded selection.","code":""},{"path":"/reference/convert_to_na.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"x, values na converted NA.","code":""},{"path":"/reference/convert_to_na.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"","code":"x <- sample(1:6, size = 30, replace = TRUE) x #>  [1] 3 5 2 3 4 4 3 1 4 3 5 5 4 3 1 5 1 5 3 6 1 4 5 6 2 6 6 5 3 6 # values 4 and 5 to NA convert_to_na(x, na = 4:5) #>  [1]  3 NA  2  3 NA NA  3  1 NA  3 NA NA NA  3  1 NA  1 NA  3  6  1 NA NA  6  2 #> [26]  6  6 NA  3  6  # data frames set.seed(123) x <- data.frame(   a = sample(1:6, size = 20, replace = TRUE),   b = sample(letters[1:6], size = 20, replace = TRUE),   c = sample(c(30:33, 99), size = 20, replace = TRUE) ) # for all numerics, convert 5 to NA. Character/factor will be ignored. convert_to_na(x, na = 5) #>     a b  c #> 1   3 a 33 #> 2   6 e 99 #> 3   3 c 99 #> 4   2 b 32 #> 5   2 b 30 #> 6   6 a 31 #> 7   3 f 99 #> 8  NA c 99 #> 9   4 d 33 #> 10  6 f 99 #> 11  6 a 31 #> 12  1 c 30 #> 13  2 e 30 #> 14  3 d 32 #> 15 NA b 30 #> 16  3 e 99 #> 17  3 a 30 #> 18  1 a 31 #> 19  4 b 33 #> 20  1 c 33  # for numerics, 5 to NA, for character/factor, \"f\" to NA convert_to_na(x, na = list(6, \"f\")) #>     a    b  c #> 1   3    a 33 #> 2  NA    e 99 #> 3   3    c 99 #> 4   2    b 32 #> 5   2    b 30 #> 6  NA    a 31 #> 7   3 <NA> 99 #> 8   5    c 99 #> 9   4    d 33 #> 10 NA <NA> 99 #> 11 NA    a 31 #> 12  1    c 30 #> 13  2    e 30 #> 14  3    d 32 #> 15  5    b 30 #> 16  3    e 99 #> 17  3    a 30 #> 18  1    a 31 #> 19  4    b 33 #> 20  1    c 33  # select specific variables convert_to_na(x, select = c(\"a\", \"b\"), na = list(6, \"f\")) #>     a    b  c #> 1   3    a 33 #> 2  NA    e 99 #> 3   3    c 99 #> 4   2    b 32 #> 5   2    b 30 #> 6  NA    a 31 #> 7   3 <NA> 99 #> 8   5    c 99 #> 9   4    d 33 #> 10 NA <NA> 99 #> 11 NA    a 31 #> 12  1    c 30 #> 13  2    e 30 #> 14  3    d 32 #> 15  5    b 30 #> 16  3    e 99 #> 17  3    a 30 #> 18  1    a 31 #> 19  4    b 33 #> 20  1    c 33"},{"path":"/reference/data_cut.html","id":null,"dir":"Reference","previous_headings":"","what":"Recode (or ","title":"Recode (or ","text":"functions divides range variables intervals recodes values inside intervals according related interval. basically wrapper around base R's cut(), providing simplified accessible way define interval breaks (cut-values).","code":""},{"path":"/reference/data_cut.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recode (or ","text":"","code":"data_cut(x, ...)  # S3 method for numeric data_cut(   x,   split = \"median\",   n_groups = NULL,   range = NULL,   lowest = 1,   labels = NULL,   verbose = TRUE,   ... )  # S3 method for data.frame data_cut(   x,   split = \"median\",   n_groups = NULL,   range = NULL,   lowest = 1,   labels = NULL,   select = NULL,   exclude = NULL,   force = FALSE,   append = FALSE,   verbose = TRUE,   ... )"},{"path":"/reference/data_cut.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recode (or ","text":"x data frame, numeric vector factor. ... used. split Character vector, indicating breaks split variables, numeric values values indicating breaks. character, may one \"median\", \"mean\", \"quantile\", \"equal_length\", \"equal_range\". \"median\" \"mean\" return dichotomous variables, split mean median, respectively. \"quantile\" \"equal_length\" split variable n_groups groups, group refers interval specific range values. Thus, length interval based number groups. \"equal_range\" also splits variable multiple groups, however, length interval given, number resulting groups (hence, number breaks) determined many intervals can generated, based full range variable. n_groups split \"quantile\" \"equal_length\", defines number requested groups (.e. resulting number levels values) recoded variable(s). \"quantile\" define intervals based distribution variable, \"equal_length\" tries divide range variable pieces equal length. range split = \"equal_range\", defines range values recoded new value. lowest Minimum value recoded variable(s). NULL (default), numeric variables, minimum original input preserved. factors, default minimum 1. split = \"equal_range\", default minimum always 1, unless specified otherwise lowest. labels Character vector value labels. NULL, data_cut() returns factors instead numeric variables, labels used labelling factor levels. verbose Toggle warnings messages . select Character vector column names. NULL (default), variables selected. exclude Character vector column names excluded selection. force Logical, TRUE, forces recoding factors well. append Logical string. TRUE, recoded variables get new column names (suffix \"_r\") appended (column bind) x, thus returning original recoded variables. FALSE, original variables x overwritten recoded versions. character value, recoded variables appended new column names (using defined suffix) original data frame.","code":""},{"path":"/reference/data_cut.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recode (or ","text":"x, recoded groups. default x numeric, unless labelsis specified. case, factor returned, factor levels (.e. recoded groups labelled accordingly.","code":""},{"path":[]},{"path":"/reference/data_cut.html","id":"splits-and-breaks-cut-off-values-","dir":"Reference","previous_headings":"","what":"Splits and breaks (cut-off values)","title":"Recode (or ","text":"Breaks general exclusive, means values indicate lower bound next group interval begin. Take simple example, numeric variable values 1 9. median 5, thus first interval ranges 1-4 recoded 1, 5-9 turn 2 (compare cbind(1:9, data_cut(1:9))). variable, using split = \"quantile\" n_groups = 3 define breaks 3.67 6.33 (see quantile(1:9, probs = c(1/3, 2/3)), means values 1 3 belong first interval recoded 1 (next interval starts 3.67), 4 6 2 7 9 3.","code":""},{"path":"/reference/data_cut.html","id":"recoding-into-groups-with-equal-size-or-range","dir":"Reference","previous_headings":"","what":"Recoding into groups with equal size or range","title":"Recode (or ","text":"split = \"equal_length\" split = \"equal_range\" try divide range x intervals similar () length. difference split = \"equal_length\" divide range x n_groups pieces thereby defining intervals used breaks (hence, equivalent cut(x, breaks = n_groups)),  split = \"equal_range\" cut x intervals length range, first interval defaults starts 1. lowest (starting) value interval can defined using lowest argument.","code":""},{"path":"/reference/data_cut.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recode (or ","text":"","code":"set.seed(123) x <- sample(1:10, size = 50, replace = TRUE)  table(x) #> x #>  1  2  3  4  5  6  7  8  9 10  #>  2  3  5  3  7  5  5  2 11  7   # by default, at median table(data_cut(x)) #>  #>  1  2  #> 25 25   # into 3 groups, based on distribution (quantiles) table(data_cut(x, split = \"quantile\", n_groups = 3)) #>  #>  1  2  3  #> 13 19 18   # into 3 groups, user-defined break table(data_cut(x, split = c(3, 5))) #>  #>  1  2  3  #>  5  8 37   set.seed(123) x <- sample(1:100, size = 500, replace = TRUE)  # into 5 groups, try to recode into intervals of similar length, # i.e. the range within groups is the same for all groups table(data_cut(x, split = \"equal_length\", n_groups = 5)) #>  #>   1   2   3   4   5  #>  89 116  96  94 105   # into 5 groups, try to return same range within groups # i.e. 1-20, 21-40, 41-60, etc. Since the range of \"x\" is # 1-100, and we have a range of 20, this results into 5 # groups, and thus is for this particular case identical # to the previous result. table(data_cut(x, split = \"equal_range\", range = 20)) #>  #>   1   2   3   4   5  #>  89 116  96  94 105   # return factor with value labels instead of numeric value set.seed(123) x <- sample(1:10, size = 30, replace = TRUE) data_cut(x, \"equal_length\", n_groups = 3) #>  [1] 1 1 3 1 2 2 2 2 3 3 2 1 3 3 3 1 3 3 3 3 3 1 2 1 3 2 3 3 3 3 data_cut(x, \"equal_length\", n_groups = 3, labels = c(\"low\", \"mid\", \"high\")) #>  [1] low  low  high low  mid  mid  mid  mid  high high mid  low  high high high #> [16] low  high high high high high low  mid  low  high mid  high high high high #> Levels: low mid high"},{"path":"/reference/data_extract.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract a single column or element from an object — data_extract","title":"Extract a single column or element from an object — data_extract","text":"extract() similar $. extracts single column element object (e.g., data frame, list, )","code":""},{"path":"/reference/data_extract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract a single column or element from an object — data_extract","text":"","code":"data_extract(data, select, name = NULL, ...)  extract(data, select, name = NULL, ...)"},{"path":"/reference/data_extract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract a single column or element from an object — data_extract","text":"data object subset. Methods currently available data frames data frame extensions (e.g., tibbles). select variable specified : literal variable name (e.g., column_name) character vector variable name (e.g., \"column_name\") positive integer, giving position counting left negative integer, giving position counting right. default returns last column. special value 0 \"row.names\" given, row names object () extracted. name optional argument specifies column used names vector extraction. Specified way select. ... use future methods.","code":""},{"path":"/reference/data_extract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract a single column or element from an object — data_extract","text":"vector containing extracted element.","code":""},{"path":"/reference/data_extract.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract a single column or element from an object — data_extract","text":"","code":"extract(mtcars, cyl, name = gear) #> 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4  #> 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4  extract(mtcars, \"cyl\", name = gear) #> 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4  #> 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4  extract(mtcars, -1, name = gear) #> 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4  #> 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2  extract(mtcars, cyl, name = 0) #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>                   6                   6                   4                   6  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>                   8                   6                   8                   4  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>                   4                   6                   6                   8  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>                   8                   8                   8                   8  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>                   8                   4                   4                   4  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>                   4                   8                   8                   8  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>                   8                   4                   4                   4  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>                   8                   6                   8                   4  extract(mtcars, cyl, name = \"row.names\") #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>                   6                   6                   4                   6  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>                   8                   6                   8                   4  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>                   4                   6                   6                   8  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>                   8                   8                   8                   8  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>                   8                   4                   4                   4  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>                   4                   8                   8                   8  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>                   8                   4                   4                   4  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>                   8                   6                   8                   4"},{"path":"/reference/data_match.html","id":null,"dir":"Reference","previous_headings":"","what":"Find row indices of a data frame matching a specific condition — data_match","title":"Find row indices of a data frame matching a specific condition — data_match","text":"Find row indices data frame match specific condition.","code":""},{"path":"/reference/data_match.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find row indices of a data frame matching a specific condition — data_match","text":"","code":"data_match(x, to, ...)"},{"path":"/reference/data_match.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find row indices of a data frame matching a specific condition — data_match","text":"x data frame. data frame matching specified conditions. ... arguments passed functions.","code":""},{"path":"/reference/data_match.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find row indices of a data frame matching a specific condition — data_match","text":"dataframe containing rows match specified configuration.","code":""},{"path":"/reference/data_match.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find row indices of a data frame matching a specific condition — data_match","text":"","code":"matching_rows <- data_match(mtcars, data.frame(vs = 0, am = 1)) mtcars[matching_rows, ] #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 #> Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 #> Maserati Bora  15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  matching_rows <- data_match(mtcars, data.frame(vs = 0, am = c(0, 1))) mtcars[matching_rows, ] #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4 #> Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3 #> Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3 #> Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3 #> Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4 #> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4 #> Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4 #> Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 #> AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2 #> Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4 #> Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2 #> Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 #> Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 #> Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8"},{"path":"/reference/data_merge.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge (join) two data frames, or a list of data frames — data_merge","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Merge (join) two data frames, list data frames. However, unlike base R's merge(), data_merge() offers methods join data frames, drop data frame column attributes.","code":""},{"path":"/reference/data_merge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"","code":"data_merge(x, ...)  data_join(x, ...)  # S3 method for data.frame data_merge(x, y, join = \"left\", by = NULL, id = NULL, verbose = TRUE, ...)  # S3 method for list data_merge(x, join = \"left\", by = NULL, id = NULL, verbose = TRUE, ...)"},{"path":"/reference/data_merge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"x, y data frame merge. x may also list data frames merged. Note list-method y argument. ... used. join Character vector, indicating method joining data frames. Can \"full\", \"left\" (default), \"right\", \"inner\", \"anti\", \"semi\" \"bind\". See details . Specifications columns used merging. id Optional name ID column created indicate source data frames appended rows. applies join = \"bind\". verbose Toggle warnings.","code":""},{"path":"/reference/data_merge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"merged data frame.","code":""},{"path":[]},{"path":"/reference/data_merge.html","id":"merging-data-frames","dir":"Reference","previous_headings":"","what":"Merging data frames","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Merging data frames performed adding rows (cases), columns (variables) source data frame (y) target data frame (x). usually requires one variables included data frames used merging, typically indicated argument. contains variable present data frames, cases matched filtered identical values x y.","code":""},{"path":"/reference/data_merge.html","id":"left-and-right-joins","dir":"Reference","previous_headings":"","what":"Left- and right-joins","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Left- right joins usually add new rows (cases), new columns (variables) existing cases x. join = \"left\" join = \"right\" work, must indicate one columns included data frames. join = \"left\", identifier variable, included x y, variables y copied x, cases y matching values identifier variable x (.e. cases x also found y get related values new columns y). match identifiers x y, copied variable y get NA value particular case. variables occur x y, used identifiers (), renamed avoid multiple identical variable names. Cases y values identifier match x's identifier removed. join = \"right\" works similar way join = \"left\", just cases x matching values identifier variable y chosen.  base R, equivalent merge(x, y, .x = TRUE) merge(x, y, .y = TRUE).","code":""},{"path":"/reference/data_merge.html","id":"full-joins","dir":"Reference","previous_headings":"","what":"Full joins","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Full joins copy cases y x. matching cases data frames, values new variables copied y x. cases y present x, added new rows x. Thus, full joins add new columns (variables), also might add new rows (cases).  base R, equivalent merge(x, y, = TRUE).","code":""},{"path":"/reference/data_merge.html","id":"inner-joins","dir":"Reference","previous_headings":"","what":"Inner joins","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Inner joins merge two data frames, however, rows (cases) kept present data frames. Thus, inner joins usually add new columns (variables), also remove rows (cases) occur one data frame.  base R, equivalent merge(x, y).","code":""},{"path":"/reference/data_merge.html","id":"binds","dir":"Reference","previous_headings":"","what":"Binds","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"join = \"bind\" row-binds complete second data frame y x. Unlike simple rbind(), requires columns data frames, join = \"bind\" bind shared columns y x, add new columns y x.","code":""},{"path":"/reference/data_merge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"","code":"x <- data.frame(a = 1:3, b = c(\"a\", \"b\", \"c\"), c = 5:7, id = 1:3) y <- data.frame(c = 6:8, d = c(\"f\", \"g\", \"h\"), e = 100:102, id = 2:4)  x #>   a b c id #> 1 1 a 5  1 #> 2 2 b 6  2 #> 3 3 c 7  3 y #>   c d   e id #> 1 6 f 100  2 #> 2 7 g 101  3 #> 3 8 h 102  4  # \"by\" will default to all shared columns, i.e. \"c\" and \"id\". new columns # \"d\" and \"e\" will be copied from \"y\" to \"x\", but there are only two cases # in \"x\" that have the same values for \"c\" and \"id\" in \"y\". only those cases # have values in the copied columns, the other case gets \"NA\". data_merge(x, y, join = \"left\") #>   a b c id    d   e #> 3 1 a 5  1 <NA>  NA #> 1 2 b 6  2    f 100 #> 2 3 c 7  3    g 101  # we change the id-value here x <- data.frame(a = 1:3, b = c(\"a\", \"b\", \"c\"), c = 5:7, id = 1:3) y <- data.frame(c = 6:8, d = c(\"f\", \"g\", \"h\"), e = 100:102, id = 3:5)  x #>   a b c id #> 1 1 a 5  1 #> 2 2 b 6  2 #> 3 3 c 7  3 y #>   c d   e id #> 1 6 f 100  3 #> 2 7 g 101  4 #> 3 8 h 102  5  # no cases in \"y\" have the same matching \"c\" and \"id\" as in \"x\", thus # copied variables from \"y\" to \"x\" copy no values, all get NA. data_merge(x, y, join = \"left\") #>   a b c id    d  e #> 1 1 a 5  1 <NA> NA #> 2 2 b 6  2 <NA> NA #> 3 3 c 7  3 <NA> NA  # one case in \"y\" has a match in \"id\" with \"x\", thus values for this # case from the remaining variables in \"y\" are copied to \"x\", all other # values (cases) in those remaining variables get NA data_merge(x, y, join = \"left\", by = \"id\") #>   a b id    d   e c.x c.y #> 2 1 a  1 <NA>  NA   5  NA #> 3 2 b  2 <NA>  NA   6  NA #> 1 3 c  3    f 100   7   6  data(mtcars) x <- mtcars[1:5, 1:3] y <- mtcars[28:32, 4:6]  # add ID common column x$id <- 1:5 y$id <- 3:7  # left-join, add new variables and copy values from y to x, # where \"id\" values match data_merge(x, y) #>    mpg cyl disp id  hp drat    wt #> 4 21.0   6  160  1  NA   NA    NA #> 5 21.0   6  160  2  NA   NA    NA #> 1 22.8   4  108  3 113 3.77 1.513 #> 2 21.4   6  258  4 264 4.22 3.170 #> 3 18.7   8  360  5 175 3.62 2.770  # right-join, add new variables and copy values from x to y, # where \"id\" values match data_merge(x, y, join = \"right\") #>    mpg cyl disp id  hp drat    wt #> 1 22.8   4  108  3 113 3.77 1.513 #> 2 21.4   6  258  4 264 4.22 3.170 #> 3 18.7   8  360  5 175 3.62 2.770 #> 4   NA  NA   NA  6 335 3.54 3.570 #> 5   NA  NA   NA  7 109 4.11 2.780  # full-join data_merge(x, y, join = \"full\") #>    mpg cyl disp id  hp drat    wt #> 4 21.0   6  160  1  NA   NA    NA #> 5 21.0   6  160  2  NA   NA    NA #> 1 22.8   4  108  3 113 3.77 1.513 #> 2 21.4   6  258  4 264 4.22 3.170 #> 3 18.7   8  360  5 175 3.62 2.770 #> 6   NA  NA   NA  6 335 3.54 3.570 #> 7   NA  NA   NA  7 109 4.11 2.780   data(mtcars) x <- mtcars[1:5, 1:3] y <- mtcars[28:32, c(1, 4:5)]  # add ID common column x$id <- 1:5 y$id <- 3:7  # left-join, no matching rows (because columns \"id\" and \"disp\" are used) # new variables get all NA values data_merge(x, y) #>    mpg cyl disp id hp drat #> 1 21.0   6  160  1 NA   NA #> 2 21.0   6  160  2 NA   NA #> 3 22.8   4  108  3 NA   NA #> 4 21.4   6  258  4 NA   NA #> 5 18.7   8  360  5 NA   NA  # one common value in \"mpg\", so one row from y is copied to x data_merge(x, y, by = \"mpg\") #>    mpg cyl disp  hp drat id.x id.y #> 2 21.0   6  160  NA   NA    1   NA #> 3 21.0   6  160  NA   NA    2   NA #> 4 22.8   4  108  NA   NA    3   NA #> 1 21.4   6  258 109 4.11    4    7 #> 5 18.7   8  360  NA   NA    5   NA  # only keep rows with matching values in by-column data_merge(x, y, join = \"semi\", by = \"mpg\") #>                 mpg cyl disp id #> Hornet 4 Drive 21.4   6  258  4  # only keep rows with non-matching values in by-column data_merge(x, y, join = \"anti\", by = \"mpg\") #>                    mpg cyl disp id #> Mazda RX4         21.0   6  160  1 #> Mazda RX4 Wag     21.0   6  160  2 #> Datsun 710        22.8   4  108  3 #> Hornet Sportabout 18.7   8  360  5  # merge list of data frames. can be of different rows x <- mtcars[1:5, 1:3] y <- mtcars[28:31, 3:5] z <- mtcars[11:18, c(1, 3:4, 6:8)] x$id <- 1:5 y$id <- 4:7 z$id <- 3:10 data_merge(list(x, y, z), join = \"bind\", by = \"id\", id = \"source\") #>     mpg cyl  disp id  hp drat    wt  qsec vs source #> 1  21.0   6 160.0  1  NA   NA    NA    NA NA      1 #> 2  21.0   6 160.0  2  NA   NA    NA    NA NA      1 #> 3  22.8   4 108.0  3  NA   NA    NA    NA NA      1 #> 4  21.4   6 258.0  4  NA   NA    NA    NA NA      1 #> 5  18.7   8 360.0  5  NA   NA    NA    NA NA      1 #> 6    NA  NA  95.1  4 113 3.77    NA    NA NA      2 #> 7    NA  NA 351.0  5 264 4.22    NA    NA NA      2 #> 8    NA  NA 145.0  6 175 3.62    NA    NA NA      2 #> 9    NA  NA 301.0  7 335 3.54    NA    NA NA      2 #> 10 17.8  NA 167.6  3 123   NA 3.440 18.90  1      3 #> 11 16.4  NA 275.8  4 180   NA 4.070 17.40  0      3 #> 12 17.3  NA 275.8  5 180   NA 3.730 17.60  0      3 #> 13 15.2  NA 275.8  6 180   NA 3.780 18.00  0      3 #> 14 10.4  NA 472.0  7 205   NA 5.250 17.98  0      3 #> 15 10.4  NA 460.0  8 215   NA 5.424 17.82  0      3 #> 16 14.7  NA 440.0  9 230   NA 5.345 17.42  0      3 #> 17 32.4  NA  78.7 10  66   NA 2.200 19.47  1      3"},{"path":"/reference/data_partition.html","id":null,"dir":"Reference","previous_headings":"","what":"Partition data into a test and a training set — data_partition","title":"Partition data into a test and a training set — data_partition","text":"Creates training test set based dataframe. Can also stratified (.e., evenly spread given factor) using group argument.","code":""},{"path":"/reference/data_partition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partition data into a test and a training set — data_partition","text":"","code":"data_partition(data, training_proportion = 0.7, group = NULL, seed = NULL, ...)"},{"path":"/reference/data_partition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partition data into a test and a training set — data_partition","text":"data data frame, object can coerced data frame. training_proportion proportion (0 1) training set. remaining part used test set. group character vector indicating name(s) column(s) used stratified partitioning. seed random number generator seed. Enter integer (e.g. 123) random sampling time run function. ... arguments passed functions.","code":""},{"path":"/reference/data_partition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partition data into a test and a training set — data_partition","text":"list two data frames, named test training.","code":""},{"path":"/reference/data_partition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Partition data into a test and a training set — data_partition","text":"","code":"df <- iris df$Smell <- rep(c(\"Strong\", \"Light\"), 75)  data_partition(df) #> $training #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 74           6.1         2.8          4.7         1.2 versicolor  Light #> 23           4.6         3.6          1.0         0.2     setosa Strong #> 53           6.9         3.1          4.9         1.5 versicolor Strong #> 135          6.1         2.6          5.6         1.4  virginica Strong #> 148          6.5         3.0          5.2         2.0  virginica  Light #> 34           5.5         4.2          1.4         0.2     setosa  Light #> 69           6.2         2.2          4.5         1.5 versicolor Strong #> 72           6.1         2.8          4.0         1.3 versicolor  Light #> 76           6.6         3.0          4.4         1.4 versicolor  Light #> 63           6.0         2.2          4.0         1.0 versicolor Strong #> 97           5.7         2.9          4.2         1.3 versicolor Strong #> 91           5.5         2.6          4.4         1.2 versicolor Strong #> 38           4.9         3.6          1.4         0.1     setosa  Light #> 21           5.4         3.4          1.7         0.2     setosa Strong #> 41           5.0         3.5          1.3         0.3     setosa Strong #> 90           5.5         2.5          4.0         1.3 versicolor  Light #> 60           5.2         2.7          3.9         1.4 versicolor  Light #> 16           5.7         4.4          1.5         0.4     setosa  Light #> 116          6.4         3.2          5.3         2.3  virginica  Light #> 94           5.0         2.3          3.3         1.0 versicolor  Light #> 6            5.4         3.9          1.7         0.4     setosa  Light #> 86           6.0         3.4          4.5         1.6 versicolor  Light #> 129          6.4         2.8          5.6         2.1  virginica Strong #> 39           4.4         3.0          1.3         0.2     setosa Strong #> 31           4.8         3.1          1.6         0.2     setosa Strong #> 112          6.4         2.7          5.3         1.9  virginica  Light #> 81           5.5         2.4          3.8         1.1 versicolor Strong #> 118          7.7         3.8          6.7         2.2  virginica  Light #> 50           5.0         3.3          1.4         0.2     setosa  Light #> 113          6.8         3.0          5.5         2.1  virginica Strong #> 145          6.7         3.3          5.7         2.5  virginica Strong #> 4            4.6         3.1          1.5         0.2     setosa  Light #> 13           4.8         3.0          1.4         0.1     setosa Strong #> 144          6.8         3.2          5.9         2.3  virginica  Light #> 115          5.8         2.8          5.1         2.4  virginica Strong #> 25           4.8         3.4          1.9         0.2     setosa Strong #> 52           6.4         3.2          4.5         1.5 versicolor  Light #> 22           5.1         3.7          1.5         0.4     setosa  Light #> 89           5.6         3.0          4.1         1.3 versicolor Strong #> 32           5.4         3.4          1.5         0.4     setosa  Light #> 110          7.2         3.6          6.1         2.5  virginica  Light #> 132          7.9         3.8          6.4         2.0  virginica  Light #> 87           6.7         3.1          4.7         1.5 versicolor Strong #> 35           4.9         3.1          1.5         0.2     setosa Strong #> 40           5.1         3.4          1.5         0.2     setosa  Light #> 30           4.7         3.2          1.6         0.2     setosa  Light #> 12           4.8         3.4          1.6         0.2     setosa  Light #> 126          7.2         3.2          6.0         1.8  virginica  Light #> 105          6.5         3.0          5.8         2.2  virginica Strong #> 64           6.1         2.9          4.7         1.4 versicolor  Light #> 99           5.1         2.5          3.0         1.1 versicolor Strong #> 14           4.3         3.0          1.1         0.1     setosa  Light #> 93           5.8         2.6          4.0         1.2 versicolor Strong #> 96           5.7         3.0          4.2         1.2 versicolor  Light #> 71           5.9         3.2          4.8         1.8 versicolor Strong #> 67           5.6         3.0          4.5         1.5 versicolor Strong #> 149          6.2         3.4          5.4         2.3  virginica Strong #> 79           6.0         2.9          4.5         1.5 versicolor Strong #> 85           5.4         3.0          4.5         1.5 versicolor Strong #> 37           5.5         3.5          1.3         0.2     setosa Strong #> 8            5.0         3.4          1.5         0.2     setosa  Light #> 51           7.0         3.2          4.7         1.4 versicolor Strong #> 150          5.9         3.0          5.1         1.8  virginica  Light #> 122          5.6         2.8          4.9         2.0  virginica  Light #> 88           6.3         2.3          4.4         1.3 versicolor  Light #> 142          6.9         3.1          5.1         2.3  virginica  Light #> 84           6.0         2.7          5.1         1.6 versicolor  Light #> 46           4.8         3.0          1.4         0.3     setosa  Light #> 17           5.4         3.9          1.3         0.4     setosa Strong #> 62           5.9         3.0          4.2         1.5 versicolor  Light #> 83           5.8         2.7          3.9         1.2 versicolor Strong #> 54           5.5         2.3          4.0         1.3 versicolor  Light #> 107          4.9         2.5          4.5         1.7  virginica Strong #> 24           5.1         3.3          1.7         0.5     setosa  Light #> 7            4.6         3.4          1.4         0.3     setosa Strong #> 131          7.4         2.8          6.1         1.9  virginica Strong #> 26           5.0         3.0          1.6         0.2     setosa  Light #> 111          6.5         3.2          5.1         2.0  virginica Strong #> 92           6.1         3.0          4.6         1.4 versicolor  Light #> 27           5.0         3.4          1.6         0.4     setosa Strong #> 42           4.5         2.3          1.3         0.3     setosa  Light #> 5            5.0         3.6          1.4         0.2     setosa Strong #> 133          6.4         2.8          5.6         2.2  virginica Strong #> 77           6.8         2.8          4.8         1.4 versicolor Strong #> 73           6.3         2.5          4.9         1.5 versicolor Strong #> 137          6.3         3.4          5.6         2.4  virginica Strong #> 55           6.5         2.8          4.6         1.5 versicolor Strong #> 11           5.4         3.7          1.5         0.2     setosa Strong #> 36           5.0         3.2          1.2         0.2     setosa  Light #> 44           5.0         3.5          1.6         0.6     setosa  Light #> 80           5.7         2.6          3.5         1.0 versicolor  Light #> 19           5.7         3.8          1.7         0.3     setosa Strong #> 109          6.7         2.5          5.8         1.8  virginica Strong #> 127          6.2         2.8          4.8         1.8  virginica Strong #> 98           6.2         2.9          4.3         1.3 versicolor  Light #> 128          6.1         3.0          4.9         1.8  virginica  Light #> 9            4.4         2.9          1.4         0.2     setosa Strong #> 143          5.8         2.7          5.1         1.9  virginica Strong #> 120          6.0         2.2          5.0         1.5  virginica  Light #> 48           4.6         3.2          1.4         0.2     setosa  Light #> 123          7.7         2.8          6.7         2.0  virginica Strong #> 59           6.6         2.9          4.6         1.3 versicolor Strong #> 47           5.1         3.8          1.6         0.2     setosa Strong #> 57           6.3         3.3          4.7         1.6 versicolor Strong #> 119          7.7         2.6          6.9         2.3  virginica Strong #>  #> $test #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 1            5.1         3.5          1.4         0.2     setosa Strong #> 2            4.9         3.0          1.4         0.2     setosa  Light #> 3            4.7         3.2          1.3         0.2     setosa Strong #> 10           4.9         3.1          1.5         0.1     setosa  Light #> 15           5.8         4.0          1.2         0.2     setosa Strong #> 18           5.1         3.5          1.4         0.3     setosa  Light #> 20           5.1         3.8          1.5         0.3     setosa  Light #> 28           5.2         3.5          1.5         0.2     setosa  Light #> 29           5.2         3.4          1.4         0.2     setosa Strong #> 33           5.2         4.1          1.5         0.1     setosa Strong #> 43           4.4         3.2          1.3         0.2     setosa Strong #> 45           5.1         3.8          1.9         0.4     setosa Strong #> 49           5.3         3.7          1.5         0.2     setosa Strong #> 56           5.7         2.8          4.5         1.3 versicolor  Light #> 58           4.9         2.4          3.3         1.0 versicolor  Light #> 61           5.0         2.0          3.5         1.0 versicolor Strong #> 65           5.6         2.9          3.6         1.3 versicolor Strong #> 66           6.7         3.1          4.4         1.4 versicolor  Light #> 68           5.8         2.7          4.1         1.0 versicolor  Light #> 70           5.6         2.5          3.9         1.1 versicolor  Light #> 75           6.4         2.9          4.3         1.3 versicolor Strong #> 78           6.7         3.0          5.0         1.7 versicolor  Light #> 82           5.5         2.4          3.7         1.0 versicolor  Light #> 95           5.6         2.7          4.2         1.3 versicolor Strong #> 100          5.7         2.8          4.1         1.3 versicolor  Light #> 101          6.3         3.3          6.0         2.5  virginica Strong #> 102          5.8         2.7          5.1         1.9  virginica  Light #> 103          7.1         3.0          5.9         2.1  virginica Strong #> 104          6.3         2.9          5.6         1.8  virginica  Light #> 106          7.6         3.0          6.6         2.1  virginica  Light #> 108          7.3         2.9          6.3         1.8  virginica  Light #> 114          5.7         2.5          5.0         2.0  virginica  Light #> 117          6.5         3.0          5.5         1.8  virginica Strong #> 121          6.9         3.2          5.7         2.3  virginica Strong #> 124          6.3         2.7          4.9         1.8  virginica  Light #> 125          6.7         3.3          5.7         2.1  virginica Strong #> 130          7.2         3.0          5.8         1.6  virginica  Light #> 134          6.3         2.8          5.1         1.5  virginica  Light #> 136          7.7         3.0          6.1         2.3  virginica  Light #> 138          6.4         3.1          5.5         1.8  virginica  Light #> 139          6.0         3.0          4.8         1.8  virginica Strong #> 140          6.9         3.1          5.4         2.1  virginica  Light #> 141          6.7         3.1          5.6         2.4  virginica Strong #> 146          6.7         3.0          5.2         2.3  virginica  Light #> 147          6.3         2.5          5.0         1.9  virginica Strong #>  data_partition(df, group = \"Species\") #> $training #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 1            5.1         3.5          1.4         0.2     setosa Strong #> 40           5.1         3.4          1.5         0.2     setosa  Light #> 30           4.7         3.2          1.6         0.2     setosa  Light #> 48           4.6         3.2          1.4         0.2     setosa  Light #> 25           4.8         3.4          1.9         0.2     setosa Strong #> 16           5.7         4.4          1.5         0.4     setosa  Light #> 24           5.1         3.3          1.7         0.5     setosa  Light #> 11           5.4         3.7          1.5         0.2     setosa Strong #> 20           5.1         3.8          1.5         0.3     setosa  Light #> 49           5.3         3.7          1.5         0.2     setosa Strong #> 3            4.7         3.2          1.3         0.2     setosa Strong #> 29           5.2         3.4          1.4         0.2     setosa Strong #> 36           5.0         3.2          1.2         0.2     setosa  Light #> 22           5.1         3.7          1.5         0.4     setosa  Light #> 42           4.5         2.3          1.3         0.3     setosa  Light #> 43           4.4         3.2          1.3         0.2     setosa Strong #> 8            5.0         3.4          1.5         0.2     setosa  Light #> 21           5.4         3.4          1.7         0.2     setosa Strong #> 13           4.8         3.0          1.4         0.1     setosa Strong #> 2            4.9         3.0          1.4         0.2     setosa  Light #> 35           4.9         3.1          1.5         0.2     setosa Strong #> 32           5.4         3.4          1.5         0.4     setosa  Light #> 14           4.3         3.0          1.1         0.1     setosa  Light #> 6            5.4         3.9          1.7         0.4     setosa  Light #> 46           4.8         3.0          1.4         0.3     setosa  Light #> 34           5.5         4.2          1.4         0.2     setosa  Light #> 12           4.8         3.4          1.6         0.2     setosa  Light #> 4            4.6         3.1          1.5         0.2     setosa  Light #> 39           4.4         3.0          1.3         0.2     setosa Strong #> 28           5.2         3.5          1.5         0.2     setosa  Light #> 45           5.1         3.8          1.9         0.4     setosa Strong #> 50           5.0         3.3          1.4         0.2     setosa  Light #> 26           5.0         3.0          1.6         0.2     setosa  Light #> 18           5.1         3.5          1.4         0.3     setosa  Light #> 10           4.9         3.1          1.5         0.1     setosa  Light #> 75           6.4         2.9          4.3         1.3 versicolor Strong #> 58           4.9         2.4          3.3         1.0 versicolor  Light #> 68           5.8         2.7          4.1         1.0 versicolor  Light #> 59           6.6         2.9          4.6         1.3 versicolor Strong #> 57           6.3         3.3          4.7         1.6 versicolor Strong #> 96           5.7         3.0          4.2         1.2 versicolor  Light #> 60           5.2         2.7          3.9         1.4 versicolor  Light #> 74           6.1         2.8          4.7         1.2 versicolor  Light #> 73           6.3         2.5          4.9         1.5 versicolor Strong #> 76           6.6         3.0          4.4         1.4 versicolor  Light #> 83           5.8         2.7          3.9         1.2 versicolor Strong #> 79           6.0         2.9          4.5         1.5 versicolor Strong #> 94           5.0         2.3          3.3         1.0 versicolor  Light #> 63           6.0         2.2          4.0         1.0 versicolor Strong #> 61           5.0         2.0          3.5         1.0 versicolor Strong #> 100          5.7         2.8          4.1         1.3 versicolor  Light #> 91           5.5         2.6          4.4         1.2 versicolor Strong #> 95           5.6         2.7          4.2         1.3 versicolor Strong #> 85           5.4         3.0          4.5         1.5 versicolor Strong #> 92           6.1         3.0          4.6         1.4 versicolor  Light #> 84           6.0         2.7          5.1         1.6 versicolor  Light #> 70           5.6         2.5          3.9         1.1 versicolor  Light #> 93           5.8         2.6          4.0         1.2 versicolor Strong #> 82           5.5         2.4          3.7         1.0 versicolor  Light #> 97           5.7         2.9          4.2         1.3 versicolor Strong #> 80           5.7         2.6          3.5         1.0 versicolor  Light #> 55           6.5         2.8          4.6         1.5 versicolor Strong #> 81           5.5         2.4          3.8         1.1 versicolor Strong #> 64           6.1         2.9          4.7         1.4 versicolor  Light #> 72           6.1         2.8          4.0         1.3 versicolor  Light #> 56           5.7         2.8          4.5         1.3 versicolor  Light #> 51           7.0         3.2          4.7         1.4 versicolor Strong #> 88           6.3         2.3          4.4         1.3 versicolor  Light #> 67           5.6         3.0          4.5         1.5 versicolor Strong #> 98           6.2         2.9          4.3         1.3 versicolor  Light #> 117          6.5         3.0          5.5         1.8  virginica Strong #> 129          6.4         2.8          5.6         2.1  virginica Strong #> 126          7.2         3.2          6.0         1.8  virginica  Light #> 127          6.2         2.8          4.8         1.8  virginica Strong #> 121          6.9         3.2          5.7         2.3  virginica Strong #> 107          4.9         2.5          4.5         1.7  virginica Strong #> 148          6.5         3.0          5.2         2.0  virginica  Light #> 141          6.7         3.1          5.6         2.4  virginica Strong #> 120          6.0         2.2          5.0         1.5  virginica  Light #> 106          7.6         3.0          6.6         2.1  virginica  Light #> 130          7.2         3.0          5.8         1.6  virginica  Light #> 131          7.4         2.8          6.1         1.9  virginica Strong #> 149          6.2         3.4          5.4         2.3  virginica Strong #> 150          5.9         3.0          5.1         1.8  virginica  Light #> 142          6.9         3.1          5.1         2.3  virginica  Light #> 135          6.1         2.6          5.6         1.4  virginica Strong #> 125          6.7         3.3          5.7         2.1  virginica Strong #> 133          6.4         2.8          5.6         2.2  virginica Strong #> 102          5.8         2.7          5.1         1.9  virginica  Light #> 104          6.3         2.9          5.6         1.8  virginica  Light #> 110          7.2         3.6          6.1         2.5  virginica  Light #> 101          6.3         3.3          6.0         2.5  virginica Strong #> 105          6.5         3.0          5.8         2.2  virginica Strong #> 146          6.7         3.0          5.2         2.3  virginica  Light #> 134          6.3         2.8          5.1         1.5  virginica  Light #> 108          7.3         2.9          6.3         1.8  virginica  Light #> 123          7.7         2.8          6.7         2.0  virginica Strong #> 147          6.3         2.5          5.0         1.9  virginica Strong #> 113          6.8         3.0          5.5         2.1  virginica Strong #> 118          7.7         3.8          6.7         2.2  virginica  Light #> 140          6.9         3.1          5.4         2.1  virginica  Light #> 143          5.8         2.7          5.1         1.9  virginica Strong #> 145          6.7         3.3          5.7         2.5  virginica Strong #> 109          6.7         2.5          5.8         1.8  virginica Strong #> 116          6.4         3.2          5.3         2.3  virginica  Light #>  #> $test #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 5            5.0         3.6          1.4         0.2     setosa Strong #> 7            4.6         3.4          1.4         0.3     setosa Strong #> 9            4.4         2.9          1.4         0.2     setosa Strong #> 15           5.8         4.0          1.2         0.2     setosa Strong #> 17           5.4         3.9          1.3         0.4     setosa Strong #> 19           5.7         3.8          1.7         0.3     setosa Strong #> 23           4.6         3.6          1.0         0.2     setosa Strong #> 27           5.0         3.4          1.6         0.4     setosa Strong #> 31           4.8         3.1          1.6         0.2     setosa Strong #> 33           5.2         4.1          1.5         0.1     setosa Strong #> 37           5.5         3.5          1.3         0.2     setosa Strong #> 38           4.9         3.6          1.4         0.1     setosa  Light #> 41           5.0         3.5          1.3         0.3     setosa Strong #> 44           5.0         3.5          1.6         0.6     setosa  Light #> 47           5.1         3.8          1.6         0.2     setosa Strong #> 52           6.4         3.2          4.5         1.5 versicolor  Light #> 53           6.9         3.1          4.9         1.5 versicolor Strong #> 54           5.5         2.3          4.0         1.3 versicolor  Light #> 62           5.9         3.0          4.2         1.5 versicolor  Light #> 65           5.6         2.9          3.6         1.3 versicolor Strong #> 66           6.7         3.1          4.4         1.4 versicolor  Light #> 69           6.2         2.2          4.5         1.5 versicolor Strong #> 71           5.9         3.2          4.8         1.8 versicolor Strong #> 77           6.8         2.8          4.8         1.4 versicolor Strong #> 78           6.7         3.0          5.0         1.7 versicolor  Light #> 86           6.0         3.4          4.5         1.6 versicolor  Light #> 87           6.7         3.1          4.7         1.5 versicolor Strong #> 89           5.6         3.0          4.1         1.3 versicolor Strong #> 90           5.5         2.5          4.0         1.3 versicolor  Light #> 99           5.1         2.5          3.0         1.1 versicolor Strong #> 103          7.1         3.0          5.9         2.1  virginica Strong #> 111          6.5         3.2          5.1         2.0  virginica Strong #> 112          6.4         2.7          5.3         1.9  virginica  Light #> 114          5.7         2.5          5.0         2.0  virginica  Light #> 115          5.8         2.8          5.1         2.4  virginica Strong #> 119          7.7         2.6          6.9         2.3  virginica Strong #> 122          5.6         2.8          4.9         2.0  virginica  Light #> 124          6.3         2.7          4.9         1.8  virginica  Light #> 128          6.1         3.0          4.9         1.8  virginica  Light #> 132          7.9         3.8          6.4         2.0  virginica  Light #> 136          7.7         3.0          6.1         2.3  virginica  Light #> 137          6.3         3.4          5.6         2.4  virginica Strong #> 138          6.4         3.1          5.5         1.8  virginica  Light #> 139          6.0         3.0          4.8         1.8  virginica Strong #> 144          6.8         3.2          5.9         2.3  virginica  Light #>  data_partition(df, group = c(\"Species\", \"Smell\")) #> $training #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 34           5.5         4.2          1.4         0.2     setosa  Light #> 42           4.5         2.3          1.3         0.3     setosa  Light #> 26           5.0         3.0          1.6         0.2     setosa  Light #> 48           4.6         3.2          1.4         0.2     setosa  Light #> 16           5.7         4.4          1.5         0.4     setosa  Light #> 14           4.3         3.0          1.1         0.1     setosa  Light #> 36           5.0         3.2          1.2         0.2     setosa  Light #> 50           5.0         3.3          1.4         0.2     setosa  Light #> 2            4.9         3.0          1.4         0.2     setosa  Light #> 4            4.6         3.1          1.5         0.2     setosa  Light #> 38           4.9         3.6          1.4         0.1     setosa  Light #> 32           5.4         3.4          1.5         0.4     setosa  Light #> 46           4.8         3.0          1.4         0.3     setosa  Light #> 44           5.0         3.5          1.6         0.6     setosa  Light #> 30           4.7         3.2          1.6         0.2     setosa  Light #> 6            5.4         3.9          1.7         0.4     setosa  Light #> 28           5.2         3.5          1.5         0.2     setosa  Light #> 54           5.5         2.3          4.0         1.3 versicolor  Light #> 60           5.2         2.7          3.9         1.4 versicolor  Light #> 74           6.1         2.8          4.7         1.2 versicolor  Light #> 76           6.6         3.0          4.4         1.4 versicolor  Light #> 70           5.6         2.5          3.9         1.1 versicolor  Light #> 62           5.9         3.0          4.2         1.5 versicolor  Light #> 92           6.1         3.0          4.6         1.4 versicolor  Light #> 88           6.3         2.3          4.4         1.3 versicolor  Light #> 90           5.5         2.5          4.0         1.3 versicolor  Light #> 96           5.7         3.0          4.2         1.2 versicolor  Light #> 56           5.7         2.8          4.5         1.3 versicolor  Light #> 58           4.9         2.4          3.3         1.0 versicolor  Light #> 72           6.1         2.8          4.0         1.3 versicolor  Light #> 94           5.0         2.3          3.3         1.0 versicolor  Light #> 80           5.7         2.6          3.5         1.0 versicolor  Light #> 64           6.1         2.9          4.7         1.4 versicolor  Light #> 82           5.5         2.4          3.7         1.0 versicolor  Light #> 132          7.9         3.8          6.4         2.0  virginica  Light #> 104          6.3         2.9          5.6         1.8  virginica  Light #> 130          7.2         3.0          5.8         1.6  virginica  Light #> 106          7.6         3.0          6.6         2.1  virginica  Light #> 118          7.7         3.8          6.7         2.2  virginica  Light #> 114          5.7         2.5          5.0         2.0  virginica  Light #> 142          6.9         3.1          5.1         2.3  virginica  Light #> 108          7.3         2.9          6.3         1.8  virginica  Light #> 148          6.5         3.0          5.2         2.0  virginica  Light #> 112          6.4         2.7          5.3         1.9  virginica  Light #> 120          6.0         2.2          5.0         1.5  virginica  Light #> 124          6.3         2.7          4.9         1.8  virginica  Light #> 138          6.4         3.1          5.5         1.8  virginica  Light #> 128          6.1         3.0          4.9         1.8  virginica  Light #> 122          5.6         2.8          4.9         2.0  virginica  Light #> 140          6.9         3.1          5.4         2.1  virginica  Light #> 110          7.2         3.6          6.1         2.5  virginica  Light #> 21           5.4         3.4          1.7         0.2     setosa Strong #> 1            5.1         3.5          1.4         0.2     setosa Strong #> 37           5.5         3.5          1.3         0.2     setosa Strong #> 23           4.6         3.6          1.0         0.2     setosa Strong #> 3            4.7         3.2          1.3         0.2     setosa Strong #> 33           5.2         4.1          1.5         0.1     setosa Strong #> 43           4.4         3.2          1.3         0.2     setosa Strong #> 5            5.0         3.6          1.4         0.2     setosa Strong #> 29           5.2         3.4          1.4         0.2     setosa Strong #> 11           5.4         3.7          1.5         0.2     setosa Strong #> 35           4.9         3.1          1.5         0.2     setosa Strong #> 39           4.4         3.0          1.3         0.2     setosa Strong #> 17           5.4         3.9          1.3         0.4     setosa Strong #> 41           5.0         3.5          1.3         0.3     setosa Strong #> 47           5.1         3.8          1.6         0.2     setosa Strong #> 31           4.8         3.1          1.6         0.2     setosa Strong #> 15           5.8         4.0          1.2         0.2     setosa Strong #> 69           6.2         2.2          4.5         1.5 versicolor Strong #> 75           6.4         2.9          4.3         1.3 versicolor Strong #> 89           5.6         3.0          4.1         1.3 versicolor Strong #> 61           5.0         2.0          3.5         1.0 versicolor Strong #> 95           5.6         2.7          4.2         1.3 versicolor Strong #> 91           5.5         2.6          4.4         1.2 versicolor Strong #> 67           5.6         3.0          4.5         1.5 versicolor Strong #> 81           5.5         2.4          3.8         1.1 versicolor Strong #> 65           5.6         2.9          3.6         1.3 versicolor Strong #> 63           6.0         2.2          4.0         1.0 versicolor Strong #> 99           5.1         2.5          3.0         1.1 versicolor Strong #> 51           7.0         3.2          4.7         1.4 versicolor Strong #> 53           6.9         3.1          4.9         1.5 versicolor Strong #> 83           5.8         2.7          3.9         1.2 versicolor Strong #> 55           6.5         2.8          4.6         1.5 versicolor Strong #> 79           6.0         2.9          4.5         1.5 versicolor Strong #> 85           5.4         3.0          4.5         1.5 versicolor Strong #> 121          6.9         3.2          5.7         2.3  virginica Strong #> 133          6.4         2.8          5.6         2.2  virginica Strong #> 123          7.7         2.8          6.7         2.0  virginica Strong #> 103          7.1         3.0          5.9         2.1  virginica Strong #> 137          6.3         3.4          5.6         2.4  virginica Strong #> 125          6.7         3.3          5.7         2.1  virginica Strong #> 139          6.0         3.0          4.8         1.8  virginica Strong #> 119          7.7         2.6          6.9         2.3  virginica Strong #> 131          7.4         2.8          6.1         1.9  virginica Strong #> 109          6.7         2.5          5.8         1.8  virginica Strong #> 107          4.9         2.5          4.5         1.7  virginica Strong #> 143          5.8         2.7          5.1         1.9  virginica Strong #> 101          6.3         3.3          6.0         2.5  virginica Strong #> 145          6.7         3.3          5.7         2.5  virginica Strong #> 141          6.7         3.1          5.6         2.4  virginica Strong #> 147          6.3         2.5          5.0         1.9  virginica Strong #> 127          6.2         2.8          4.8         1.8  virginica Strong #>  #> $test #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 8            5.0         3.4          1.5         0.2     setosa  Light #> 10           4.9         3.1          1.5         0.1     setosa  Light #> 12           4.8         3.4          1.6         0.2     setosa  Light #> 18           5.1         3.5          1.4         0.3     setosa  Light #> 20           5.1         3.8          1.5         0.3     setosa  Light #> 22           5.1         3.7          1.5         0.4     setosa  Light #> 24           5.1         3.3          1.7         0.5     setosa  Light #> 40           5.1         3.4          1.5         0.2     setosa  Light #> 52           6.4         3.2          4.5         1.5 versicolor  Light #> 66           6.7         3.1          4.4         1.4 versicolor  Light #> 68           5.8         2.7          4.1         1.0 versicolor  Light #> 78           6.7         3.0          5.0         1.7 versicolor  Light #> 84           6.0         2.7          5.1         1.6 versicolor  Light #> 86           6.0         3.4          4.5         1.6 versicolor  Light #> 98           6.2         2.9          4.3         1.3 versicolor  Light #> 100          5.7         2.8          4.1         1.3 versicolor  Light #> 102          5.8         2.7          5.1         1.9  virginica  Light #> 116          6.4         3.2          5.3         2.3  virginica  Light #> 126          7.2         3.2          6.0         1.8  virginica  Light #> 134          6.3         2.8          5.1         1.5  virginica  Light #> 136          7.7         3.0          6.1         2.3  virginica  Light #> 144          6.8         3.2          5.9         2.3  virginica  Light #> 146          6.7         3.0          5.2         2.3  virginica  Light #> 150          5.9         3.0          5.1         1.8  virginica  Light #> 7            4.6         3.4          1.4         0.3     setosa Strong #> 9            4.4         2.9          1.4         0.2     setosa Strong #> 13           4.8         3.0          1.4         0.1     setosa Strong #> 19           5.7         3.8          1.7         0.3     setosa Strong #> 25           4.8         3.4          1.9         0.2     setosa Strong #> 27           5.0         3.4          1.6         0.4     setosa Strong #> 45           5.1         3.8          1.9         0.4     setosa Strong #> 49           5.3         3.7          1.5         0.2     setosa Strong #> 57           6.3         3.3          4.7         1.6 versicolor Strong #> 59           6.6         2.9          4.6         1.3 versicolor Strong #> 71           5.9         3.2          4.8         1.8 versicolor Strong #> 73           6.3         2.5          4.9         1.5 versicolor Strong #> 77           6.8         2.8          4.8         1.4 versicolor Strong #> 87           6.7         3.1          4.7         1.5 versicolor Strong #> 93           5.8         2.6          4.0         1.2 versicolor Strong #> 97           5.7         2.9          4.2         1.3 versicolor Strong #> 105          6.5         3.0          5.8         2.2  virginica Strong #> 111          6.5         3.2          5.1         2.0  virginica Strong #> 113          6.8         3.0          5.5         2.1  virginica Strong #> 115          5.8         2.8          5.1         2.4  virginica Strong #> 117          6.5         3.0          5.5         1.8  virginica Strong #> 129          6.4         2.8          5.6         2.1  virginica Strong #> 135          6.1         2.6          5.6         1.4  virginica Strong #> 149          6.2         3.4          5.4         2.3  virginica Strong #>"},{"path":"/reference/data_relocate.html","id":null,"dir":"Reference","previous_headings":"","what":"Relocate (reorder) columns of a data frame — data_relocate","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"Relocate (reorder) columns data frame","code":""},{"path":"/reference/data_relocate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"","code":"data_relocate(data, cols, before = NULL, after = NULL, safe = TRUE, ...)"},{"path":"/reference/data_relocate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"data data frame pivot. cols character vector indicating names columns move. , Destination columns. Supplying neither move columns left-hand side; specifying error. Can character vector, indicating name destination column, numeric value, indicating index number destination column. -1, added last column. safe TRUE, disregard non-existing columns. ... arguments passed functions.","code":""},{"path":"/reference/data_relocate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"data frame reordered columns.","code":""},{"path":"/reference/data_relocate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"","code":"# Reorder columns head(data_relocate(iris, cols = \"Species\", before = \"Sepal.Length\")) #>   Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1  setosa          5.1         3.5          1.4         0.2 #> 2  setosa          4.9         3.0          1.4         0.2 #> 3  setosa          4.7         3.2          1.3         0.2 #> 4  setosa          4.6         3.1          1.5         0.2 #> 5  setosa          5.0         3.6          1.4         0.2 #> 6  setosa          5.4         3.9          1.7         0.4 head(data_relocate(iris, cols = \"Species\", before = \"Sepal.Width\")) #>   Sepal.Length Species Sepal.Width Petal.Length Petal.Width #> 1          5.1  setosa         3.5          1.4         0.2 #> 2          4.9  setosa         3.0          1.4         0.2 #> 3          4.7  setosa         3.2          1.3         0.2 #> 4          4.6  setosa         3.1          1.5         0.2 #> 5          5.0  setosa         3.6          1.4         0.2 #> 6          5.4  setosa         3.9          1.7         0.4 head(data_relocate(iris, cols = \"Sepal.Width\", after = \"Species\")) #>   Sepal.Length Petal.Length Petal.Width Species Sepal.Width #> 1          5.1          1.4         0.2  setosa         3.5 #> 2          4.9          1.4         0.2  setosa         3.0 #> 3          4.7          1.3         0.2  setosa         3.2 #> 4          4.6          1.5         0.2  setosa         3.1 #> 5          5.0          1.4         0.2  setosa         3.6 #> 6          5.4          1.7         0.4  setosa         3.9 # same as head(data_relocate(iris, cols = \"Sepal.Width\", after = -1)) #>   Sepal.Length Petal.Length Petal.Width Species Sepal.Width #> 1          5.1          1.4         0.2  setosa         3.5 #> 2          4.9          1.4         0.2  setosa         3.0 #> 3          4.7          1.3         0.2  setosa         3.2 #> 4          4.6          1.5         0.2  setosa         3.1 #> 5          5.0          1.4         0.2  setosa         3.6 #> 6          5.4          1.7         0.4  setosa         3.9  # reorder multiple columns head(data_relocate(iris, cols = c(\"Species\", \"Petal.Length\"), after = \"Sepal.Width\")) #>   Sepal.Length Sepal.Width Species Petal.Length Petal.Width #> 1          5.1         3.5  setosa          1.4         0.2 #> 2          4.9         3.0  setosa          1.4         0.2 #> 3          4.7         3.2  setosa          1.3         0.2 #> 4          4.6         3.1  setosa          1.5         0.2 #> 5          5.0         3.6  setosa          1.4         0.2 #> 6          5.4         3.9  setosa          1.7         0.4 # same as head(data_relocate(iris, cols = c(\"Species\", \"Petal.Length\"), after = 2)) #>   Sepal.Length Sepal.Width Species Petal.Length Petal.Width #> 1          5.1         3.5  setosa          1.4         0.2 #> 2          4.9         3.0  setosa          1.4         0.2 #> 3          4.7         3.2  setosa          1.3         0.2 #> 4          4.6         3.1  setosa          1.5         0.2 #> 5          5.0         3.6  setosa          1.4         0.2 #> 6          5.4         3.9  setosa          1.7         0.4"},{"path":"/reference/data_rename.html","id":null,"dir":"Reference","previous_headings":"","what":"Convenient dataframe manipulation functionalities — data_addprefix","title":"Convenient dataframe manipulation functionalities — data_addprefix","text":"Safe intuitive functions manipulate dataframes.","code":""},{"path":"/reference/data_rename.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convenient dataframe manipulation functionalities — data_addprefix","text":"","code":"data_addprefix(data, pattern, ...)  data_addsuffix(data, pattern, ...)  data_findcols(data, pattern = NULL, starts_with = NULL, ends_with = NULL, ...)  data_remove(data, pattern, ...)  data_rename(data, pattern = NULL, replacement = NULL, safe = TRUE, ...)  data_rename_rows(data, rows = NULL)  data_reorder(data, cols, safe = TRUE, ...)"},{"path":"/reference/data_rename.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convenient dataframe manipulation functionalities — data_addprefix","text":"data data frame, object can coerced data frame. pattern, replacement, starts_with, ends_with Character strings. ... arguments passed functions. safe throw error instance variable renamed/removed exist. cols, rows Vector column row names.","code":""},{"path":"/reference/data_rename.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convenient dataframe manipulation functionalities — data_addprefix","text":"modified data frame.","code":""},{"path":"/reference/data_rename.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convenient dataframe manipulation functionalities — data_addprefix","text":"","code":"# Add prefix / suffix to all columns head(data_addprefix(iris, \"NEW_\")) #>   NEW_Sepal.Length NEW_Sepal.Width NEW_Petal.Length NEW_Petal.Width NEW_Species #> 1              5.1             3.5              1.4             0.2      setosa #> 2              4.9             3.0              1.4             0.2      setosa #> 3              4.7             3.2              1.3             0.2      setosa #> 4              4.6             3.1              1.5             0.2      setosa #> 5              5.0             3.6              1.4             0.2      setosa #> 6              5.4             3.9              1.7             0.4      setosa head(data_addsuffix(iris, \"_OLD\")) #>   Sepal.Length_OLD Sepal.Width_OLD Petal.Length_OLD Petal.Width_OLD Species_OLD #> 1              5.1             3.5              1.4             0.2      setosa #> 2              4.9             3.0              1.4             0.2      setosa #> 3              4.7             3.2              1.3             0.2      setosa #> 4              4.6             3.1              1.5             0.2      setosa #> 5              5.0             3.6              1.4             0.2      setosa #> 6              5.4             3.9              1.7             0.4      setosa # Find columns names by pattern data_findcols(iris, starts_with = \"Sepal\") #> [1] \"Sepal.Length\" \"Sepal.Width\"  data_findcols(iris, ends_with = \"Width\") #> [1] \"Sepal.Width\" \"Petal.Width\" data_findcols(iris, pattern = \"\\\\.\") #> [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  data_findcols(iris, c(\"Petal.Width\", \"Sepal.Length\")) #> [1] \"Petal.Width\"  \"Sepal.Length\" # Remove columns head(data_remove(iris, \"Sepal.Length\")) #>   Sepal.Width Petal.Length Petal.Width Species #> 1         3.5          1.4         0.2  setosa #> 2         3.0          1.4         0.2  setosa #> 3         3.2          1.3         0.2  setosa #> 4         3.1          1.5         0.2  setosa #> 5         3.6          1.4         0.2  setosa #> 6         3.9          1.7         0.4  setosa # Rename columns head(data_rename(iris, \"Sepal.Length\", \"length\")) #>   length Sepal.Width Petal.Length Petal.Width Species #> 1    5.1         3.5          1.4         0.2  setosa #> 2    4.9         3.0          1.4         0.2  setosa #> 3    4.7         3.2          1.3         0.2  setosa #> 4    4.6         3.1          1.5         0.2  setosa #> 5    5.0         3.6          1.4         0.2  setosa #> 6    5.4         3.9          1.7         0.4  setosa # data_rename(iris, \"FakeCol\", \"length\", safe=FALSE)  # This fails head(data_rename(iris, \"FakeCol\", \"length\")) # This doesn't #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa head(data_rename(iris, c(\"Sepal.Length\", \"Sepal.Width\"), c(\"length\", \"width\"))) #>   length width Petal.Length Petal.Width Species #> 1    5.1   3.5          1.4         0.2  setosa #> 2    4.9   3.0          1.4         0.2  setosa #> 3    4.7   3.2          1.3         0.2  setosa #> 4    4.6   3.1          1.5         0.2  setosa #> 5    5.0   3.6          1.4         0.2  setosa #> 6    5.4   3.9          1.7         0.4  setosa  # Reset names head(data_rename(iris, NULL)) #>     1   2   3   4      5 #> 1 5.1 3.5 1.4 0.2 setosa #> 2 4.9 3.0 1.4 0.2 setosa #> 3 4.7 3.2 1.3 0.2 setosa #> 4 4.6 3.1 1.5 0.2 setosa #> 5 5.0 3.6 1.4 0.2 setosa #> 6 5.4 3.9 1.7 0.4 setosa  # Change all head(data_rename(iris, paste0(\"Var\", 1:5))) #>   Var1 Var2 Var3 Var4   Var5 #> 1  5.1  3.5  1.4  0.2 setosa #> 2  4.9  3.0  1.4  0.2 setosa #> 3  4.7  3.2  1.3  0.2 setosa #> 4  4.6  3.1  1.5  0.2 setosa #> 5  5.0  3.6  1.4  0.2 setosa #> 6  5.4  3.9  1.7  0.4 setosa # Reorder columns head(data_reorder(iris, c(\"Species\", \"Sepal.Length\"))) #>   Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1  setosa          5.1         3.5          1.4         0.2 #> 2  setosa          4.9         3.0          1.4         0.2 #> 3  setosa          4.7         3.2          1.3         0.2 #> 4  setosa          4.6         3.1          1.5         0.2 #> 5  setosa          5.0         3.6          1.4         0.2 #> 6  setosa          5.4         3.9          1.7         0.4 head(data_reorder(iris, c(\"Species\", \"dupa\"))) # Safe for non-existing cols #>   Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1  setosa          5.1         3.5          1.4         0.2 #> 2  setosa          4.9         3.0          1.4         0.2 #> 3  setosa          4.7         3.2          1.3         0.2 #> 4  setosa          4.6         3.1          1.5         0.2 #> 5  setosa          5.0         3.6          1.4         0.2 #> 6  setosa          5.4         3.9          1.7         0.4"},{"path":"/reference/data_rescale.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescale Variables to a New Range — data_rescale","title":"Rescale Variables to a New Range — data_rescale","text":"Rescale variables new range.","code":""},{"path":"/reference/data_rescale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescale Variables to a New Range — data_rescale","text":"","code":"data_rescale(x, ...)  change_scale(x, ...)  # S3 method for numeric data_rescale(x, to = c(0, 100), range = NULL, verbose = TRUE, ...)  # S3 method for grouped_df data_rescale(   x,   to = c(0, 100),   range = NULL,   select = NULL,   exclude = NULL,   ... )  # S3 method for data.frame data_rescale(   x,   to = c(0, 100),   range = NULL,   select = NULL,   exclude = NULL,   ... )"},{"path":"/reference/data_rescale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescale Variables to a New Range — data_rescale","text":"x numeric variable. ... Arguments passed methods. New range variable rescaling. range Initial (old) range values. NULL, take range input vector (range(x)). verbose Toggle warnings messages . select Character vector column names. NULL (default), variables selected. exclude Character vector column names excluded selection.","code":""},{"path":"/reference/data_rescale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescale Variables to a New Range — data_rescale","text":"rescaled object.","code":""},{"path":[]},{"path":"/reference/data_rescale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rescale Variables to a New Range — data_rescale","text":"","code":"data_rescale(c(0, 1, 5, -5, -2)) #> [1]  50  60 100   0  30 data_rescale(c(0, 1, 5, -5, -2), to = c(-5, 5)) #> [1]  0  1  5 -5 -2  # Specify the \"theoretical\" range of the input vector data_rescale(c(1, 3, 4), to = c(0, 40), range = c(0, 4)) #> [1] 10 30 40  # Dataframes head(data_rescale(iris, to = c(0, 1))) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   0.22222222   0.6250000   0.06779661  0.04166667  setosa #> 2   0.16666667   0.4166667   0.06779661  0.04166667  setosa #> 3   0.11111111   0.5000000   0.05084746  0.04166667  setosa #> 4   0.08333333   0.4583333   0.08474576  0.04166667  setosa #> 5   0.19444444   0.6666667   0.06779661  0.04166667  setosa #> 6   0.30555556   0.7916667   0.11864407  0.12500000  setosa head(data_rescale(iris, to = c(0, 1), select = \"Sepal.Length\")) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   0.22222222         3.5          1.4         0.2  setosa #> 2   0.16666667         3.0          1.4         0.2  setosa #> 3   0.11111111         3.2          1.3         0.2  setosa #> 4   0.08333333         3.1          1.5         0.2  setosa #> 5   0.19444444         3.6          1.4         0.2  setosa #> 6   0.30555556         3.9          1.7         0.4  setosa  # One can specify a list of ranges head(data_rescale(iris, to = list(   \"Sepal.Length\" = c(0, 1),   \"Petal.Length\" = c(-1, 0) ))) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   0.22222222         3.5   -0.9322034         0.2  setosa #> 2   0.16666667         3.0   -0.9322034         0.2  setosa #> 3   0.11111111         3.2   -0.9491525         0.2  setosa #> 4   0.08333333         3.1   -0.9152542         0.2  setosa #> 5   0.19444444         3.6   -0.9322034         0.2  setosa #> 6   0.30555556         3.9   -0.8813559         0.4  setosa"},{"path":"/reference/data_restoretype.html","id":null,"dir":"Reference","previous_headings":"","what":"Restore the type of columns according to a reference data frame — data_restoretype","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"Restore type columns according reference data frame","code":""},{"path":"/reference/data_restoretype.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"","code":"data_restoretype(data, reference = NULL, ...)"},{"path":"/reference/data_restoretype.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"data data frame pivot. reference reference data frame find correct column types. ... Additional arguments passed methods.","code":""},{"path":"/reference/data_restoretype.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"dataframe columns whose types restored based reference dataframe.","code":""},{"path":"/reference/data_restoretype.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"","code":"data <- data.frame(   Sepal.Length = c(\"1\", \"3\", \"2\"),   Species = c(\"setosa\", \"versicolor\", \"setosa\"),   New = c(\"1\", \"3\", \"4\") )  fixed <- data_restoretype(data, reference = iris) summary(fixed) #>   Sepal.Length       Species      New            #>  Min.   :1.0   setosa    :2   Length:3           #>  1st Qu.:1.5   versicolor:1   Class :character   #>  Median :2.0   virginica :0   Mode  :character   #>  Mean   :2.0                                     #>  3rd Qu.:2.5                                     #>  Max.   :3.0"},{"path":"/reference/data_rotate.html","id":null,"dir":"Reference","previous_headings":"","what":"Rotate a data frame — data_rotate","title":"Rotate a data frame — data_rotate","text":"function rotates data frame, .e. columns become rows vice versa. equivalent using t() restores data.frame class, preserves attributes prints warning data type modified (see example).","code":""},{"path":"/reference/data_rotate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rotate a data frame — data_rotate","text":"","code":"data_rotate(data, rownames = NULL, colnames = FALSE, verbose = TRUE)  data_transpose(data, rownames = NULL, colnames = FALSE, verbose = TRUE)"},{"path":"/reference/data_rotate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rotate a data frame — data_rotate","text":"data data frame. rownames Character vector (optional). NULL, data frame's rownames added (first) column output, rownames name column. colnames Logical character vector (optional). TRUE, values first column x used column names rotated data frame. character vector, values column used column names. verbose Toggle warnings.","code":""},{"path":"/reference/data_rotate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rotate a data frame — data_rotate","text":"(rotated) data frame.","code":""},{"path":"/reference/data_rotate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rotate a data frame — data_rotate","text":"","code":"x <- mtcars[1:3, 1:4] data_rotate(x) #>      Mazda RX4 Mazda RX4 Wag Datsun 710 #> mpg         21            21       22.8 #> cyl          6             6        4.0 #> disp       160           160      108.0 #> hp         110           110       93.0 data_rotate(x, rownames = \"property\") #>   property Mazda RX4 Mazda RX4 Wag Datsun 710 #> 1      mpg        21            21       22.8 #> 2      cyl         6             6        4.0 #> 3     disp       160           160      108.0 #> 4       hp       110           110       93.0  # use values in 1. column as column name data_rotate(x, colnames = TRUE) #>       21  21 22.8 #> cyl    6   6    4 #> disp 160 160  108 #> hp   110 110   93 data_rotate(x, rownames = \"property\", colnames = TRUE) #>   property  21  21 22.8 #> 1      cyl   6   6    4 #> 2     disp 160 160  108 #> 3       hp 110 110   93  # warn that data types are changed str(data_rotate(iris[1:4, ])) #> Warning: Your data frame contains mixed types of data. After transposition, all variables #>   will be transformed into characters. #> 'data.frame':\t5 obs. of  4 variables: #>  $ 1: chr  \"5.1\" \"3.5\" \"1.4\" \"0.2\" ... #>  $ 2: chr  \"4.9\" \"3.0\" \"1.4\" \"0.2\" ... #>  $ 3: chr  \"4.7\" \"3.2\" \"1.3\" \"0.2\" ... #>  $ 4: chr  \"4.6\" \"3.1\" \"1.5\" \"0.2\" ...  # use either first column or specific column for column names x <- data.frame(a = 1:5, b = 11:15, c = letters[1:5], d = rnorm(5)) data_rotate(x, colnames = TRUE) #> Warning: Your data frame contains mixed types of data. After transposition, all variables #>   will be transformed into characters. #>            1          2          3          4          5 #> b         11         12         13         14         15 #> c          a          b          c          d          e #> d  2.4544004 -1.6493455  0.7958052 -0.3412932 -1.4052538 data_rotate(x, colnames = \"c\") #> Warning: Your data frame contains mixed types of data. After transposition, all variables #>   will be transformed into characters. #>         a         b          c          d         e #> a  1.0000  2.000000  3.0000000  4.0000000  5.000000 #> b 11.0000 12.000000 13.0000000 14.0000000 15.000000 #> d  2.4544 -1.649345  0.7958052 -0.3412932 -1.405254"},{"path":"/reference/data_to_long.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape (pivot) data from wide to long — data_to_long","title":"Reshape (pivot) data from wide to long — data_to_long","text":"function \"lengthens\" data, increasing number rows decreasing number columns. dependency-free base-R equivalent tidyr::pivot_longer().","code":""},{"path":"/reference/data_to_long.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape (pivot) data from wide to long — data_to_long","text":"","code":"data_to_long(   data,   cols = \"all\",   colnames_to = \"Name\",   values_to = \"Value\",   rows_to = NULL,   ...,   names_to = colnames_to )  data_to_wide(   data,   values_from = \"Value\",   colnames_from = \"Name\",   rows_from = NULL,   sep = \"_\",   ...,   names_from = colnames_from )  reshape_longer(   data,   cols = \"all\",   colnames_to = \"Name\",   values_to = \"Value\",   rows_to = NULL,   ...,   names_to = colnames_to )  reshape_wider(   data,   values_from = \"Value\",   colnames_from = \"Name\",   rows_from = NULL,   sep = \"_\",   ...,   names_from = colnames_from )"},{"path":"/reference/data_to_long.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape (pivot) data from wide to long — data_to_long","text":"data data frame pivot. cols vector column names indices pivot longer format. colnames_to name new column contain column names. values_to name new column contain values pivoted variables. rows_to name column contain row-number original data. NULL, removed. ... Additional arguments passed methods. names_to, names_from colnames_to, compatibility tidyr::pivot_longer(). values_from name column contains values put columns. colnames_from name column contains levels used future columns. rows_from name column identifies rows. NULL, use unique rows. sep indicating separating character variable names wide format.","code":""},{"path":"/reference/data_to_long.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape (pivot) data from wide to long — data_to_long","text":"data.frame","code":""},{"path":"/reference/data_to_long.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reshape (pivot) data from wide to long — data_to_long","text":"","code":"wide_data <- data.frame(replicate(5, rnorm(10)))  # From wide to long # ------------------ # Default behaviour (equivalent to tidyr::pivot_longer(wide_data, cols = 1:5)) data_to_long(wide_data) #>    Name       Value #> 1    X1 -0.92296082 #> 2    X2  0.46494175 #> 3    X3  1.80105086 #> 4    X4  0.39550987 #> 5    X5 -1.03071105 #> 6    X1  0.00989813 #> 7    X2 -1.95103061 #> 8    X3  1.05533859 #> 9    X4  0.88642168 #> 10   X5  0.43083317 #> 11   X1 -0.40885977 #> 12   X2 -0.51611334 #> 13   X3 -0.29192076 #> 14   X4  1.20104255 #> 15   X5  0.20890740 #> 16   X1 -1.38824529 #> 17   X2  0.48908032 #> 18   X3 -0.82410938 #> 19   X4  1.31680568 #> 20   X5 -1.54824026 #> 21   X1 -0.26459524 #> 22   X2  0.90202473 #> 23   X3  1.21625024 #> 24   X4 -0.19304042 #> 25   X5 -1.17380222 #> 26   X1 -0.94729828 #> 27   X2  0.64036299 #> 28   X3  0.21972840 #> 29   X4  0.55057625 #> 30   X5  1.17881520 #> 31   X1  0.73952133 #> 32   X2  0.95120888 #> 33   X3  0.07499953 #> 34   X4 -2.08877294 #> 35   X5 -0.41930944 #> 36   X1  0.89677871 #> 37   X2 -0.59912321 #> 38   X3 -2.14041848 #> 39   X4 -0.02490979 #> 40   X5 -1.85257530 #> 41   X1 -0.34600089 #> 42   X2 -1.33069499 #> 43   X3  1.47097224 #> 44   X4  0.88845765 #> 45   X5  0.48592647 #> 46   X1 -1.78205707 #> 47   X2 -0.59220975 #> 48   X3 -0.83776761 #> 49   X4 -0.33743115 #> 50   X5 -0.47191259  # Customizing the names data_to_long(wide_data,   cols = c(1, 2),   colnames_to = \"Column\",   values_to = \"Numbers\",   rows_to = \"Row\" ) #>             X3          X4         X5 Row Column     Numbers #> 1   1.80105086  0.39550987 -1.0307111   1     X1 -0.92296082 #> 2   1.80105086  0.39550987 -1.0307111   1     X2  0.46494175 #> 3   1.05533859  0.88642168  0.4308332   2     X1  0.00989813 #> 4   1.05533859  0.88642168  0.4308332   2     X2 -1.95103061 #> 5  -0.29192076  1.20104255  0.2089074   3     X1 -0.40885977 #> 6  -0.29192076  1.20104255  0.2089074   3     X2 -0.51611334 #> 7  -0.82410938  1.31680568 -1.5482403   4     X1 -1.38824529 #> 8  -0.82410938  1.31680568 -1.5482403   4     X2  0.48908032 #> 9   1.21625024 -0.19304042 -1.1738022   5     X1 -0.26459524 #> 10  1.21625024 -0.19304042 -1.1738022   5     X2  0.90202473 #> 11  0.21972840  0.55057625  1.1788152   6     X1 -0.94729828 #> 12  0.21972840  0.55057625  1.1788152   6     X2  0.64036299 #> 13  0.07499953 -2.08877294 -0.4193094   7     X1  0.73952133 #> 14  0.07499953 -2.08877294 -0.4193094   7     X2  0.95120888 #> 15 -2.14041848 -0.02490979 -1.8525753   8     X1  0.89677871 #> 16 -2.14041848 -0.02490979 -1.8525753   8     X2 -0.59912321 #> 17  1.47097224  0.88845765  0.4859265   9     X1 -0.34600089 #> 18  1.47097224  0.88845765  0.4859265   9     X2 -1.33069499 #> 19 -0.83776761 -0.33743115 -0.4719126  10     X1 -1.78205707 #> 20 -0.83776761 -0.33743115 -0.4719126  10     X2 -0.59220975  # From long to wide # ----------------- long_data <- data_to_long(wide_data, rows_to = \"Row_ID\") # Save row number data_to_wide(long_data,   colnames_from = \"Name\",   values_from = \"Value\",   rows_from = \"Row_ID\" ) #>    Row_ID    Value_X1   Value_X2    Value_X3    Value_X4   Value_X5 #> 1       1 -0.92296082  0.4649418  1.80105086  0.39550987 -1.0307111 #> 2       2  0.00989813 -1.9510306  1.05533859  0.88642168  0.4308332 #> 3       3 -0.40885977 -0.5161133 -0.29192076  1.20104255  0.2089074 #> 4       4 -1.38824529  0.4890803 -0.82410938  1.31680568 -1.5482403 #> 5       5 -0.26459524  0.9020247  1.21625024 -0.19304042 -1.1738022 #> 6       6 -0.94729828  0.6403630  0.21972840  0.55057625  1.1788152 #> 7       7  0.73952133  0.9512089  0.07499953 -2.08877294 -0.4193094 #> 8       8  0.89677871 -0.5991232 -2.14041848 -0.02490979 -1.8525753 #> 9       9 -0.34600089 -1.3306950  1.47097224  0.88845765  0.4859265 #> 10     10 -1.78205707 -0.5922097 -0.83776761 -0.33743115 -0.4719126  # Full example # ------------------ if (require(\"psych\")) {   data <- psych::bfi # Wide format with one row per participant's personality test    # Pivot long format   long <- data_to_long(data,     cols = \"\\\\d\", # Select all columns that contain a digit     colnames_to = \"Item\",     values_to = \"Score\",     rows_to = \"Participant\"   )    # Separate facet and question number   long$Facet <- gsub(\"\\\\d\", \"\", long$Item)   long$Item <- gsub(\"[A-Z]\", \"\", long$Item)   long$Item <- paste0(\"I\", long$Item)    wide <- data_to_wide(long,     colnames_from = \"Item\",     values_from = \"Score\"   )   head(wide) } #> Loading required package: psych #>   gender education age Participant Facet Score_I1 Score_I2 Score_I3 Score_I4 #> 1      1        NA  16       61617     A        2        4        3        4 #> 2      1        NA  16       61617     C        2        3        3        4 #> 3      1        NA  16       61617     E        3        3        3        4 #> 4      1        NA  16       61617     N        3        4        2        2 #> 5      1        NA  16       61617     O        3        6        3        4 #> 6      2        NA  18       61618     A        2        4        5        2 #>   Score_I5 #> 1        4 #> 2        4 #> 3        4 #> 4        3 #> 5        3 #> 6        5"},{"path":"/reference/demean.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute group-meaned and de-meaned variables — demean","title":"Compute group-meaned and de-meaned variables — demean","text":"demean() computes group- de-meaned versions variable can used regression analysis model - within-subject effect. degroup() generic terms centering-operation. demean() always uses mean-centering, degroup() can also use mode median centering.","code":""},{"path":"/reference/demean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute group-meaned and de-meaned variables — demean","text":"","code":"demean(   x,   select,   group,   suffix_demean = \"_within\",   suffix_groupmean = \"_between\",   add_attributes = TRUE,   verbose = TRUE )  degroup(   x,   select,   group,   center = \"mean\",   suffix_demean = \"_within\",   suffix_groupmean = \"_between\",   add_attributes = TRUE,   verbose = TRUE )  detrend(   x,   select,   group,   center = \"mean\",   suffix_demean = \"_within\",   suffix_groupmean = \"_between\",   add_attributes = TRUE,   verbose = TRUE )"},{"path":"/reference/demean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute group-meaned and de-meaned variables — demean","text":"x data frame. select Character vector (formula) names variables select group- de-meaned. group Character vector (formula) name variable indicates group- cluster-ID. suffix_demean, suffix_groupmean String value, appended names group-meaned de-meaned variables x. default, de-meaned variables suffixed \"_within\" grouped-meaned variables \"_between\". add_attributes Logical, TRUE, returned variables gain attributes indicate within- -effects. relevant printing model_parameters() - cases, within- -effects printed separated blocks. verbose Toggle warnings messages. center Method centering. demean() always performs mean-centering, degroup() can use center = \"median\" center = \"mode\" median- mode-centering, also \"min\" \"max\".","code":""},{"path":"/reference/demean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute group-meaned and de-meaned variables — demean","text":"data frame group-/de-meaned variables, get suffix \"_between\" (group-meaned variable) \"_within\" (de-meaned variable) default.","code":""},{"path":[]},{"path":"/reference/demean.html","id":"heterogeneity-bias","dir":"Reference","previous_headings":"","what":"Heterogeneity Bias","title":"Compute group-meaned and de-meaned variables — demean","text":"Mixed models include different levels sources variability, .e. error terms level. macro-indicators (level-2 predictors, higher-level units, general: group-level predictors vary within across groups) included fixed effects (.e. treated covariate level-1), variance left unaccounted covariate absorbed error terms level-1 level-2 (Bafumi Gelman 2006; Gelman Hill 2007, Chapter 12.6.): “covariates contain two parts: one specific higher-level entity vary occasions, one represents difference occasions, within higher-level entities” (Bell et al. 2015). Hence, error terms correlated covariate, violates one assumptions mixed models (iid, independent identically distributed error terms). bias also called heterogeneity bias (Bell et al. 2015). resolve problem, level-2 predictors used (level-1) covariates separated \"within\" \"\" effects \"de-meaning\" \"group-meaning\": demeaning time-varying predictors, “higher level, mean term longer constrained Level 1 effects, free account higher-level variance associated variable” (Bell et al. 2015).","code":""},{"path":"/reference/demean.html","id":"panel-data-and-correlating-fixed-and-group-effects","dir":"Reference","previous_headings":"","what":"Panel data and correlating fixed and group effects","title":"Compute group-meaned and de-meaned variables — demean","text":"demean() intended create group- de-meaned variables panel regression models (fixed effects models), complex random-effect-within-models (see Bell et al. 2015, 2018), group-effects (random effects) fixed effects correlate (see Bafumi Gelman 2006). can happen, instance, analyzing panel data, can lead Heterogeneity Bias. control correlating predictors group effects, recommended include group-meaned de-meaned version time-varying covariates (group-meaned version time-invariant covariates higher level, e.g. level-2 predictors) model. , one can fit complex multilevel models panel data, including time-varying predictors, time-invariant predictors random effects.","code":""},{"path":"/reference/demean.html","id":"why-mixed-models-are-preferred-over-fixed-effects-models","dir":"Reference","previous_headings":"","what":"Why mixed models are preferred over fixed effects models","title":"Compute group-meaned and de-meaned variables — demean","text":"mixed models approach can model causes endogeneity explicitly including (separated) within- -effects time-varying fixed effects including time-constant fixed effects. Furthermore, mixed models also include random effects, thus mixed models approach superior classic fixed-effects models, lack information variation group-effects -subject effects. Furthermore, fixed effects regression include random slopes, means fixed effects regressions neglecting “cross-cluster differences effects lower-level controls () reduces precision estimated context effects, resulting unnecessarily wide confidence intervals low statistical power” (Heisig et al. 2017).","code":""},{"path":"/reference/demean.html","id":"terminology","dir":"Reference","previous_headings":"","what":"Terminology","title":"Compute group-meaned and de-meaned variables — demean","text":"group-meaned variable simply mean independent variable within group (id-level cluster) represented group. represents cluster-mean independent variable. regression coefficient group-meaned variable -subject-effect. de-meaned variable centered version group-meaned variable. De-meaning sometimes also called person-mean centering centering within clusters. regression coefficient de-meaned variable represents within-subject-effect.","code":""},{"path":"/reference/demean.html","id":"de-meaning-with-continuous-predictors","dir":"Reference","previous_headings":"","what":"De-meaning with continuous predictors","title":"Compute group-meaned and de-meaned variables — demean","text":"continuous time-varying predictors, recommendation include de-meaned group-meaned versions fixed effects, raw (untransformed) time-varying predictors . de-meaned predictor also included random effect (random slope). regression models, coefficient de-meaned predictors indicates within-subject effect, coefficient group-meaned predictor indicates -subject effect.","code":""},{"path":"/reference/demean.html","id":"de-meaning-with-binary-predictors","dir":"Reference","previous_headings":"","what":"De-meaning with binary predictors","title":"Compute group-meaned and de-meaned variables — demean","text":"binary time-varying predictors, two recommendations. First include raw (untransformed) binary predictor fixed effect de-meaned variable random effect (random slope). alternative add de-meaned version(s) binary time-varying covariates additional fixed effect well (instead adding random slope). Centering time-varying binary variables obtain within-effects (level 1) necessary. sensible interpretation left typical 0/1 format (Hoffmann 2015, chapter 8-2.). demean() thus coerce categorical time-varying predictors numeric compute de- group-meaned versions variables, raw (untransformed) binary predictor de-meaned version added model.","code":""},{"path":"/reference/demean.html","id":"de-meaning-of-factors-with-more-than-levels","dir":"Reference","previous_headings":"","what":"De-meaning of factors with more than 2 levels","title":"Compute group-meaned and de-meaned variables — demean","text":"Factors two levels demeaned two ways: first, also converted numeric de-meaned; second, dummy variables created (binary, 0/1 coding level) binary dummy-variables de-meaned way (described ). Packages like panelr internally convert factors dummies demeaning, behaviour can mimicked .","code":""},{"path":"/reference/demean.html","id":"de-meaning-interaction-terms","dir":"Reference","previous_headings":"","what":"De-meaning interaction terms","title":"Compute group-meaned and de-meaned variables — demean","text":"multiple ways deal interaction terms within- -effects. classical approach simply use product term de-meaned variables (.e. introducing de-meaned variables interaction term model formula, e.g. y ~ x_within * time_within). approach, however, might subject bias (see Giesselmann & Schmidt-Catran 2020).  Another option first calculate product term apply de-meaning . approach produces estimator “reflects unit-level differences interacted variables whose moderators vary within units”, desirable within interaction two time-dependent variables required.  third option, interaction result genuine within estimator, \"double de-mean\" interaction terms (Giesselmann & Schmidt-Catran 2018), however, currently supported demean(). required, wmb() function panelr package used.  de-mean interaction terms within-models, simply specify term interaction select-argument, e.g. select = \"*b\" (see 'Examples').","code":""},{"path":"/reference/demean.html","id":"analysing-panel-data-with-mixed-models-using-lme-","dir":"Reference","previous_headings":"","what":"Analysing panel data with mixed models using lme4","title":"Compute group-meaned and de-meaned variables — demean","text":"description translate formulas described Bell et al. 2018 R using lmer() lme4 can found vignette.","code":""},{"path":"/reference/demean.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute group-meaned and de-meaned variables — demean","text":"Bafumi J, Gelman . 2006. Fitting Multilevel Models Predictors Group Effects Correlate. . Philadelphia, PA: Annual meeting American Political Science Association. Bell , Fairbrother M, Jones K. 2019. Fixed Random Effects Models: Making Informed Choice. Quality & Quantity (53); 1051-1074 Bell , Jones K. 2015. Explaining Fixed Effects: Random Effects Modeling Time-Series Cross-Sectional Panel Data. Political Science Research Methods, 3(1), 133–153. Gelman , Hill J. 2007. Data Analysis Using Regression Multilevel/Hierarchical Models. Analytical Methods Social Research. Cambridge, New York: Cambridge University Press Giesselmann M, Schmidt-Catran, AW. 2020. Interactions fixed effects regression models. Sociological Methods & Research, 1–28. https://doi.org/10.1177/0049124120914934 Heisig JP, Schaeffer M, Giesecke J. 2017. Costs Simplicity: Multilevel Models May Benefit Accounting Cross-Cluster Differences Effects Controls. American Sociological Review 82 (4): 796–827. Hoffman L. 2015. Longitudinal analysis: modeling within-person fluctuation change. New York: Routledge","code":""},{"path":[]},{"path":"/reference/demean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute group-meaned and de-meaned variables — demean","text":"","code":"data(iris) iris$ID <- sample(1:4, nrow(iris), replace = TRUE) # fake-ID iris$binary <- as.factor(rbinom(150, 1, .35)) # binary variable  x <- demean(iris, select = c(\"Sepal.Length\", \"Petal.Length\"), group = \"ID\") head(x) #>   Sepal.Length_between Petal.Length_between Sepal.Length_within #> 1             5.900000             3.802273          -0.8000000 #> 2             5.900000             3.802273          -1.0000000 #> 3             5.743590             3.579487          -1.0435897 #> 4             5.734375             3.812500          -1.1343750 #> 5             5.743590             3.579487          -0.7435897 #> 6             5.900000             3.802273          -0.5000000 #>   Petal.Length_within #> 1           -2.402273 #> 2           -2.402273 #> 3           -2.279487 #> 4           -2.312500 #> 5           -2.179487 #> 6           -2.102273  x <- demean(iris, select = c(\"Sepal.Length\", \"binary\", \"Species\"), group = \"ID\") #> Categorical predictors (Species, binary) have been coerced to numeric values to compute de- and group-meaned variables. #>  head(x) #>   Sepal.Length_between Species_between binary_between Species_setosa_between #> 1             5.900000       0.9772727      0.3636364              0.3409091 #> 2             5.900000       0.9772727      0.3636364              0.3409091 #> 3             5.743590       0.9230769      0.4102564              0.3333333 #> 4             5.734375       1.1250000      0.2812500              0.3125000 #> 5             5.743590       0.9230769      0.4102564              0.3333333 #> 6             5.900000       0.9772727      0.3636364              0.3409091 #>   Species_versicolor_between Species_virginica_between Sepal.Length_within #> 1                  0.3409091                 0.3181818          -0.8000000 #> 2                  0.3409091                 0.3181818          -1.0000000 #> 3                  0.4102564                 0.2564103          -1.0435897 #> 4                  0.2500000                 0.4375000          -1.1343750 #> 5                  0.4102564                 0.2564103          -0.7435897 #> 6                  0.3409091                 0.3181818          -0.5000000 #>   Species_within binary_within Species_setosa_within Species_versicolor_within #> 1     -0.9772727    -0.3636364             0.6590909                -0.3409091 #> 2     -0.9772727    -0.3636364             0.6590909                -0.3409091 #> 3     -0.9230769     0.5897436             0.6666667                -0.4102564 #> 4     -1.1250000    -0.2812500             0.6875000                -0.2500000 #> 5     -0.9230769    -0.4102564             0.6666667                -0.4102564 #> 6     -0.9772727    -0.3636364             0.6590909                -0.3409091 #>   Species_virginica_within #> 1               -0.3181818 #> 2               -0.3181818 #> 3               -0.2564103 #> 4               -0.4375000 #> 5               -0.2564103 #> 6               -0.3181818   # demean interaction term x*y dat <- data.frame(   a = c(1, 2, 3, 4, 1, 2, 3, 4),   x = c(4, 3, 3, 4, 1, 2, 1, 2),   y = c(1, 2, 1, 2, 4, 3, 2, 1),   ID = c(1, 2, 3, 1, 2, 3, 1, 2) ) demean(dat, select = c(\"a\", \"x*y\"), group = \"ID\") #>   a_between x_y_between   a_within x_y_within #> 1  2.666667    4.666667 -1.6666667 -0.6666667 #> 2  2.333333    4.000000 -0.3333333  2.0000000 #> 3  2.500000    4.500000  0.5000000 -1.5000000 #> 4  2.666667    4.666667  1.3333333  3.3333333 #> 5  2.333333    4.000000 -1.3333333  0.0000000 #> 6  2.500000    4.500000 -0.5000000  1.5000000 #> 7  2.666667    4.666667  0.3333333 -2.6666667 #> 8  2.333333    4.000000  1.6666667 -2.0000000  # or in formula-notation demean(dat, select = ~ a + x * y, group = ~ID) #>   a_between x_y_between   a_within x_y_within #> 1  2.666667    4.666667 -1.6666667 -0.6666667 #> 2  2.333333    4.000000 -0.3333333  2.0000000 #> 3  2.500000    4.500000  0.5000000 -1.5000000 #> 4  2.666667    4.666667  1.3333333  3.3333333 #> 5  2.333333    4.000000 -1.3333333  0.0000000 #> 6  2.500000    4.500000 -0.5000000  1.5000000 #> 7  2.666667    4.666667  0.3333333 -2.6666667 #> 8  2.333333    4.000000  1.6666667 -2.0000000"},{"path":"/reference/describe_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Describe a distribution — describe_distribution","title":"Describe a distribution — describe_distribution","text":"function describes distribution set indices (e.g., measures centrality, dispersion, range, skewness, kurtosis).","code":""},{"path":"/reference/describe_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Describe a distribution — describe_distribution","text":"","code":"describe_distribution(x, ...)  # S3 method for numeric describe_distribution(   x,   centrality = \"mean\",   dispersion = TRUE,   iqr = TRUE,   range = TRUE,   quartiles = FALSE,   ci = NULL,   iterations = 100,   threshold = 0.1,   verbose = TRUE,   ... )  # S3 method for factor describe_distribution(x, dispersion = TRUE, range = TRUE, verbose = TRUE, ...)  # S3 method for data.frame describe_distribution(   x,   centrality = \"mean\",   dispersion = TRUE,   iqr = TRUE,   range = TRUE,   quartiles = FALSE,   include_factors = FALSE,   ci = NULL,   iterations = 100,   threshold = 0.1,   verbose = TRUE,   ... )"},{"path":"/reference/describe_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Describe a distribution — describe_distribution","text":"x numeric vector. ... Additional arguments passed methods. centrality point-estimates (centrality indices) compute.  Character (vector) list one options: \"median\", \"mean\", \"MAP\" \"\". dispersion Logical, TRUE, computes indices dispersion related estimate(s) (SD MAD mean median, respectively). iqr Logical, TRUE, interquartile range calculated (based stats::IQR(), using type = 6). range Return range (min max). quartiles Return first third quartiles (25th 75pth percentiles). ci Confidence Interval (CI) level. Default NULL, .e. confidence intervals computed. NULL, confidence intervals based bootstrap replicates (see iterations). centrality = \"\", bootstrapped confidence interval refers first centrality index (typically median). iterations number bootstrap replicates computing confidence intervals. applies ci NULL. threshold centrality = \"trimmed\" (.e. trimmed mean), indicates fraction (0 0.5) observations trimmed end vector mean computed. verbose Toggle warnings messages. include_factors Logical, TRUE, factors included output, however, columns range (first last factor levels) well n missing contain information.","code":""},{"path":"/reference/describe_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Describe a distribution — describe_distribution","text":"data frame columns describe properties variables.","code":""},{"path":"/reference/describe_distribution.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Describe a distribution — describe_distribution","text":"also plot()-method implemented see-package.","code":""},{"path":"/reference/describe_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Describe a distribution — describe_distribution","text":"","code":"describe_distribution(rnorm(100)) #> Mean  |   SD | IQR |  Min | Max | Skewness | Kurtosis |   n | n_Missing #> ----------------------------------------------------------------------- #> -0.22 | 0.92 | 1.3 | -2.5 | 2.4 |   -0.041 |    0.072 | 100 |         0  data(iris) describe_distribution(iris) #> Variable     | Mean |   SD |  IQR | Min | Max | Skewness | Kurtosis |   n | n_Missing #> ------------------------------------------------------------------------------------- #> Sepal.Length |  5.8 | 0.83 | 1.30 | 4.3 | 7.9 |     0.31 |    -0.55 | 150 |         0 #> Sepal.Width  |  3.1 | 0.44 | 0.52 | 2.0 | 4.4 |     0.32 |     0.23 | 150 |         0 #> Petal.Length |  3.8 | 1.77 | 3.52 | 1.0 | 6.9 |    -0.27 |    -1.40 | 150 |         0 #> Petal.Width  |  1.2 | 0.76 | 1.50 | 0.1 | 2.5 |    -0.10 |    -1.34 | 150 |         0 describe_distribution(iris, include_factors = TRUE, quartiles = TRUE) #> Variable     | Mean |   SD |  IQR |    Min |       Max |  Q1 |  Q3 | Skewness | Kurtosis |   n | n_Missing #> ---------------------------------------------------------------------------------------------------------- #> Sepal.Length |  5.8 | 0.83 | 1.30 |    4.3 |       7.9 | 5.1 | 6.4 |     0.31 |    -0.55 | 150 |         0 #> Sepal.Width  |  3.1 | 0.44 | 0.52 |      2 |       4.4 | 2.8 | 3.3 |     0.32 |     0.23 | 150 |         0 #> Petal.Length |  3.8 | 1.77 | 3.52 |      1 |       6.9 | 1.6 | 5.1 |    -0.27 |    -1.40 | 150 |         0 #> Petal.Width  |  1.2 | 0.76 | 1.50 |    0.1 |       2.5 | 0.3 | 1.8 |    -0.10 |    -1.34 | 150 |         0 #> Species      |   NA |   NA |   NA | setosa | virginica |  NA |  NA |     0.00 |    -1.51 | 150 |         0"},{"path":"/reference/format_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Convenient text formatting functionalities — format_text","title":"Convenient text formatting functionalities — format_text","text":"Convenience functions manipulate format text.","code":""},{"path":"/reference/format_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convenient text formatting functionalities — format_text","text":"","code":"format_text(text, sep = \", \", last = \" and \", width = NULL, ...)  text_fullstop(text)  text_lastchar(text, n = 1)  text_concatenate(text, sep = \", \", last = \" and \")  text_paste(text, text2 = NULL, sep = \", \", ...)  text_remove(text, pattern = \"\", ...)  text_wrap(text, width = NULL, ...)"},{"path":"/reference/format_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convenient text formatting functionalities — format_text","text":"text, text2 character string. sep Separator. last Last separator. width Positive integer giving target column width wrapping lines output. Can \"auto\", case select 90\\ default width. ... arguments passed functions. n number characters find. pattern Character strings.","code":""},{"path":"/reference/format_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convenient text formatting functionalities — format_text","text":"character string.","code":""},{"path":"/reference/format_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convenient text formatting functionalities — format_text","text":"","code":"# Add full stop if missing text_fullstop(c(\"something\", \"something else.\")) #> [1] \"something.\"      \"something else.\"  # Find last characters text_lastchar(c(\"ABC\", \"DEF\"), n = 2) #>  ABC  DEF  #> \"BC\" \"EF\"   # Smart concatenation text_concatenate(c(\"First\", \"Second\", \"Last\")) #> [1] \"First, Second and Last\"  # Remove parts of string text_remove(c(\"one!\", \"two\", \"three!\"), \"!\") #> [1] \"one\"   \"two\"   \"three\"  # Wrap text long_text <- paste(rep(\"abc \", 100), collapse = \"\") cat(text_wrap(long_text, width = 50)) #>  abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc  # Paste with optional separator text_paste(c(\"A\", \"\", \"B\"), c(\"42\", \"42\", \"42\")) #> [1] \"A, 42\" \"42\"    \"B, 42\""},{"path":"/reference/is_empty_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if object is empty — is_empty_object","title":"Check if object is empty — is_empty_object","text":"Check object empty","code":""},{"path":"/reference/is_empty_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if object is empty — is_empty_object","text":"","code":"is_empty_object(x)"},{"path":"/reference/is_empty_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if object is empty — is_empty_object","text":"x list, vector, dataframe.","code":""},{"path":"/reference/is_empty_object.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if object is empty — is_empty_object","text":"logical indicating whether entered object empty.","code":""},{"path":"/reference/is_empty_object.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if object is empty — is_empty_object","text":"","code":"is_empty_object(c(1, 2, 3, NA)) #> [1] FALSE is_empty_object(list(NULL, c(NA, NA))) #> [1] FALSE is_empty_object(list(NULL, NA)) #> [1] FALSE"},{"path":"/reference/nhanes_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample dataset from the National Health and Nutrition Examination Survey — nhanes_sample","title":"Sample dataset from the National Health and Nutrition Examination Survey — nhanes_sample","text":"Selected variables National Health Nutrition Examination Survey used example Lumley (2010), Appendix E.","code":""},{"path":"/reference/nhanes_sample.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sample dataset from the National Health and Nutrition Examination Survey — nhanes_sample","text":"Lumley T (2010). Complex Surveys: guide analysis using R. Wiley","code":""},{"path":"/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize numeric variable to 0-1 range — normalize","title":"Normalize numeric variable to 0-1 range — normalize","text":"Performs normalization data, .e., scales variables range 0 - special case data_rescale().","code":""},{"path":"/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize numeric variable to 0-1 range — normalize","text":"","code":"normalize(x, ...)  # S3 method for numeric normalize(x, include_bounds = TRUE, verbose = TRUE, ...)  # S3 method for grouped_df normalize(   x,   select = NULL,   exclude = NULL,   include_bounds = TRUE,   verbose = TRUE,   ... )  # S3 method for data.frame normalize(   x,   select = NULL,   exclude = NULL,   include_bounds = TRUE,   verbose = TRUE,   ... )"},{"path":"/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize numeric variable to 0-1 range — normalize","text":"x numeric vector, data frame, matrix. See details. ... Arguments passed methods. include_bounds Logical, TRUE, return value may include 0 1. FALSE, return value compressed, using Smithson Verkuilen's (2006) formula (x * (n - 1) + 0.5) / n, avoid zeros ones normalized variables. can useful case beta-regression, response variable allowed include zeros ones. verbose Toggle warnings messages . select Character vector column names. NULL (default), variables selected. exclude Character vector column names excluded selection.","code":""},{"path":"/reference/normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize numeric variable to 0-1 range — normalize","text":"normalized object.","code":""},{"path":"/reference/normalize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Normalize numeric variable to 0-1 range — normalize","text":"x matrix, normalization performed across values (column- row-wise). column-wise normalization, convert matrix data.frame. x grouped data frame (grouped_df), normalization performed separately group.","code":""},{"path":"/reference/normalize.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Normalize numeric variable to 0-1 range — normalize","text":"Smithson M, Verkuilen J (2006). Better Lemon Squeezer? Maximum-Likelihood Regression Beta-Distributed Dependent Variables. Psychological Methods, 11(1), 54–71.","code":""},{"path":[]},{"path":"/reference/normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize numeric variable to 0-1 range — normalize","text":"","code":"normalize(c(0, 1, 5, -5, -2)) #> [1] 0.5 0.6 1.0 0.0 0.3 normalize(c(0, 1, 5, -5, -2), include_bounds = FALSE) #> [1] 0.50 0.58 0.90 0.10 0.34  head(normalize(trees)) #>        Girth     Height      Volume #> 1 0.00000000 0.29166667 0.001497006 #> 2 0.02439024 0.08333333 0.001497006 #> 3 0.04065041 0.00000000 0.000000000 #> 4 0.17886179 0.37500000 0.092814371 #> 5 0.19512195 0.75000000 0.128742515 #> 6 0.20325203 0.83333333 0.142215569"},{"path":"/reference/object_has_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Check names and rownames — object_has_names","title":"Check names and rownames — object_has_names","text":"object_has_names() checks specified names present given object. object_has_rownames() checks rownames present dataframe.","code":""},{"path":"/reference/object_has_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check names and rownames — object_has_names","text":"","code":"object_has_names(x, names)  object_has_rownames(x)"},{"path":"/reference/object_has_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check names and rownames — object_has_names","text":"x named object (atomic vector, list, dataframe, etc.). names single character vector characters.","code":""},{"path":"/reference/object_has_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check names and rownames — object_has_names","text":"logical vector logicals.","code":""},{"path":"/reference/object_has_names.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check names and rownames — object_has_names","text":"","code":"# check if specified names are present in the given object object_has_names(mtcars, \"am\") #> [1] TRUE object_has_names(anscombe, c(\"x1\", \"z1\", \"y1\")) #> [1]  TRUE FALSE  TRUE object_has_names(list(\"x\" = 1, \"y\" = 2), c(\"x\", \"a\")) #> [1]  TRUE FALSE  # check if a dataframe has rownames object_has_rownames(mtcars) #> [1] TRUE"},{"path":"/reference/ranktransform.html","id":null,"dir":"Reference","previous_headings":"","what":"(Signed) rank transformation — ranktransform","title":"(Signed) rank transformation — ranktransform","text":"Transform numeric values integers rank (.e., 1st smallest, 2nd smallest, 3rd smallest, etc.). Setting sign argument TRUE give signed ranks, ranking done according absolute size sign preserved (.e., 2, 1, -3, 4).","code":""},{"path":"/reference/ranktransform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Signed) rank transformation — ranktransform","text":"","code":"ranktransform(x, ...)  # S3 method for numeric ranktransform(x, sign = FALSE, method = \"average\", verbose = TRUE, ...)  # S3 method for grouped_df ranktransform(   x,   select = NULL,   exclude = NULL,   sign = FALSE,   method = \"average\",   ... )  # S3 method for data.frame ranktransform(   x,   select = NULL,   exclude = NULL,   sign = FALSE,   method = \"average\",   ... )"},{"path":"/reference/ranktransform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Signed) rank transformation — ranktransform","text":"x Object. ... Arguments passed methods. sign Logical, TRUE, return signed ranks. method Treatment ties. Can one \"average\" (default), \"first\", \"last\", \"random\", \"max\" \"min\". See rank() details. verbose Toggle warnings messages . select Character vector column names. NULL (default), variables selected. exclude Character vector column names excluded selection.","code":""},{"path":"/reference/ranktransform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Signed) rank transformation — ranktransform","text":"rank-transformed object.","code":""},{"path":[]},{"path":"/reference/ranktransform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"(Signed) rank transformation — ranktransform","text":"","code":"ranktransform(c(0, 1, 5, -5, -2)) #> [1] 3 4 5 1 2 ranktransform(c(0, 1, 5, -5, -2), sign = TRUE) #> Warning: Zeros detected. These cannot be sign-rank transformed. #> [1]   NA  1.0  3.5 -3.5 -2.0  head(ranktransform(trees)) #>   Girth Height Volume #> 1     1    6.0    2.5 #> 2     2    3.0    2.5 #> 3     3    1.0    1.0 #> 4     4    8.5    5.0 #> 5     5   25.5    7.0 #> 6     6   28.0    9.0"},{"path":"/reference/remove_empty.html","id":null,"dir":"Reference","previous_headings":"","what":"Return or remove variables or observations that are completely missing — remove_empty","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"functions check rows columns data frame completely contain missing values, .e. observations variables completely missing values, either (1) returns indices; (2) removes data frame.","code":""},{"path":"/reference/remove_empty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"","code":"empty_columns(x)  empty_rows(x)  remove_empty_columns(x)  remove_empty_rows(x)  remove_empty(x)"},{"path":"/reference/remove_empty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"x data frame.","code":""},{"path":"/reference/remove_empty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"empty_columns() empty_rows(), numeric (named) vector row column indices variables completely missing values. remove_empty_columns() remove_empty_rows(), data frame \"empty\" columns rows removed, respectively. remove_empty, empty rows columns removed.","code":""},{"path":"/reference/remove_empty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"","code":"tmp <- data.frame(   a = c(1, 2, 3, NA, 5),   b = c(1, NA, 3, NA, 5),   c = c(NA, NA, NA, NA, NA),   d = c(1, NA, 3, NA, 5) )  tmp #>    a  b  c  d #> 1  1  1 NA  1 #> 2  2 NA NA NA #> 3  3  3 NA  3 #> 4 NA NA NA NA #> 5  5  5 NA  5  # indices of empty columns or rows empty_columns(tmp) #> c  #> 3  empty_rows(tmp) #> [1] 4  # remove empty columns or rows remove_empty_columns(tmp) #>    a  b  d #> 1  1  1  1 #> 2  2 NA NA #> 3  3  3  3 #> 4 NA NA NA #> 5  5  5  5 remove_empty_rows(tmp) #>   a  b  c  d #> 1 1  1 NA  1 #> 2 2 NA NA NA #> 3 3  3 NA  3 #> 5 5  5 NA  5  # remove empty columns and rows remove_empty(tmp) #>   a  b  d #> 1 1  1  1 #> 2 2 NA NA #> 3 3  3  3 #> 5 5  5  5"},{"path":"/reference/replace_nan_inf.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert infinite or NaN values into NA — replace_nan_inf","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"Replaces infinite (Inf -Inf) NaN values NA.","code":""},{"path":"/reference/replace_nan_inf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"","code":"replace_nan_inf(data)"},{"path":"/reference/replace_nan_inf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"data vector data frame.","code":""},{"path":"/reference/replace_nan_inf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"Data Inf, -Inf, NaN converted NA.","code":""},{"path":"/reference/replace_nan_inf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"","code":"# a vector x <- c(1, 2, NA, 3, NaN, 4, NA, 5, Inf, -Inf, 6, 7) replace_nan_inf(x) #>  [1]  1  2 NA  3 NA  4 NA  5 NA NA  6  7  # a dataframe df <- data.frame(   x = c(1, NA, 5, Inf, 2, NA),   y = c(3, NaN, 4, -Inf, 6, 7),   stringsAsFactors = FALSE ) replace_nan_inf(df) #>    x  y #> 1  1  3 #> 2 NA NA #> 3  5  4 #> 4 NA NA #> 5  2  6 #> 6 NA  7"},{"path":"/reference/rescale_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescale design weights for multilevel analysis — rescale_weights","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"functions fit multilevel mixed effects models allow specify frequency weights, design (.e. sampling probability) weights, used analyzing complex samples survey data. rescale_weights() implements algorithm proposed Asparouhov (2006) Carle (2009) rescale design weights survey data account grouping structure multilevel models, can used multilevel modelling.","code":""},{"path":"/reference/rescale_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"","code":"rescale_weights(data, group, probability_weights, nest = FALSE)"},{"path":"/reference/rescale_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"data data frame. group Variable names (character vector, formula), indicating grouping structure (strata) survey data (level-2-cluster variable). also possible create weights multiple group variables; cases, created weighting variable suffixed name group variable. probability_weights Variable indicating probability (design sampling) weights survey data (level-1-weight). nest Logical, TRUE group indicates least two group variables, groups \"nested\", .e. groups now combination group level variables group.","code":""},{"path":"/reference/rescale_weights.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"data, including new weighting variables: pweights_aand pweights_b, represent rescaled design weights use multilevel models (use variables weights argument).","code":""},{"path":"/reference/rescale_weights.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"Rescaling based two methods: pweights_a, sample weights probability_weights adjusted factor represents proportion group size divided sum sampling weights within group. adjustment factor pweights_b sum sample weights within group divided sum squared sample weights within group (see Carle (2009), Appendix B). words, pweights_a \"scales weights new weights sum cluster sample size\" pweights_b \"scales weights new weights sum effective cluster size\". Regarding choice scaling methods B, Carle suggests \"analysts wish discuss point estimates report results based weighting method . analysts interested residual -group variance, method B may generally provide least biased estimates\". general, recommended fit non-weighted model weighted models scaling methods comparing models, see whether \"inferential decisions converge\", gain confidence results. Though bias scaled weights decreases increasing group size, method preferred insufficient low group size concern. group ID probably PSU may used random effects (e.g. nested design, group PSU varying intercepts), depending survey design mimicked.","code":""},{"path":"/reference/rescale_weights.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"Carle .C. (2009). Fitting multilevel models complex survey data design weights: Recommendations. BMC Medical Research Methodology 9(49): 1-13 Asparouhov T. (2006). General Multi-Level Modeling Sampling Weights. Communications Statistics - Theory Methods 35: 439-460","code":""},{"path":"/reference/rescale_weights.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"","code":"if (require(\"lme4\")) {   data(nhanes_sample)   head(rescale_weights(nhanes_sample, \"SDMVSTRA\", \"WTINT2YR\"))    # also works with multiple group-variables   head(rescale_weights(nhanes_sample, c(\"SDMVSTRA\", \"SDMVPSU\"), \"WTINT2YR\"))    # or nested structures.   x <- rescale_weights(     data = nhanes_sample,     group = c(\"SDMVSTRA\", \"SDMVPSU\"),     probability_weights = \"WTINT2YR\",     nest = TRUE   )   head(x)    nhanes_sample <- rescale_weights(nhanes_sample, \"SDMVSTRA\", \"WTINT2YR\")    glmer(     total ~ factor(RIAGENDR) * (log(age) + factor(RIDRETH1)) + (1 | SDMVPSU),     family = poisson(),     data = nhanes_sample,     weights = pweights_a   ) } #> Loading required package: lme4 #> Loading required package: Matrix #> Generalized linear mixed model fit by maximum likelihood (Laplace #>   Approximation) [glmerMod] #>  Family: poisson  ( log ) #> Formula: total ~ factor(RIAGENDR) * (log(age) + factor(RIDRETH1)) + (1 |   #>     SDMVPSU) #>    Data: nhanes_sample #> Weights: pweights_a #>       AIC       BIC    logLik  deviance  df.resid  #>  78844.27  78920.47 -39409.14  78818.27      2582  #> Random effects: #>  Groups  Name        Std.Dev. #>  SDMVPSU (Intercept) 0.1018   #> Number of obs: 2595, groups:  SDMVPSU, 2 #> Fixed Effects: #>                         (Intercept)                    factor(RIAGENDR)2   #>                            2.491801                            -1.021308   #>                            log(age)                    factor(RIDRETH1)2   #>                            0.838726                            -0.088627   #>                   factor(RIDRETH1)3                    factor(RIDRETH1)4   #>                           -0.013333                             0.722511   #>                   factor(RIDRETH1)5           factor(RIAGENDR)2:log(age)   #>                           -0.106521                            -1.012695   #> factor(RIAGENDR)2:factor(RIDRETH1)2  factor(RIAGENDR)2:factor(RIDRETH1)3   #>                           -0.009086                             0.732985   #> factor(RIAGENDR)2:factor(RIDRETH1)4  factor(RIAGENDR)2:factor(RIDRETH1)5   #>                            0.275967                             0.542074"},{"path":"/reference/reshape_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape CI between wide/long formats — reshape_ci","title":"Reshape CI between wide/long formats — reshape_ci","text":"Reshape CI wide/long formats.","code":""},{"path":"/reference/reshape_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape CI between wide/long formats — reshape_ci","text":"","code":"reshape_ci(x, ci_type = \"CI\")"},{"path":"/reference/reshape_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape CI between wide/long formats — reshape_ci","text":"x data frame containing columns named CI_low CI_high (similar, see ci_type). ci_type String indicating \"type\" (.e. prefix) interval columns. Per easystats convention, confidence credible intervals named CI_low CI_high, related ci_type \"CI\". column names intervals differ, ci_type can used indicate name, e.g. ci_type = \"SI\" can used support intervals, column names data frame SI_low SI_high.","code":""},{"path":"/reference/reshape_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape CI between wide/long formats — reshape_ci","text":"dataframe columns corresponding confidence intervals reshaped either wide long format.","code":""},{"path":"/reference/reshape_ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reshape CI between wide/long formats — reshape_ci","text":"","code":"x <- data.frame(   Parameter = c(\"Term 1\", \"Term 2\", \"Term 1\", \"Term 2\"),   CI = c(.8, .8, .9, .9),   CI_low = c(.2, .3, .1, .15),   CI_high = c(.5, .6, .8, .85),   stringsAsFactors = FALSE )  reshape_ci(x) #>   Parameter CI_low_0.8 CI_high_0.8 CI_low_0.9 CI_high_0.9 #> 1    Term 1        0.2         0.5       0.10        0.80 #> 2    Term 2        0.3         0.6       0.15        0.85 reshape_ci(reshape_ci(x)) #>   Parameter  CI CI_low CI_high #> 1    Term 1 0.8   0.20    0.50 #> 2    Term 1 0.9   0.10    0.80 #> 3    Term 2 0.8   0.30    0.60 #> 4    Term 2 0.9   0.15    0.85"},{"path":"/reference/skewness.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Skewness and (Excess) Kurtosis — skewness","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"Compute Skewness (Excess) Kurtosis","code":""},{"path":"/reference/skewness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"","code":"skewness(x, na.rm = TRUE, type = \"2\", iterations = NULL, verbose = TRUE, ...)  kurtosis(x, na.rm = TRUE, type = \"2\", iterations = NULL, verbose = TRUE, ...)  # S3 method for parameters_kurtosis print(x, digits = 3, test = FALSE, ...)  # S3 method for parameters_skewness print(x, digits = 3, test = FALSE, ...)  # S3 method for parameters_skewness summary(object, test = FALSE, ...)  # S3 method for parameters_kurtosis summary(object, test = FALSE, ...)"},{"path":"/reference/skewness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"x numeric vector data.frame. na.rm Remove missing values. type Type algorithm computing skewness. May one 1 (\"1\", \"\" \"classic\"), 2 (\"2\", \"II\" \"SPSS\" \"SAS\") 3 ( \"3\", \"III\" \"Minitab\"). See 'Details'. iterations number bootstrap replicates computing standard errors. NULL (default), parametric standard errors computed. verbose Toggle warnings messages. ... Arguments passed methods. digits Number decimal places. test Logical, TRUE, tests skewness kurtosis significantly different zero. object object returned skewness() kurtosis().","code":""},{"path":"/reference/skewness.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"Values skewness kurtosis.","code":""},{"path":[]},{"path":"/reference/skewness.html","id":"skewness","dir":"Reference","previous_headings":"","what":"Skewness","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"Symmetric distributions skewness around zero, negative skewness values indicates \"left-skewed\" distribution, positive skewness values indicates \"right-skewed\" distribution. Examples relationship skewness distributions : Normal distribution (symmetric distribution) skewness 0 Half-normal distribution skewness just 1 Exponential distribution skewness 2 Lognormal distribution can skewness positive value, depending parameters (https://en.wikipedia.org/wiki/Skewness)","code":""},{"path":"/reference/skewness.html","id":"types-of-skewness","dir":"Reference","previous_headings":"","what":"Types of Skewness","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"skewness() supports three different methods estimating skewness, discussed Joanes Gill (1988): Type \"1\" \"classical\" method, g1 = (sum((x - mean(x))^3) / n) / (sum((x - mean(x))^2) / n)^1.5 Type \"2\" first calculates type-1 skewness, adjusts result: G1 = g1 * sqrt(n * (n - 1)) / (n - 2). SAS SPSS usually return Type \"3\" first calculates type-1 skewness, adjusts result: b1 = g1 * ((1 - 1 / n))^1.5. Minitab usually returns.","code":""},{"path":"/reference/skewness.html","id":"kurtosis","dir":"Reference","previous_headings":"","what":"Kurtosis","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"kurtosis measure \"tailedness\" distribution. distribution kurtosis values zero called \"mesokurtic\". kurtosis value larger zero indicates \"leptokurtic\" distribution fatter tails. kurtosis value zero indicates \"platykurtic\" distribution thinner tails (https://en.wikipedia.org/wiki/Kurtosis).","code":""},{"path":"/reference/skewness.html","id":"types-of-kurtosis","dir":"Reference","previous_headings":"","what":"Types of Kurtosis","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"kurtosis() supports three different methods estimating kurtosis, discussed Joanes Gill (1988): Type \"1\" \"classical\" method, g2 = n * sum((x - mean(x))^4) / (sum((x - mean(x))^2)^2) - 3. Type \"2\" first calculates type-1 kurtosis, adjusts result: G2 = ((n + 1) * g2 + 6) * (n - 1)/((n - 2) * (n - 3)). SAS SPSS usually return Type \"3\" first calculates type-1 kurtosis, adjusts result: b2 = (g2 + 3) * (1 - 1 / n)^2 - 3. Minitab usually returns.","code":""},{"path":"/reference/skewness.html","id":"standard-errors","dir":"Reference","previous_headings":"","what":"Standard Errors","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"recommended compute empirical (bootstrapped) standard errors (via iterations argument) relying analytic standard errors (Wright & Herrington, 2011).","code":""},{"path":"/reference/skewness.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"D. N. Joanes C. . Gill (1998). Comparing measures sample skewness kurtosis. Statistician, 47, 183–189. Wright, D. B., & Herrington, J. . (2011). Problematic standard errors confidence intervals skewness kurtosis. Behavior research methods, 43(1), 8-17.","code":""},{"path":"/reference/skewness.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"","code":"skewness(rnorm(1000)) #> Skewness |    SE #> ---------------- #>    0.029 | 0.077 kurtosis(rnorm(1000)) #> Kurtosis |    SE #> ---------------- #>    0.163 | 0.154"},{"path":"/reference/smoothness.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantify the smoothness of a vector — smoothness","title":"Quantify the smoothness of a vector — smoothness","text":"Quantify smoothness vector","code":""},{"path":"/reference/smoothness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantify the smoothness of a vector — smoothness","text":"","code":"smoothness(x, method = \"cor\", lag = 1, iterations = NULL, ...)"},{"path":"/reference/smoothness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantify the smoothness of a vector — smoothness","text":"x Numeric vector (similar time series). method Can \"diff\" (standard deviation standardized differences) \"cor\" (default, lag-one autocorrelation). lag integer indicating lag use. less 1, interpreted expressed percentage length vector. iterations number bootstrap replicates computing standard errors. NULL (default), parametric standard errors computed. ... Arguments passed methods.","code":""},{"path":"/reference/smoothness.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quantify the smoothness of a vector — smoothness","text":"Value smoothness.","code":""},{"path":"/reference/smoothness.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Quantify the smoothness of a vector — smoothness","text":"https://stats.stackexchange.com/questions/24607/--measure-smoothness---time-series--r","code":""},{"path":"/reference/smoothness.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantify the smoothness of a vector — smoothness","text":"","code":"x <- (-10:10)^3 + rnorm(21, 0, 100) plot(x)  smoothness(x, method = \"cor\") #> [1] 0.9203155 #> attr(,\"class\") #> [1] \"parameters_smoothness\" \"numeric\"               smoothness(x, method = \"diff\") #> [1] 1.708527 #> attr(,\"class\") #> [1] \"parameters_smoothness\" \"numeric\""},{"path":"/reference/standardize.html","id":null,"dir":"Reference","previous_headings":"","what":"Standardization (Z-scoring) — standardize","title":"Standardization (Z-scoring) — standardize","text":"Performs standardization data (z-scoring), .e., centering scaling, data expressed terms standard deviation (.e., mean = 0, SD = 1) Median Absolute Deviance (median = 0, MAD = 1). applied statistical model, function extracts dataset, standardizes , refits model standardized version dataset. normalize() function can also used scale numeric variables within 0 - 1 range.  model standardization, see effectsize::standardize.default()","code":""},{"path":"/reference/standardize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standardization (Z-scoring) — standardize","text":"","code":"standardize(x, ...)  standardise(x, ...)  # S3 method for numeric standardize(   x,   robust = FALSE,   two_sd = FALSE,   weights = NULL,   verbose = TRUE,   reference = NULL,   center = NULL,   scale = NULL,   ... )  # S3 method for factor standardize(   x,   robust = FALSE,   two_sd = FALSE,   weights = NULL,   verbose = TRUE,   force = FALSE,   ... )  # S3 method for data.frame standardize(   x,   robust = FALSE,   two_sd = FALSE,   weights = NULL,   verbose = TRUE,   reference = NULL,   select = NULL,   exclude = NULL,   remove_na = c(\"none\", \"selected\", \"all\"),   force = FALSE,   append = FALSE,   center = NULL,   scale = NULL,   ... )  unstandardize(x, ...)  unstandardise(x, ...)  # S3 method for numeric unstandardize(   x,   center = NULL,   scale = NULL,   reference = NULL,   robust = FALSE,   two_sd = FALSE,   ... )  # S3 method for data.frame unstandardize(   x,   center = NULL,   scale = NULL,   reference = NULL,   robust = FALSE,   two_sd = FALSE,   select = NULL,   exclude = NULL,   ... )"},{"path":"/reference/standardize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standardization (Z-scoring) — standardize","text":"x data frame, vector statistical model (unstandardize() model). ... Arguments passed methods. robust Logical, TRUE, centering done subtracting median variables dividing median absolute deviation (MAD). FALSE, variables standardized subtracting mean dividing standard deviation (SD). two_sd TRUE, variables scaled two times deviation (SD MAD depending robust). method can useful obtain model coefficients continuous parameters comparable coefficients related binary predictors, applied predictors (outcome) (Gelman, 2008). weights Can NULL (weighting), : model: TRUE (default), weighted-standardization carried . data.frames: numeric vector weights, character name column data.frame contains weights. numeric vectors: numeric vector weights. verbose Toggle warnings messages . reference data frame variable centrality deviation computed instead input variable. Useful standardizing subset new data according another data frame. center, scale standardize():  Numeric values, can used alternative reference define reference centrality deviation. scale center length 1, recycled match length selected variables standardization. Else, center scale must length number selected variables. Values center scale matched selected variables provided order, unless named vector given. case, names matched names selected variables. unstandardize(): center scale correspond center (mean / median) scale (SD / MAD) original non-standardized data (data frames, named, column order correspond numeric column). However, one can also directly provide original data reference, center scale computed (according robust two_sd). Alternatively, input contains attributes center scale (output standardize()), take rest arguments absent. force Logical, TRUE, forces standardization factors dates well. Factors converted numerical values, lowest level value 1 (unless factor numeric levels, converted corresponding numeric value). select Character vector column names. NULL (default), variables selected. exclude Character vector column names excluded selection. remove_na missing values (NA) treated: \"none\" (default): column's standardization done separately, ignoring NAs. Else, rows NA columns selected select / exclude (\"selected\") columns (\"\") dropped standardization, resulting data frame include cases. append Logical string. TRUE, standardized variables get new column names (suffix \"_z\") appended (column bind) x, thus returning original standardized variables. FALSE, original variables x overwritten standardized versions. character value, standardized variables appended new column names (using defined suffix) original data frame.","code":""},{"path":"/reference/standardize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Standardization (Z-scoring) — standardize","text":"standardized object (either standardize data frame statistical model fitted standardized data).","code":""},{"path":"/reference/standardize.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Standardization (Z-scoring) — standardize","text":"x vector data frame remove_na = \"none\"), missing values preserved, return value length / number rows original input.","code":""},{"path":[]},{"path":"/reference/standardize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Standardization (Z-scoring) — standardize","text":"","code":"d <- iris[1:4, ]  # vectors standardise(d$Petal.Length) #> [1]  0.000000  0.000000 -1.224745  1.224745 #> attr(,\"center\") #> [1] 1.4 #> attr(,\"scale\") #> [1] 0.08164966 #> attr(,\"robust\") #> [1] FALSE  # Data frames # overwrite standardise(d, select = c(\"Sepal.Length\", \"Sepal.Width\")) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1    1.2402159   1.3887301          1.4         0.2  setosa #> 2    0.3382407  -0.9258201          1.4         0.2  setosa #> 3   -0.5637345   0.0000000          1.3         0.2  setosa #> 4   -1.0147221  -0.4629100          1.5         0.2  setosa  # append standardise(d, select = c(\"Sepal.Length\", \"Sepal.Width\"), append = TRUE) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length_z #> 1          5.1         3.5          1.4         0.2  setosa      1.2402159 #> 2          4.9         3.0          1.4         0.2  setosa      0.3382407 #> 3          4.7         3.2          1.3         0.2  setosa     -0.5637345 #> 4          4.6         3.1          1.5         0.2  setosa     -1.0147221 #>   Sepal.Width_z #> 1     1.3887301 #> 2    -0.9258201 #> 3     0.0000000 #> 4    -0.4629100  # append, suffix standardise(d, select = c(\"Sepal.Length\", \"Sepal.Width\"), append = \"_std\") #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length_std #> 1          5.1         3.5          1.4         0.2  setosa        1.2402159 #> 2          4.9         3.0          1.4         0.2  setosa        0.3382407 #> 3          4.7         3.2          1.3         0.2  setosa       -0.5637345 #> 4          4.6         3.1          1.5         0.2  setosa       -1.0147221 #>   Sepal.Width_std #> 1       1.3887301 #> 2      -0.9258201 #> 3       0.0000000 #> 4      -0.4629100  # standardizing with reference center and scale d <- data.frame(   a = c(-2, -1, 0, 1, 2),   b = c(3, 4, 5, 6, 7) )  # default standardization, based on mean and sd of each variable standardize(d) # means are 0 and 5, sd ~ 1.581139 #>            a          b #> 1 -1.2649111 -1.2649111 #> 2 -0.6324555 -0.6324555 #> 3  0.0000000  0.0000000 #> 4  0.6324555  0.6324555 #> 5  1.2649111  1.2649111  # standardization, based on mean and sd set to the same values standardize(d, center = c(0, 5), scale = c(1.581, 1.581)) #>            a          b #> 1 -1.2650221 -1.2650221 #> 2 -0.6325111 -0.6325111 #> 3  0.0000000  0.0000000 #> 4  0.6325111  0.6325111 #> 5  1.2650221  1.2650221  # standardization, mean and sd for each variable newly defined standardize(d, center = c(3, 4), scale = c(2, 4)) #>      a     b #> 1 -2.5 -0.25 #> 2 -2.0  0.00 #> 3 -1.5  0.25 #> 4 -1.0  0.50 #> 5 -0.5  0.75  # standardization, taking same mean and sd for each variable standardize(d, center = 1, scale = 3) #>            a         b #> 1 -1.0000000 0.6666667 #> 2 -0.6666667 1.0000000 #> 3 -0.3333333 1.3333333 #> 4  0.0000000 1.6666667 #> 5  0.3333333 2.0000000"},{"path":"/reference/to_numeric.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to Numeric (if possible) — to_numeric","title":"Convert to Numeric (if possible) — to_numeric","text":"Tries convert vector numeric possible (warnings errors). Otherwise, leaves .","code":""},{"path":"/reference/to_numeric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to Numeric (if possible) — to_numeric","text":"","code":"to_numeric(x)"},{"path":"/reference/to_numeric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to Numeric (if possible) — to_numeric","text":"x vector converted.","code":""},{"path":"/reference/to_numeric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert to Numeric (if possible) — to_numeric","text":"Numeric vector (possible)","code":""},{"path":"/reference/to_numeric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert to Numeric (if possible) — to_numeric","text":"","code":"to_numeric(c(\"1\", \"2\")) #> [1] 1 2 to_numeric(c(\"1\", \"2\", \"A\")) #> [1] \"1\" \"2\" \"A\""},{"path":"/reference/visualisation_recipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare objects for visualisation — visualisation_recipe","title":"Prepare objects for visualisation — visualisation_recipe","text":"function prepares objects visualisation returning list layers data geoms can easily plotted using instance ggplot2. See documentation object's class:","code":""},{"path":"/reference/visualisation_recipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare objects for visualisation — visualisation_recipe","text":"","code":"visualisation_recipe(x, ...)"},{"path":"/reference/visualisation_recipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare objects for visualisation — visualisation_recipe","text":"x easystats object. ... arguments passed functions.","code":""},{"path":"/reference/visualisation_recipe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare objects for visualisation — visualisation_recipe","text":"modelbased (estimate_means, estimate_contrasts, estimate_slopes, estimate_predicted, estimate_grouplevel)","code":""},{"path":"/reference/winsorize.html","id":null,"dir":"Reference","previous_headings":"","what":"Winsorize data — winsorize","title":"Winsorize data — winsorize","text":"Winsorize data","code":""},{"path":"/reference/winsorize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Winsorize data — winsorize","text":"","code":"winsorize(data, ...)  # S3 method for numeric winsorize(data, threshold = 0.2, verbose = TRUE, ...)"},{"path":"/reference/winsorize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Winsorize data — winsorize","text":"data Dataframe vector. ... Currently used. threshold amount winsorization. verbose Toggle warnings.","code":""},{"path":"/reference/winsorize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Winsorize data — winsorize","text":"dataframe winsorized columns winsorized vector.","code":""},{"path":"/reference/winsorize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Winsorize data — winsorize","text":"Winsorizing winsorization transformation statistics limiting extreme values statistical data reduce effect possibly spurious outliers. distribution many statistics can heavily influenced outliers. typical strategy set outliers (values beyond certain threshold) specified percentile data; example, 90\\ 5th percentile, data 95th percentile set 95th percentile. Winsorized estimators usually robust outliers standard forms.","code":""},{"path":"/reference/winsorize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Winsorize data — winsorize","text":"","code":"winsorize(iris$Sepal.Length, threshold = 0.2) #>   [1] 5.1 5.0 5.0 5.0 5.0 5.4 5.0 5.0 5.0 5.0 5.4 5.0 5.0 5.0 5.8 5.7 5.4 5.1 #>  [19] 5.7 5.1 5.4 5.1 5.0 5.1 5.0 5.0 5.0 5.2 5.2 5.0 5.0 5.4 5.2 5.5 5.0 5.0 #>  [37] 5.5 5.0 5.0 5.1 5.0 5.0 5.0 5.0 5.1 5.0 5.1 5.0 5.3 5.0 6.5 6.4 6.5 5.5 #>  [55] 6.5 5.7 6.3 5.0 6.5 5.2 5.0 5.9 6.0 6.1 5.6 6.5 5.6 5.8 6.2 5.6 5.9 6.1 #>  [73] 6.3 6.1 6.4 6.5 6.5 6.5 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.5 6.3 5.6 5.5 #>  [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 6.5 6.3 6.5 6.5 5.0 6.5 #> [109] 6.5 6.5 6.5 6.4 6.5 5.7 5.8 6.4 6.5 6.5 6.5 6.0 6.5 5.6 6.5 6.3 6.5 6.5 #> [127] 6.2 6.1 6.4 6.5 6.5 6.5 6.4 6.3 6.1 6.5 6.3 6.4 6.0 6.5 6.5 6.5 5.8 6.5 #> [145] 6.5 6.5 6.3 6.5 6.2 5.9 winsorize(iris, threshold = 0.2) #>        Sepal.Length Sepal.Width Petal.Length Petal.Width Species #>   [1,]          5.1         3.4          1.5         0.2       1 #>   [2,]          5.0         3.0          1.5         0.2       1 #>   [3,]          5.0         3.2          1.5         0.2       1 #>   [4,]          5.0         3.1          1.5         0.2       1 #>   [5,]          5.0         3.4          1.5         0.2       1 #>   [6,]          5.4         3.4          1.7         0.4       1 #>   [7,]          5.0         3.4          1.5         0.3       1 #>   [8,]          5.0         3.4          1.5         0.2       1 #>   [9,]          5.0         2.9          1.5         0.2       1 #>  [10,]          5.0         3.1          1.5         0.2       1 #>  [11,]          5.4         3.4          1.5         0.2       1 #>  [12,]          5.0         3.4          1.6         0.2       1 #>  [13,]          5.0         3.0          1.5         0.2       1 #>  [14,]          5.0         3.0          1.5         0.2       1 #>  [15,]          5.8         3.4          1.5         0.2       1 #>  [16,]          5.7         3.4          1.5         0.4       1 #>  [17,]          5.4         3.4          1.5         0.4       1 #>  [18,]          5.1         3.4          1.5         0.3       1 #>  [19,]          5.7         3.4          1.7         0.3       1 #>  [20,]          5.1         3.4          1.5         0.3       1 #>  [21,]          5.4         3.4          1.7         0.2       1 #>  [22,]          5.1         3.4          1.5         0.4       1 #>  [23,]          5.0         3.4          1.5         0.2       1 #>  [24,]          5.1         3.3          1.7         0.5       1 #>  [25,]          5.0         3.4          1.9         0.2       1 #>  [26,]          5.0         3.0          1.6         0.2       1 #>  [27,]          5.0         3.4          1.6         0.4       1 #>  [28,]          5.2         3.4          1.5         0.2       1 #>  [29,]          5.2         3.4          1.5         0.2       1 #>  [30,]          5.0         3.2          1.6         0.2       1 #>  [31,]          5.0         3.1          1.6         0.2       1 #>  [32,]          5.4         3.4          1.5         0.4       1 #>  [33,]          5.2         3.4          1.5         0.2       1 #>  [34,]          5.5         3.4          1.5         0.2       1 #>  [35,]          5.0         3.1          1.5         0.2       1 #>  [36,]          5.0         3.2          1.5         0.2       1 #>  [37,]          5.5         3.4          1.5         0.2       1 #>  [38,]          5.0         3.4          1.5         0.2       1 #>  [39,]          5.0         3.0          1.5         0.2       1 #>  [40,]          5.1         3.4          1.5         0.2       1 #>  [41,]          5.0         3.4          1.5         0.3       1 #>  [42,]          5.0         2.7          1.5         0.3       1 #>  [43,]          5.0         3.2          1.5         0.2       1 #>  [44,]          5.0         3.4          1.6         0.6       1 #>  [45,]          5.1         3.4          1.9         0.4       1 #>  [46,]          5.0         3.0          1.5         0.3       1 #>  [47,]          5.1         3.4          1.6         0.2       1 #>  [48,]          5.0         3.2          1.5         0.2       1 #>  [49,]          5.3         3.4          1.5         0.2       1 #>  [50,]          5.0         3.3          1.5         0.2       1 #>  [51,]          6.5         3.2          4.7         1.4       2 #>  [52,]          6.4         3.2          4.5         1.5       2 #>  [53,]          6.5         3.1          4.9         1.5       2 #>  [54,]          5.5         2.7          4.0         1.3       2 #>  [55,]          6.5         2.8          4.6         1.5       2 #>  [56,]          5.7         2.8          4.5         1.3       2 #>  [57,]          6.3         3.3          4.7         1.6       2 #>  [58,]          5.0         2.7          3.3         1.0       2 #>  [59,]          6.5         2.9          4.6         1.3       2 #>  [60,]          5.2         2.7          3.9         1.4       2 #>  [61,]          5.0         2.7          3.5         1.0       2 #>  [62,]          5.9         3.0          4.2         1.5       2 #>  [63,]          6.0         2.7          4.0         1.0       2 #>  [64,]          6.1         2.9          4.7         1.4       2 #>  [65,]          5.6         2.9          3.6         1.3       2 #>  [66,]          6.5         3.1          4.4         1.4       2 #>  [67,]          5.6         3.0          4.5         1.5       2 #>  [68,]          5.8         2.7          4.1         1.0       2 #>  [69,]          6.2         2.7          4.5         1.5       2 #>  [70,]          5.6         2.7          3.9         1.1       2 #>  [71,]          5.9         3.2          4.8         1.8       2 #>  [72,]          6.1         2.8          4.0         1.3       2 #>  [73,]          6.3         2.7          4.9         1.5       2 #>  [74,]          6.1         2.8          4.7         1.2       2 #>  [75,]          6.4         2.9          4.3         1.3       2 #>  [76,]          6.5         3.0          4.4         1.4       2 #>  [77,]          6.5         2.8          4.8         1.4       2 #>  [78,]          6.5         3.0          5.0         1.7       2 #>  [79,]          6.0         2.9          4.5         1.5       2 #>  [80,]          5.7         2.7          3.5         1.0       2 #>  [81,]          5.5         2.7          3.8         1.1       2 #>  [82,]          5.5         2.7          3.7         1.0       2 #>  [83,]          5.8         2.7          3.9         1.2       2 #>  [84,]          6.0         2.7          5.1         1.6       2 #>  [85,]          5.4         3.0          4.5         1.5       2 #>  [86,]          6.0         3.4          4.5         1.6       2 #>  [87,]          6.5         3.1          4.7         1.5       2 #>  [88,]          6.3         2.7          4.4         1.3       2 #>  [89,]          5.6         3.0          4.1         1.3       2 #>  [90,]          5.5         2.7          4.0         1.3       2 #>  [91,]          5.5         2.7          4.4         1.2       2 #>  [92,]          6.1         3.0          4.6         1.4       2 #>  [93,]          5.8         2.7          4.0         1.2       2 #>  [94,]          5.0         2.7          3.3         1.0       2 #>  [95,]          5.6         2.7          4.2         1.3       2 #>  [96,]          5.7         3.0          4.2         1.2       2 #>  [97,]          5.7         2.9          4.2         1.3       2 #>  [98,]          6.2         2.9          4.3         1.3       2 #>  [99,]          5.1         2.7          3.0         1.1       2 #> [100,]          5.7         2.8          4.1         1.3       2 #> [101,]          6.3         3.3          5.3         1.9       3 #> [102,]          5.8         2.7          5.1         1.9       3 #> [103,]          6.5         3.0          5.3         1.9       3 #> [104,]          6.3         2.9          5.3         1.8       3 #> [105,]          6.5         3.0          5.3         1.9       3 #> [106,]          6.5         3.0          5.3         1.9       3 #> [107,]          5.0         2.7          4.5         1.7       3 #> [108,]          6.5         2.9          5.3         1.8       3 #> [109,]          6.5         2.7          5.3         1.8       3 #> [110,]          6.5         3.4          5.3         1.9       3 #> [111,]          6.5         3.2          5.1         1.9       3 #> [112,]          6.4         2.7          5.3         1.9       3 #> [113,]          6.5         3.0          5.3         1.9       3 #> [114,]          5.7         2.7          5.0         1.9       3 #> [115,]          5.8         2.8          5.1         1.9       3 #> [116,]          6.4         3.2          5.3         1.9       3 #> [117,]          6.5         3.0          5.3         1.8       3 #> [118,]          6.5         3.4          5.3         1.9       3 #> [119,]          6.5         2.7          5.3         1.9       3 #> [120,]          6.0         2.7          5.0         1.5       3 #> [121,]          6.5         3.2          5.3         1.9       3 #> [122,]          5.6         2.8          4.9         1.9       3 #> [123,]          6.5         2.8          5.3         1.9       3 #> [124,]          6.3         2.7          4.9         1.8       3 #> [125,]          6.5         3.3          5.3         1.9       3 #> [126,]          6.5         3.2          5.3         1.8       3 #> [127,]          6.2         2.8          4.8         1.8       3 #> [128,]          6.1         3.0          4.9         1.8       3 #> [129,]          6.4         2.8          5.3         1.9       3 #> [130,]          6.5         3.0          5.3         1.6       3 #> [131,]          6.5         2.8          5.3         1.9       3 #> [132,]          6.5         3.4          5.3         1.9       3 #> [133,]          6.4         2.8          5.3         1.9       3 #> [134,]          6.3         2.8          5.1         1.5       3 #> [135,]          6.1         2.7          5.3         1.4       3 #> [136,]          6.5         3.0          5.3         1.9       3 #> [137,]          6.3         3.4          5.3         1.9       3 #> [138,]          6.4         3.1          5.3         1.8       3 #> [139,]          6.0         3.0          4.8         1.8       3 #> [140,]          6.5         3.1          5.3         1.9       3 #> [141,]          6.5         3.1          5.3         1.9       3 #> [142,]          6.5         3.1          5.1         1.9       3 #> [143,]          5.8         2.7          5.1         1.9       3 #> [144,]          6.5         3.2          5.3         1.9       3 #> [145,]          6.5         3.3          5.3         1.9       3 #> [146,]          6.5         3.0          5.2         1.9       3 #> [147,]          6.3         2.7          5.0         1.9       3 #> [148,]          6.5         3.0          5.2         1.9       3 #> [149,]          6.2         3.4          5.3         1.9       3 #> [150,]          5.9         3.0          5.1         1.8       3"},{"path":"/news/index.html","id":"datawizard-0239000","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.3.9000","title":"datawizard 0.2.3.9000","text":"New functions: find remove empty rows columns data frame: empty_rows(), empty_columns(), remove_empty_rows(), remove_empty_columns(), remove_empty. check names: object_has_names() object_has_rownames(). rotate data frames: data_rotate(). merge/join multiple data frames: data_merge() (alias data_join()). cut (recode) data groups: data_cut(). replace specific values NAs: convert_to_na(). replace Inf NaN values NAs: replace_nan_inf(). Arguments cols, data_relocate() can now also numeric values, indicating position destination column.","code":""},{"path":"/news/index.html","id":"datawizard-023","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.3","title":"datawizard 0.2.3","text":"CRAN release: 2022-01-26 New functions: work lists: is_empty_object() compact_list() work strings: compact_character()","code":""},{"path":"/news/index.html","id":"datawizard-022","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.2","title":"datawizard 0.2.2","text":"CRAN release: 2022-01-04 New function data_extract() (alias extract()) pull single variables data frame, possibly naming value row names data frame. reshape_ci() gains ci_type argument, reshape data frames CI-columns prefixes \"CI\". standardize() center() gain arguments center scale, define references centrality deviation used centering standardizing variables. center() gains arguments force reference, similar standardize(). functionality append argument center() standardize() revised. made suffix argument redundant, thus removed. Fixed issue standardize(). Fixed issue data_findcols().","code":""},{"path":"/news/index.html","id":"datawizard-021","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.1","title":"datawizard 0.2.1","text":"CRAN release: 2021-10-04 Exports plot method visualisation_recipe() objects see package. centre(), standardise(), unstandardise() exported aliases center(), standardize(), unstandardize(), respectively.","code":""},{"path":"/news/index.html","id":"datawizard-0201","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.0.1","title":"datawizard 0.2.0.1","text":"CRAN release: 2021-09-02 mainly maintenance release addresses issues conflicting namespaces.","code":""},{"path":"/news/index.html","id":"datawizard-020","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.0","title":"datawizard 0.2.0","text":"CRAN release: 2021-08-17 New function: visualisation_recipe(). following function now moved performance package: check_multimodal(). Minor updates documentation, including new vignette demean().","code":""},{"path":"/news/index.html","id":"datawizard-010","dir":"Changelog","previous_headings":"","what":"datawizard 0.1.0","title":"datawizard 0.1.0","text":"CRAN release: 2021-06-18 First release.","code":""}]
