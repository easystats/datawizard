[{"path":[]},{"path":"/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement patilindrajeet.science@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to datawizard","title":"Contributing to datawizard","text":"outlines propose change datawizard.","code":""},{"path":"/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to datawizard","text":"Small typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. want fix typos documentation, please edit related .R file R/ folder. edit .Rd file man/.","code":""},{"path":"/CONTRIBUTING.html","id":"filing-an-issue","dir":"","previous_headings":"","what":"Filing an issue","title":"Contributing to datawizard","text":"easiest way propose change new feature file issue. ’ve found bug, may also create associated issue. possible, try illustrate proposal bug minimal reproducible example.","code":""},{"path":"/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"","what":"Pull requests","title":"Contributing to datawizard","text":"Please create Git branch pull request (PR). contributed code roughly follow R style guide, particular easystats convention code-style. datawizard uses roxygen2, Markdown syntax, documentation. datawizard uses testthat. Adding tests PR makes easier merge PR code base. PR user-visible change, may add bullet top NEWS.md describing changes made. may optionally add GitHub username, links relevant issue(s)/PR(s).","code":""},{"path":"/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to datawizard","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"/articles/standardize_data.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Data Standardization","text":"make sense data effects, scientists might want standardize (Z-score) variables. makes data unitless, expressed terms deviation index centrality (e.g., mean median). However, aside benefits, standardization also comes challenges issues, scientist aware .","code":""},{"path":"/articles/standardize_data.html","id":"methods-of-standardization","dir":"Articles","previous_headings":"Introduction","what":"Methods of Standardization","title":"Data Standardization","text":"datawizard package offers two methods standardization via standardize() function: Normal standardization: center around mean, SD units (default). Robust standardization: center around median, MAD (median absolute deviation) units (robust = TRUE). Let’s look following example: can see different methods give different central variation values: standardize() can also used standardize full data frame - numeric variable standardized separately: Weighted standardization also supported via weights argument, factors can also standardized (’re kind thing) setting force = TRUE, converts factors treatment-coded dummy variables standardizing.","code":"library(datawizard) library(effectsize) # for data  # let's have a look at what the data look like data(\"hardlyworking\", package = \"effectsize\") head(hardlyworking) >     salary xtra_hours n_comps age seniority > 1 19744.65   4.161812       1  32         3 > 2 11301.95   1.621868       0  34         3 > 3 20635.62   1.190859       3  33         5 > 4 23047.16   7.190997       1  35         3 > 5 27342.15  11.261399       0  33         4 > 6 25656.63   3.633346       2  30         5 # let's use both methods of standardization hardlyworking$xtra_hours_z <- standardize(hardlyworking$xtra_hours) hardlyworking$xtra_hours_zr <- standardize(hardlyworking$xtra_hours, robust = TRUE) library(dplyr) library(tidyr)  hardlyworking %>%   select(starts_with(\"xtra_hours\")) %>%   pivot_longer(everything()) %>%   group_by(name) %>%   summarise(     mean = mean(value),     sd = sd(value),     median = median(value),     mad = mad(value)   ) hardlyworking_z <- standardize(hardlyworking) hardlyworking_z %>%   select(-xtra_hours_z, -xtra_hours_zr) %>%   pivot_longer(everything()) %>%   group_by(name) %>%   summarise(     mean = mean(value),     sd = sd(value),     median = median(value),     mad = mad(value)   )"},{"path":"/articles/standardize_data.html","id":"variable-wise-vs--participant-wise","dir":"Articles","previous_headings":"Introduction","what":"Variable-wise vs. Participant-wise","title":"Data Standardization","text":"Standardization important step extra caution required repeated-measures designs, three ways standardizing data: Variable-wise: common method. simple scaling column. Participant-wise: Variables standardized “within” participant, .e., participant, participant’s mean SD. Full: Participant-wise first re-standardizing variable-wise. Unfortunately, method used often explicitly stated. issue methods can generate important discrepancies (can turn contribute reproducibility crisis). Let’s investigate 3 methods.","code":""},{"path":"/articles/standardize_data.html","id":"the-data","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"The Data","title":"Data Standardization","text":"take emotion dataset participants exposed negative pictures rate emotions (valence) amount memories associated picture (autobiographical link). One make hypothesis young participants context war violence, negative pictures (mutilations) less related memories less negative pictures (involving example car crashes sick people). words, expect positive relationship valence (high values corresponding less negativity) autobiographical link. Let’s look data, averaged participants: can see means SDs, lot variability participants means individual within-participant SD.","code":"# Download the 'emotion' dataset load(url(\"https://raw.github.com/neuropsychology/psycho.R/master/data/emotion.rda\"))  # Discard neutral pictures (keep only negative) emotion <- emotion %>% filter(Emotion_Condition == \"Negative\")  # Summary emotion %>%   drop_na(Subjective_Valence, Autobiographical_Link) %>%   group_by(Participant_ID) %>%   summarise(     n_Trials = n(),     Valence_Mean = mean(Subjective_Valence),     Valence_SD = sd(Subjective_Valence)   ) >  [38;5;246m# A tibble: 19 × 4 [39m >  [38;5;246m# Groups:   Participant_ID [19] [39m >    Participant_ID n_Trials Valence_Mean Valence_SD >     [3m [38;5;246m<fct> [39m [23m              [3m [38;5;246m<int> [39m [23m         [3m [38;5;246m<dbl> [39m [23m       [3m [38;5;246m<dbl> [39m [23m >  [38;5;250m 1 [39m 10S                  24       - [31m58 [39m [31m. [39m [31m1 [39m       42.6  >  [38;5;250m 2 [39m 11S                  24       - [31m73 [39m [31m. [39m [31m2 [39m       37.0  >  [38;5;250m 3 [39m 12S                  24       - [31m57 [39m [31m. [39m [31m5 [39m       26.6  >  [38;5;250m 4 [39m 13S                  24       - [31m63 [39m [31m. [39m [31m2 [39m       23.7  >  [38;5;250m 5 [39m 14S                  24       - [31m56 [39m [31m. [39m [31m6 [39m       26.5  >  [38;5;250m 6 [39m 15S                  24       - [31m60 [39m [31m. [39m [31m6 [39m       33.7  >  [38;5;250m 7 [39m 16S                  24       - [31m46 [39m [31m. [39m [31m1 [39m       24.9  >  [38;5;250m 8 [39m 17S                  24        - [31m1 [39m [31m. [39m [31m54 [39m       4.98 >  [38;5;250m 9 [39m 18S                  24       - [31m67 [39m [31m. [39m [31m2 [39m       35.0  >  [38;5;250m10 [39m 19S                  24       - [31m59 [39m [31m. [39m [31m6 [39m       33.2  >  [38;5;250m11 [39m 1S                   24       - [31m53 [39m [31m. [39m [31m0 [39m       42.9  >  [38;5;250m12 [39m 2S                   23       - [31m43 [39m [31m. [39m [31m0 [39m       39.2  >  [38;5;250m13 [39m 3S                   24       - [31m64 [39m [31m. [39m [31m3 [39m       34.4  >  [38;5;250m14 [39m 4S                   24       - [31m81 [39m [31m. [39m [31m6 [39m       27.6  >  [38;5;250m15 [39m 5S                   24       - [31m58 [39m [31m. [39m [31m1 [39m       25.3  >  [38;5;250m16 [39m 6S                   24       - [31m74 [39m [31m. [39m [31m7 [39m       29.2  >  [38;5;250m17 [39m 7S                   24       - [31m62 [39m [31m. [39m [31m3 [39m       39.7  >  [38;5;250m18 [39m 8S                   24       - [31m56 [39m [31m. [39m [31m9 [39m       32.7  >  [38;5;250m19 [39m 9S                   24       - [31m31 [39m [31m. [39m [31m5 [39m       52.7"},{"path":"/articles/standardize_data.html","id":"effect-of-standardization","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"Effect of Standardization","title":"Data Standardization","text":"create three data frames standardized three techniques. Let’s see three standardization techniques affected Valence variable.","code":"Z_VariableWise <- emotion %>%   standardize()  Z_ParticipantWise <- emotion %>%   group_by(Participant_ID) %>%   standardize()  Z_Full <- emotion %>%   group_by(Participant_ID) %>%   standardize() %>%   ungroup() %>%   standardize()"},{"path":"/articles/standardize_data.html","id":"across-participants","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"Across Participants","title":"Data Standardization","text":"can calculate mean SD Valence across participants: means SD appear fairly similar (0 1)…  marginal distributions…","code":"# Create a convenient function to print summarise_Subjective_Valence <- function(data) {   df_name <- deparse(substitute(data))   data %>%     ungroup() %>%     summarise(       DF = df_name,       Mean = mean(Subjective_Valence),       SD = sd(Subjective_Valence)     ) } # Check the results rbind(   summarise_Subjective_Valence(Z_VariableWise),   summarise_Subjective_Valence(Z_ParticipantWise),   summarise_Subjective_Valence(Z_Full) ) library(see) library(ggplot2)  ggplot() +   geom_density(aes(Z_VariableWise$Subjective_Valence,     color = \"Z_VariableWise\"   ), size = 1) +   geom_density(aes(Z_ParticipantWise$Subjective_Valence,     color = \"Z_ParticipantWise\"   ), size = 1) +   geom_density(aes(Z_Full$Subjective_Valence,     color = \"Z_Full\"   ), size = 1) +   see::theme_modern() +   labs(color = \"\")"},{"path":"/articles/standardize_data.html","id":"at-the-participant-level","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"At the Participant Level","title":"Data Standardization","text":"However, can also look happens participant level. Let’s look first 5 participants: Seems like full participant-wise standardization give similar results, different ones variable-wise standardization.","code":"# Create convenient function print_participants <- function(data) {   df_name <- deparse(substitute(data))   data %>%     group_by(Participant_ID) %>%     summarise(       DF = df_name,       Mean = mean(Subjective_Valence),       SD = sd(Subjective_Valence)     ) %>%     head(5) %>%     select(DF, everything()) }  # Check the results rbind(   print_participants(Z_VariableWise),   print_participants(Z_ParticipantWise),   print_participants(Z_Full) )"},{"path":"/articles/standardize_data.html","id":"compare","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"Compare","title":"Data Standardization","text":"Let’s correlation variable-wise participant-wise methods.  three standardization methods roughly present characteristics general level (mean 0 SD 1) similar distribution, values exactly ! Let’s now answer original question investigating linear relationship valence autobiographical link. can running mixed-effects model participants entered random effects. can extract parameters interest model, find: can see, variable-wise standardization affects coefficient (expected, changes unit), test statistic statistical significance. However, using participant-wise standardization affect coefficient significance. method better justified, choice depends specific case, context, data goal.","code":"r <- cor.test(   Z_VariableWise$Subjective_Valence,   Z_ParticipantWise$Subjective_Valence )  data.frame(   Original = emotion$Subjective_Valence,   VariableWise = Z_VariableWise$Subjective_Valence,   ParticipantWise = Z_ParticipantWise$Subjective_Valence ) %>%   ggplot(aes(x = VariableWise, y = ParticipantWise, colour = Original)) +   geom_point(alpha = 0.75, shape = 16) +   geom_smooth(method = \"lm\", color = \"black\") +   scale_color_distiller(palette = 1) +   ggtitle(paste0(\"r = \", round(r$estimate, 2))) +   see::theme_modern() library(lme4) m_raw <- lmer(   formula = Subjective_Valence ~ Autobiographical_Link + (1 | Participant_ID),   data = emotion ) m_VariableWise <- update(m_raw, data = Z_VariableWise) m_ParticipantWise <- update(m_raw, data = Z_ParticipantWise) m_Full <- update(m_raw, data = Z_Full) # Convenient function get_par <- function(model) {   mod_name <- deparse(substitute(model))   parameters::model_parameters(model) %>%     mutate(Model = mod_name) %>%     select(-Parameter) %>%     select(Model, everything()) %>%     .[-1, ] }  # Run the model on all datasets rbind(   get_par(m_raw),   get_par(m_VariableWise),   get_par(m_ParticipantWise),   get_par(m_Full) ) > # Fixed Effects >  > Model             | Coefficient |   SE |        95% CI | t(451) |     p > ----------------------------------------------------------------------- > m_raw             |        0.09 | 0.07 | [-0.04, 0.22] |   1.36 | 0.174 > m_VariableWise    |        0.07 | 0.05 | [-0.03, 0.17] |   1.36 | 0.174 > m_ParticipantWise |        0.08 | 0.05 | [-0.01, 0.17] |   1.75 | 0.080 > m_Full            |        0.08 | 0.05 | [-0.01, 0.17] |   1.75 | 0.080 >  > # Random Effects: Participant_ID >  > Model             | Coefficient > ------------------------------- > m_raw             |       16.49 > m_VariableWise    |        0.45 > m_ParticipantWise |        0.00 > m_Full            |        0.00 >  > # Random Effects: Residual >  > Model             | Coefficient > ------------------------------- > m_raw             |       33.56 > m_VariableWise    |        0.91 > m_ParticipantWise |        0.98 > m_Full            |        1.00"},{"path":"/articles/standardize_data.html","id":"conclusion","dir":"Articles","previous_headings":"Introduction > Variable-wise vs. Participant-wise","what":"Conclusion","title":"Data Standardization","text":"Standardization can useful cases justified. Variable Participant-wise standardization methods appear produce similar data. Variable Participant-wise standardization can lead different results. chosen method can strongly influence results therefore explicitly stated justified enhance reproducibility results. showed yet another way sneakily tweaking data can change results. prevent use bad practice, can highlight importance open data, open analysis/scripts, preregistration.","code":""},{"path":"/articles/standardize_data.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See also","title":"Data Standardization","text":"datawizard::demean(): https://easystats.github.io/datawizard/reference/demean.html standardize_parameters(method = \"pseudo\") mixed-effects models https://easystats.github.io/effectsize/reference/standardize_parameters.html","code":""},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dominique Makowski. Author.            @Dom_Makowski Daniel Lüdecke. Author.            @strengejacke Indrajeet Patil. Author, maintainer.            @patilindrajeets Mattan S. Ben-Shachar. Author. Brenton M. Wiernik. Author.            @bmwiernik Etienne Bacher. Contributor.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Makowski, Lüdecke, Patil, Ben-Shachar, & Wiernik (2021). datawizard: Easy Data Wrangling. CRAN. Available https://easystats.github.io/datawizard/","code":"@Article{,   title = {datawizard: Easy Data Wrangling},   author = {Dominique Makowski and Daniel Lüdecke and Indrajeet Patil and Mattan S. Ben-Shachar and Brenton M. Wiernik},   journal = {CRAN},   year = {2021},   note = {R package},   url = {https://easystats.github.io/datawizard/}, }"},{"path":"/index.html","id":"datawizard-easy-data-wrangling-","dir":"","previous_headings":"","what":"Easy Data Wrangling","title":"Easy Data Wrangling","text":"Hockety pockety wockety wack, prepare data forth back ✨ datawizard lightweight package easily manipulate, clean, transform, prepare data analysis.","code":""},{"path":[]},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Easy Data Wrangling","text":"cite package, run following command:","code":"citation(\"datawizard\")  To cite datawizard in publications use:    Makowski, Lüdecke, Patil, Ben-Shachar, & Wiernik (2021). datawizard:   Easy Data Wrangling. CRAN. Available from   https://easystats.github.io/datawizard/  A BibTeX entry for LaTeX users is    @Article{,     title = {datawizard: Easy Data Wrangling},     author = {Dominique Makowski and Daniel Lüdecke and Indrajeet Patil and Mattan S. Ben-Shachar and Brenton M. Wiernik},     journal = {CRAN},     year = {2021},     note = {R package},     url = {https://easystats.github.io/datawizard/},   }"},{"path":[]},{"path":[]},{"path":"/index.html","id":"select-filter-and-remove-variables","dir":"","previous_headings":"Data wrangling","what":"Select, filter and remove variables","title":"Easy Data Wrangling","text":"package provides helpers filter rows meeting certain conditions: also possible select one variables: Due consistent API, removing variables just simple:","code":"matching_rows <- data_match(mtcars, data.frame(vs = 0, am = 1)) mtcars[matching_rows, ] #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 #> Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 #> Maserati Bora  15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8 # single variable data_extract(mtcars, \"gear\") #>  [1] 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4  # more variables head(data_extract(iris, ends_with(\"Width\"))) #>   Sepal.Width Petal.Width #> 1         3.5         0.2 #> 2         3.0         0.2 #> 3         3.2         0.2 #> 4         3.1         0.2 #> 5         3.6         0.2 #> 6         3.9         0.4 head(data_remove(iris, starts_with(\"Sepal\"))) #>   Petal.Length Petal.Width Species #> 1          1.4         0.2  setosa #> 2          1.4         0.2  setosa #> 3          1.3         0.2  setosa #> 4          1.5         0.2  setosa #> 5          1.4         0.2  setosa #> 6          1.7         0.4  setosa"},{"path":"/index.html","id":"reorder-or-rename","dir":"","previous_headings":"Data wrangling","what":"Reorder or rename","title":"Easy Data Wrangling","text":"","code":"head(data_relocate(iris, select = \"Species\", before = \"Sepal.Length\")) #>   Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1  setosa          5.1         3.5          1.4         0.2 #> 2  setosa          4.9         3.0          1.4         0.2 #> 3  setosa          4.7         3.2          1.3         0.2 #> 4  setosa          4.6         3.1          1.5         0.2 #> 5  setosa          5.0         3.6          1.4         0.2 #> 6  setosa          5.4         3.9          1.7         0.4 head(data_rename(iris, c(\"Sepal.Length\", \"Sepal.Width\"), c(\"length\", \"width\"))) #>   length width Petal.Length Petal.Width Species #> 1    5.1   3.5          1.4         0.2  setosa #> 2    4.9   3.0          1.4         0.2  setosa #> 3    4.7   3.2          1.3         0.2  setosa #> 4    4.6   3.1          1.5         0.2  setosa #> 5    5.0   3.6          1.4         0.2  setosa #> 6    5.4   3.9          1.7         0.4  setosa"},{"path":"/index.html","id":"merge","dir":"","previous_headings":"Data wrangling","what":"Merge","title":"Easy Data Wrangling","text":"","code":"x <- data.frame(a = 1:3, b = c(\"a\", \"b\", \"c\"), c = 5:7, id = 1:3) y <- data.frame(c = 6:8, d = c(\"f\", \"g\", \"h\"), e = 100:102, id = 2:4)  x #>   a b c id #> 1 1 a 5  1 #> 2 2 b 6  2 #> 3 3 c 7  3 y #>   c d   e id #> 1 6 f 100  2 #> 2 7 g 101  3 #> 3 8 h 102  4  data_merge(x, y, join = \"full\") #>    a    b c id    d   e #> 3  1    a 5  1 <NA>  NA #> 1  2    b 6  2    f 100 #> 2  3    c 7  3    g 101 #> 4 NA <NA> 8  4    h 102  data_merge(x, y, join = \"left\") #>   a b c id    d   e #> 3 1 a 5  1 <NA>  NA #> 1 2 b 6  2    f 100 #> 2 3 c 7  3    g 101  data_merge(x, y, join = \"right\") #>    a    b c id d   e #> 1  2    b 6  2 f 100 #> 2  3    c 7  3 g 101 #> 3 NA <NA> 8  4 h 102  data_merge(x, y, join = \"semi\", by = \"c\") #>   a b c id #> 2 2 b 6  2 #> 3 3 c 7  3  data_merge(x, y, join = \"anti\", by = \"c\") #>   a b c id #> 1 1 a 5  1  data_merge(x, y, join = \"inner\") #>   a b c id d   e #> 1 2 b 6  2 f 100 #> 2 3 c 7  3 g 101  data_merge(x, y, join = \"bind\") #>    a    b c id    d   e #> 1  1    a 5  1 <NA>  NA #> 2  2    b 6  2 <NA>  NA #> 3  3    c 7  3 <NA>  NA #> 4 NA <NA> 6  2    f 100 #> 5 NA <NA> 7  3    g 101 #> 6 NA <NA> 8  4    h 102"},{"path":"/index.html","id":"reshape","dir":"","previous_headings":"Data wrangling","what":"Reshape","title":"Easy Data Wrangling","text":"common data wrangling task reshape data. Either go wide/Cartesian long/tidy format way","code":"wide_data <- data.frame(replicate(5, rnorm(10)))  head(data_to_long(wide_data)) #>   Name       Value #> 1   X1 -0.08281164 #> 2   X2 -1.12490028 #> 3   X3 -0.70632036 #> 4   X4 -0.70278946 #> 5   X5  0.07633326 #> 6   X1  1.93468099 long_data <- data_to_long(wide_data, rows_to = \"Row_ID\") # Save row number  data_to_wide(long_data,   colnames_from = \"Name\",   values_from = \"Value\",   rows_from = \"Row_ID\" ) #>    Row_ID    Value_X1    Value_X2    Value_X3   Value_X4    Value_X5 #> 1       1 -0.08281164 -1.12490028 -0.70632036 -0.7027895  0.07633326 #> 2       2  1.93468099 -0.87430362  0.96687656  0.2998642 -0.23035595 #> 3       3 -2.05128979  0.04386162 -0.71016648  1.1494697  0.31746484 #> 4       4  0.27773897 -0.58397514 -0.05917365 -0.3016415 -1.59268440 #> 5       5 -1.52596060 -0.82329858 -0.23094342 -0.5473394 -0.18194062 #> 6       6 -0.26916362  0.11059280  0.69200045 -0.3854041  1.75614174 #> 7       7  1.23305388  0.36472778  1.35682290  0.2763720  0.11394932 #> 8       8  0.63360774  0.05370100  1.78872284  0.1518608 -0.29216508 #> 9       9  0.35271746  1.36867235  0.41071582 -0.4313808  1.75409316 #> 10     10 -0.56048248 -0.38045724 -2.18785470 -1.8705001  1.80958455"},{"path":"/index.html","id":"empty-rows-and-columns","dir":"","previous_headings":"Data wrangling","what":"Empty rows and columns","title":"Easy Data Wrangling","text":"","code":"tmp <- data.frame(   a = c(1, 2, 3, NA, 5),   b = c(1, NA, 3, NA, 5),   c = c(NA, NA, NA, NA, NA),   d = c(1, NA, 3, NA, 5) )  tmp #>    a  b  c  d #> 1  1  1 NA  1 #> 2  2 NA NA NA #> 3  3  3 NA  3 #> 4 NA NA NA NA #> 5  5  5 NA  5  # indices of empty columns or rows empty_columns(tmp) #> c  #> 3 empty_rows(tmp) #> [1] 4  # remove empty columns or rows remove_empty_columns(tmp) #>    a  b  d #> 1  1  1  1 #> 2  2 NA NA #> 3  3  3  3 #> 4 NA NA NA #> 5  5  5  5 remove_empty_rows(tmp) #>   a  b  c  d #> 1 1  1 NA  1 #> 2 2 NA NA NA #> 3 3  3 NA  3 #> 5 5  5 NA  5  # remove empty columns and rows remove_empty(tmp) #>   a  b  d #> 1 1  1  1 #> 2 2 NA NA #> 3 3  3  3 #> 5 5  5  5"},{"path":"/index.html","id":"recode-or-cut-dataframe","dir":"","previous_headings":"Data wrangling","what":"Recode or cut dataframe","title":"Easy Data Wrangling","text":"","code":"set.seed(123) x <- sample(1:10, size = 50, replace = TRUE)  table(x) #> x #>  1  2  3  4  5  6  7  8  9 10  #>  2  3  5  3  7  5  5  2 11  7  # cut into 3 groups, based on distribution (quantiles) table(data_cut(x, split = \"quantile\", n_groups = 3)) #>  #>  1  2  3  #> 13 19 18"},{"path":"/index.html","id":"data-transformations","dir":"","previous_headings":"","what":"Data Transformations","title":"Easy Data Wrangling","text":"packages also contains multiple functions help transform data.","code":""},{"path":"/index.html","id":"standardize","dir":"","previous_headings":"Data Transformations","what":"Standardize","title":"Easy Data Wrangling","text":"example, standardize (z-score) data:","code":"# before summary(swiss) #>    Fertility      Agriculture     Examination      Education     #>  Min.   :35.00   Min.   : 1.20   Min.   : 3.00   Min.   : 1.00   #>  1st Qu.:64.70   1st Qu.:35.90   1st Qu.:12.00   1st Qu.: 6.00   #>  Median :70.40   Median :54.10   Median :16.00   Median : 8.00   #>  Mean   :70.14   Mean   :50.66   Mean   :16.49   Mean   :10.98   #>  3rd Qu.:78.45   3rd Qu.:67.65   3rd Qu.:22.00   3rd Qu.:12.00   #>  Max.   :92.50   Max.   :89.70   Max.   :37.00   Max.   :53.00   #>     Catholic       Infant.Mortality #>  Min.   :  2.150   Min.   :10.80    #>  1st Qu.:  5.195   1st Qu.:18.15    #>  Median : 15.140   Median :20.00    #>  Mean   : 41.144   Mean   :19.94    #>  3rd Qu.: 93.125   3rd Qu.:21.70    #>  Max.   :100.000   Max.   :26.60  # after summary(standardize(swiss)) #>    Fertility         Agriculture       Examination         Education       #>  Min.   :-2.81327   Min.   :-2.1778   Min.   :-1.69084   Min.   :-1.0378   #>  1st Qu.:-0.43569   1st Qu.:-0.6499   1st Qu.:-0.56273   1st Qu.:-0.5178   #>  Median : 0.02061   Median : 0.1515   Median :-0.06134   Median :-0.3098   #>  Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   #>  3rd Qu.: 0.66504   3rd Qu.: 0.7481   3rd Qu.: 0.69074   3rd Qu.: 0.1062   #>  Max.   : 1.78978   Max.   : 1.7190   Max.   : 2.57094   Max.   : 4.3702   #>     Catholic       Infant.Mortality   #>  Min.   :-0.9350   Min.   :-3.13886   #>  1st Qu.:-0.8620   1st Qu.:-0.61543   #>  Median :-0.6235   Median : 0.01972   #>  Mean   : 0.0000   Mean   : 0.00000   #>  3rd Qu.: 1.2464   3rd Qu.: 0.60337   #>  Max.   : 1.4113   Max.   : 2.28566"},{"path":"/index.html","id":"winsorize","dir":"","previous_headings":"Data Transformations","what":"Winsorize","title":"Easy Data Wrangling","text":"winsorize data:","code":"# before anscombe #>    x1 x2 x3 x4    y1   y2    y3    y4 #> 1  10 10 10  8  8.04 9.14  7.46  6.58 #> 2   8  8  8  8  6.95 8.14  6.77  5.76 #> 3  13 13 13  8  7.58 8.74 12.74  7.71 #> 4   9  9  9  8  8.81 8.77  7.11  8.84 #> 5  11 11 11  8  8.33 9.26  7.81  8.47 #> 6  14 14 14  8  9.96 8.10  8.84  7.04 #> 7   6  6  6  8  7.24 6.13  6.08  5.25 #> 8   4  4  4 19  4.26 3.10  5.39 12.50 #> 9  12 12 12  8 10.84 9.13  8.15  5.56 #> 10  7  7  7  8  4.82 7.26  6.42  7.91 #> 11  5  5  5  8  5.68 4.74  5.73  6.89  # after winsorize(anscombe) #>       x1 x2 x3 x4   y1   y2   y3   y4 #>  [1,] 10 10 10  8 8.04 9.13 7.46 6.58 #>  [2,]  8  8  8  8 6.95 8.14 6.77 5.76 #>  [3,] 12 12 12  8 7.58 8.74 8.15 7.71 #>  [4,]  9  9  9  8 8.81 8.77 7.11 8.47 #>  [5,] 11 11 11  8 8.33 9.13 7.81 8.47 #>  [6,] 12 12 12  8 8.81 8.10 8.15 7.04 #>  [7,]  6  6  6  8 7.24 6.13 6.08 5.76 #>  [8,]  6  6  6  8 5.68 6.13 6.08 8.47 #>  [9,] 12 12 12  8 8.81 9.13 8.15 5.76 #> [10,]  7  7  7  8 5.68 7.26 6.42 7.91 #> [11,]  6  6  6  8 5.68 6.13 6.08 6.89"},{"path":"/index.html","id":"center","dir":"","previous_headings":"Data Transformations","what":"Center","title":"Easy Data Wrangling","text":"grand-mean center data","code":"center(anscombe) #>    x1 x2 x3 x4          y1         y2    y3         y4 #> 1   1  1  1 -1  0.53909091  1.6390909 -0.04 -0.9209091 #> 2  -1 -1 -1 -1 -0.55090909  0.6390909 -0.73 -1.7409091 #> 3   4  4  4 -1  0.07909091  1.2390909  5.24  0.2090909 #> 4   0  0  0 -1  1.30909091  1.2690909 -0.39  1.3390909 #> 5   2  2  2 -1  0.82909091  1.7590909  0.31  0.9690909 #> 6   5  5  5 -1  2.45909091  0.5990909  1.34 -0.4609091 #> 7  -3 -3 -3 -1 -0.26090909 -1.3709091 -1.42 -2.2509091 #> 8  -5 -5 -5 10 -3.24090909 -4.4009091 -2.11  4.9990909 #> 9   3  3  3 -1  3.33909091  1.6290909  0.65 -1.9409091 #> 10 -2 -2 -2 -1 -2.68090909 -0.2409091 -1.08  0.4090909 #> 11 -4 -4 -4 -1 -1.82090909 -2.7609091 -1.77 -0.6109091"},{"path":"/index.html","id":"ranktransform","dir":"","previous_headings":"Data Transformations","what":"Ranktransform","title":"Easy Data Wrangling","text":"rank-transform data:","code":"# before head(trees) #>   Girth Height Volume #> 1   8.3     70   10.3 #> 2   8.6     65   10.3 #> 3   8.8     63   10.2 #> 4  10.5     72   16.4 #> 5  10.7     81   18.8 #> 6  10.8     83   19.7  # after head(ranktransform(trees)) #>   Girth Height Volume #> 1     1    6.0    2.5 #> 2     2    3.0    2.5 #> 3     3    1.0    1.0 #> 4     4    8.5    5.0 #> 5     5   25.5    7.0 #> 6     6   28.0    9.0"},{"path":"/index.html","id":"rescale","dir":"","previous_headings":"Data Transformations","what":"Rescale","title":"Easy Data Wrangling","text":"rescale numeric variable new range:","code":"change_scale(c(0, 1, 5, -5, -2)) #> [1]  50  60 100   0  30"},{"path":"/index.html","id":"rotate-or-transpose","dir":"","previous_headings":"Data Transformations","what":"Rotate or transpose","title":"Easy Data Wrangling","text":"","code":"x <- mtcars[1:3, 1:4]  x #>                mpg cyl disp  hp #> Mazda RX4     21.0   6  160 110 #> Mazda RX4 Wag 21.0   6  160 110 #> Datsun 710    22.8   4  108  93  data_rotate(x) #>      Mazda RX4 Mazda RX4 Wag Datsun 710 #> mpg         21            21       22.8 #> cyl          6             6        4.0 #> disp       160           160      108.0 #> hp         110           110       93.0"},{"path":"/index.html","id":"data-proprties","dir":"","previous_headings":"","what":"Data proprties","title":"Easy Data Wrangling","text":"datawizard provides way provide comprehensive descriptive summary variables dataframe: even just variable also additional data properties can computed using package.","code":"data(iris) describe_distribution(iris) #> Variable     | Mean |   SD |  IQR | Min | Max | Skewness | Kurtosis |   n | n_Missing #> ------------------------------------------------------------------------------------- #> Sepal.Length |  5.8 | 0.83 | 1.30 | 4.3 | 7.9 |     0.31 |    -0.55 | 150 |         0 #> Sepal.Width  |  3.1 | 0.44 | 0.52 | 2.0 | 4.4 |     0.32 |     0.23 | 150 |         0 #> Petal.Length |  3.8 | 1.77 | 3.52 | 1.0 | 6.9 |    -0.27 |    -1.40 | 150 |         0 #> Petal.Width  |  1.2 | 0.76 | 1.50 | 0.1 | 2.5 |    -0.10 |    -1.34 | 150 |         0 describe_distribution(mtcars$wt) #> Mean |   SD | IQR | Min | Max | Skewness | Kurtosis |  n | n_Missing #> -------------------------------------------------------------------- #> 3.2  | 0.98 | 1.2 | 1.5 | 5.4 |     0.47 |     0.42 | 32 |         0 x <- (-10:10)^3 + rnorm(21, 0, 100) smoothness(x, method = \"diff\") #> [1] 1.791243 #> attr(,\"class\") #> [1] \"parameters_smoothness\" \"numeric\""},{"path":"/index.html","id":"contributing-and-support","dir":"","previous_headings":"","what":"Contributing and Support","title":"Easy Data Wrangling","text":"case want file issue contribute another way package, please follow guide. questions functionality, may either contact us via email also file issue.","code":""},{"path":"/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Easy Data Wrangling","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"/reference/adjust.html","id":null,"dir":"Reference","previous_headings":"","what":"Adjust data for the effect of other variable(s) — adjust","title":"Adjust data for the effect of other variable(s) — adjust","text":"function can used adjust data effect variables present dataset. based underlying fitting regressions models, allowing quite flexibility, including factors random effects mixed models (multilevel partialization), continuous variables smooth terms general additive models (non-linear partialization) /fitting models Bayesian framework. values returned function residuals regression models. Note regular correlation two \"adjusted\" variables equivalent partial correlation .","code":""},{"path":"/reference/adjust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adjust data for the effect of other variable(s) — adjust","text":"","code":"adjust(   data,   effect = NULL,   select = NULL,   exclude = NULL,   multilevel = FALSE,   additive = FALSE,   bayesian = FALSE,   keep_intercept = FALSE,   ignore_case = FALSE )  data_adjust(   data,   effect = NULL,   select = NULL,   exclude = NULL,   multilevel = FALSE,   additive = FALSE,   bayesian = FALSE,   keep_intercept = FALSE,   ignore_case = FALSE )"},{"path":"/reference/adjust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adjust data for the effect of other variable(s) — adjust","text":"data dataframe. effect Character vector column names adjusted (regressed ). NULL (default), variables selected. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. multilevel TRUE, factors included random factors. Else, FALSE (default), included fixed effects simple regression model. additive TRUE, continuous variables included smooth terms additive models. goal regress-potential non-linear effects. bayesian TRUE, models fitted Bayesian framework using rstanarm. keep_intercept FALSE (default), intercept model re-added. avoids centering around 0 happens default regressing another variable (see examples visual representation ). ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/adjust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adjust data for the effect of other variable(s) — adjust","text":"data frame comparable data, adjusted variables.","code":""},{"path":"/reference/adjust.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adjust data for the effect of other variable(s) — adjust","text":"","code":"adjusted_all <- adjust(attitude) head(adjusted_all) #>        rating complaints privileges    learning     raises   critical #> 1  -8.1102953  5.5583770 -15.848949 -2.75102306  0.5742664  15.605502 #> 2   1.6472337  0.0646564  -1.422592 -3.06207012 -1.5567655  -2.315781 #> 3   1.0605589 -7.5116953  11.174609  5.59808033  4.8603132   8.061801 #> 4  -0.2268416  3.8345277  -4.567441  0.03866933 -7.1185324  13.002574 #> 5   6.5462010 -1.2420122  -3.051098  0.87312095 -2.7131349   6.500353 #> 6 -10.9418499  5.2030745   2.664156 -1.24552098  4.1370346 -21.678382 #>      advance #> 1  2.8684130 #> 2  5.3937097 #> 3 -6.4236221 #> 4 -0.3951046 #> 5  2.1988621 #> 6 -3.1912418 adjusted_one <- adjust(attitude, effect = \"complaints\", select = \"rating\") head(adjusted_one) #>        rating complaints privileges learning raises critical advance #> 1  -9.8614202         51         30       39     61       92      45 #> 2   0.3286522         64         51       54     63       73      47 #> 3   3.8009933         70         68       69     76       86      48 #> 4  -0.9167380         63         45       47     54       84      35 #> 5   7.7641147         78         56       66     71       83      47 #> 6 -12.8798594         55         49       44     54       49      34 # \\donttest{ adjust(attitude, effect = \"complaints\", select = \"rating\", bayesian = TRUE) #>          rating complaints privileges learning raises critical advance #> 1   -9.84362803         51         30       39     61       92      45 #> 2    0.32908421         64         51       54     63       73      47 #> 3    3.79341294         70         68       69     76       86      48 #> 4   -0.91497058         63         45       47     54       84      35 #> 5    7.74585124         78         56       66     71       83      47 #> 6  -12.86740888         55         49       44     54       49      34 #> 7   -6.93875143         67         42       56     66       68      35 #> 8    0.01368687         75         50       55     70       66      41 #> 9   -4.27792961         82         72       67     71       83      31 #> 10   6.59691985         61         45       47     62       80      41 #> 11   9.64448154         53         53       58     58       67      34 #> 12   7.35286506         60         47       39     59       74      41 #> 13   7.84097463         62         57       42     55       63      25 #> 14  -9.03387482         83         83       45     59       77      35 #> 15   4.50179645         77         54       72     79       77      46 #> 16  -1.32549131         90         50       72     60       54      36 #> 17  -4.54576525         85         64       69     79       79      63 #> 18   5.35286506         60         65       75     55       80      60 #> 19  -2.20658706         70         46       57     75       85      46 #> 20  -8.13524452         58         68       54     64       78      52 #> 21   5.47176930         40         33       34     43       64      33 #> 22   3.59691985         61         52       62     66       80      41 #> 23 -11.18280622         66         52       50     63       80      37 #> 24  -2.26039506         37         42       58     50       57      49 #> 25   7.88853633         54         42       48     66       75      33 #> 26  -6.49820355         77         66       63     88       76      72 #> 27   7.01368687         75         58       74     80       78      49 #> 28  -9.37929930         57         44       45     51       83      38 #> 29   6.45423475         85         71       71     77       74      55 #> 30   5.72207039         82         39       59     64       78      39 adjust(attitude, effect = \"complaints\", select = \"rating\", additive = TRUE) #>          rating complaints privileges learning raises critical advance #> 1   -9.86142016         51         30       39     61       92      45 #> 2    0.32865220         64         51       54     63       73      47 #> 3    3.80099328         70         68       69     76       86      48 #> 4   -0.91673799         63         45       47     54       84      35 #> 5    7.76411473         78         56       66     71       83      47 #> 6  -12.87985944         55         49       44     54       49      34 #> 7   -6.93517726         67         42       56     66       68      35 #> 8    0.02794419         75         50       55     70       66      41 #> 9   -4.25432454         82         72       67     71       83      31 #> 10   6.59248165         61         45       47     62       80      41 #> 11   9.62936020         53         53       58     58       67      34 #> 12   7.34709147         60         47       39     59       74      41 #> 13   7.83787183         62         57       42     55       63      25 #> 14  -9.00893436         83         83       45     59       77      35 #> 15   4.51872455         77         54       72     79       77      46 #> 16  -1.29120309         90         50       72     60       54      36 #> 17  -4.51815400         85         64       69     79       79      63 #> 18   5.34709147         60         65       75     55       80      60 #> 19  -2.19900672         70         46       57     75       85      46 #> 20  -8.14368889         58         68       54     64       78      52 #> 21   5.43928784         40         33       34     43       64      33 #> 22   3.59248165         61         52       62     66       80      41 #> 23 -11.18056744         66         52       50     63       80      37 #> 24  -2.29688270         37         42       58     50       57      49 #> 25   7.87475038         54         42       48     66       75      33 #> 26  -6.48127545         77         66       63     88       76      72 #> 27   7.02794419         75         58       74     80       78      49 #> 28  -9.38907907         57         44       45     51       83      38 #> 29   6.48184600         85         71       71     77       74      55 #> 30   5.74567546         82         39       59     64       78      39 attitude$complaints_LMH <- cut(attitude$complaints, 3) adjust(attitude, effect = \"complaints_LMH\", select = \"rating\", multilevel = TRUE) #>         rating complaints privileges learning raises critical advance #> 1   -9.9809282         51         30       39     61       92      45 #> 2    2.6250549         64         51       54     63       73      47 #> 3   10.6250549         70         68       69     76       86      48 #> 4    0.6250549         63         45       47     54       84      35 #> 5    5.6503521         78         56       66     71       83      47 #> 6  -17.3749451         55         49       44     54       49      34 #> 7   -2.3749451         67         42       56     66       68      35 #> 8   -4.3496479         75         50       55     70       66      41 #> 9   -3.3496479         82         72       67     71       83      31 #> 10   6.6250549         61         45       47     62       80      41 #> 11  11.0190718         53         53       58     58       67      34 #> 12   6.6250549         60         47       39     59       74      41 #> 13   8.6250549         62         57       42     55       63      25 #> 14  -7.3496479         83         83       45     59       77      35 #> 15   1.6503521         77         54       72     79       77      46 #> 16   5.6503521         90         50       72     60       54      36 #> 17  -1.3496479         85         64       69     79       79      63 #> 18   4.6250549         60         65       75     55       80      60 #> 19   4.6250549         70         46       57     75       85      46 #> 20 -10.3749451         58         68       54     64       78      52 #> 21  -2.9809282         40         33       34     43       64      33 #> 22   3.6250549         61         52       62     66       80      41 #> 23  -7.3749451         66         52       50     63       80      37 #> 24 -12.9809282         37         42       58     50       57      49 #> 25  10.0190718         54         42       48     66       75      33 #> 26  -9.3496479         77         66       63     88       76      72 #> 27   2.6503521         75         58       74     80       78      49 #> 28 -12.3749451         57         44       45     51       83      38 #> 29   9.6503521         85         71       71     77       74      55 #> 30   6.6503521         82         39       59     64       78      39 #>    complaints_LMH #> 1     (36.9,54.7] #> 2     (54.7,72.3] #> 3     (54.7,72.3] #> 4     (54.7,72.3] #> 5     (72.3,90.1] #> 6     (54.7,72.3] #> 7     (54.7,72.3] #> 8     (72.3,90.1] #> 9     (72.3,90.1] #> 10    (54.7,72.3] #> 11    (36.9,54.7] #> 12    (54.7,72.3] #> 13    (54.7,72.3] #> 14    (72.3,90.1] #> 15    (72.3,90.1] #> 16    (72.3,90.1] #> 17    (72.3,90.1] #> 18    (54.7,72.3] #> 19    (54.7,72.3] #> 20    (54.7,72.3] #> 21    (36.9,54.7] #> 22    (54.7,72.3] #> 23    (54.7,72.3] #> 24    (36.9,54.7] #> 25    (36.9,54.7] #> 26    (72.3,90.1] #> 27    (72.3,90.1] #> 28    (54.7,72.3] #> 29    (72.3,90.1] #> 30    (72.3,90.1] # }  if (require(\"MASS\") && require(\"bayestestR\")) {   # Generate data   data <- simulate_correlation(n = 100, r = 0.7)   data$V2 <- (5 * data$V2) + 20 # Add intercept    # Adjust   adjusted <- adjust(data, effect = \"V1\", select = \"V2\")   adjusted_icpt <- adjust(data, effect = \"V1\", select = \"V2\", keep_intercept = TRUE)    # Visualize   plot(data$V1, data$V2,     pch = 19, col = \"blue\",     ylim = c(min(adjusted$V2), max(data$V2)),     main = \"Original (blue), adjusted (green), and adjusted - intercept kept (red) data\"   )   abline(lm(V2 ~ V1, data = data), col = \"blue\")   points(adjusted$V1, adjusted$V2, pch = 19, col = \"green\")   abline(lm(V2 ~ V1, data = adjusted), col = \"green\")   points(adjusted_icpt$V1, adjusted_icpt$V2, pch = 19, col = \"red\")   abline(lm(V2 ~ V1, data = adjusted_icpt), col = \"red\") } #> Loading required package: MASS #> Loading required package: bayestestR"},{"path":"/reference/center.html","id":null,"dir":"Reference","previous_headings":"","what":"Centering (Grand-Mean Centering) — center","title":"Centering (Grand-Mean Centering) — center","text":"Performs grand-mean centering data.","code":""},{"path":"/reference/center.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Centering (Grand-Mean Centering) — center","text":"","code":"center(x, ...)  centre(x, ...)  # S3 method for numeric center(   x,   robust = FALSE,   weights = NULL,   reference = NULL,   center = NULL,   verbose = TRUE,   ... )  # S3 method for data.frame center(   x,   robust = FALSE,   weights = NULL,   reference = NULL,   center = NULL,   force = FALSE,   remove_na = c(\"none\", \"selected\", \"all\"),   append = FALSE,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   verbose = TRUE,   ... )"},{"path":"/reference/center.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Centering (Grand-Mean Centering) — center","text":"x (grouped) data frame, (numeric character) vector factor. ... Currently used. robust Logical, TRUE, centering done subtracting median variables. FALSE, variables centered subtracting mean. weights Can NULL (weighting), : data frames: numeric vector weights, character name column data.frame contains weights. numeric vectors: numeric vector weights. reference data frame variable centrality deviation computed instead input variable. Useful standardizing subset new data according another data frame. center Numeric value, can used alternative reference define reference centrality. center length 1, recycled match length selected variables centering. Else, center must length number selected variables. Values center matched selected variables provided order, unless named vector given. case, names matched names selected variables. verbose Toggle warnings messages. force Logical, TRUE, forces centering factors well. Factors converted numerical values, lowest level value 1 (unless factor numeric levels, converted corresponding numeric value). remove_na missing values (NA) treated: \"none\" (default): column's standardization done separately, ignoring NAs. Else, rows NA columns selected select / exclude (\"selected\") columns (\"\") dropped standardization, resulting data frame include cases. append Logical string. TRUE, centered variables get new column names (suffix \"_c\") appended (column bind) x, thus returning original centered variables. FALSE, original variables x overwritten centered versions. character value, centered variables appended new column names (using defined suffix) original data frame. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/center.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Centering (Grand-Mean Centering) — center","text":"centered variables.","code":""},{"path":"/reference/center.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Centering (Grand-Mean Centering) — center","text":"Difference centering standardizing: Standardized variables computed subtracting mean variable dividing standard deviation, centering variables involves subtraction.","code":""},{"path":[]},{"path":"/reference/center.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Centering (Grand-Mean Centering) — center","text":"","code":"data(iris)  # entire dataframe or a vector head(iris$Sepal.Width) #> [1] 3.5 3.0 3.2 3.1 3.6 3.9 head(center(iris$Sepal.Width)) #> [1]  0.44266667 -0.05733333  0.14266667  0.04266667  0.54266667  0.84266667 head(center(iris)) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   -0.7433333  0.44266667       -2.358  -0.9993333  setosa #> 2   -0.9433333 -0.05733333       -2.358  -0.9993333  setosa #> 3   -1.1433333  0.14266667       -2.458  -0.9993333  setosa #> 4   -1.2433333  0.04266667       -2.258  -0.9993333  setosa #> 5   -0.8433333  0.54266667       -2.358  -0.9993333  setosa #> 6   -0.4433333  0.84266667       -2.058  -0.7993333  setosa head(center(iris, force = TRUE)) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   -0.7433333  0.44266667       -2.358  -0.9993333      -1 #> 2   -0.9433333 -0.05733333       -2.358  -0.9993333      -1 #> 3   -1.1433333  0.14266667       -2.458  -0.9993333      -1 #> 4   -1.2433333  0.04266667       -2.258  -0.9993333      -1 #> 5   -0.8433333  0.54266667       -2.358  -0.9993333      -1 #> 6   -0.4433333  0.84266667       -2.058  -0.7993333      -1  # only the selected columns from a dataframe center(anscombe, select = c(\"x1\", \"x3\")) #>    x1 x2 x3 x4    y1   y2    y3    y4 #> 1   1 10  1  8  8.04 9.14  7.46  6.58 #> 2  -1  8 -1  8  6.95 8.14  6.77  5.76 #> 3   4 13  4  8  7.58 8.74 12.74  7.71 #> 4   0  9  0  8  8.81 8.77  7.11  8.84 #> 5   2 11  2  8  8.33 9.26  7.81  8.47 #> 6   5 14  5  8  9.96 8.10  8.84  7.04 #> 7  -3  6 -3  8  7.24 6.13  6.08  5.25 #> 8  -5  4 -5 19  4.26 3.10  5.39 12.50 #> 9   3 12  3  8 10.84 9.13  8.15  5.56 #> 10 -2  7 -2  8  4.82 7.26  6.42  7.91 #> 11 -4  5 -4  8  5.68 4.74  5.73  6.89 center(anscombe, exclude = c(\"x1\", \"x3\")) #>    x1 x2 x3 x4          y1         y2    y3         y4 #> 1  10  1 10 -1  0.53909091  1.6390909 -0.04 -0.9209091 #> 2   8 -1  8 -1 -0.55090909  0.6390909 -0.73 -1.7409091 #> 3  13  4 13 -1  0.07909091  1.2390909  5.24  0.2090909 #> 4   9  0  9 -1  1.30909091  1.2690909 -0.39  1.3390909 #> 5  11  2 11 -1  0.82909091  1.7590909  0.31  0.9690909 #> 6  14  5 14 -1  2.45909091  0.5990909  1.34 -0.4609091 #> 7   6 -3  6 -1 -0.26090909 -1.3709091 -1.42 -2.2509091 #> 8   4 -5  4 10 -3.24090909 -4.4009091 -2.11  4.9990909 #> 9  12  3 12 -1  3.33909091  1.6290909  0.65 -1.9409091 #> 10  7 -2  7 -1 -2.68090909 -0.2409091 -1.08  0.4090909 #> 11  5 -4  5 -1 -1.82090909 -2.7609091 -1.77 -0.6109091  # centering with reference center and scale d <- data.frame(   a = c(-2, -1, 0, 1, 2),   b = c(3, 4, 5, 6, 7) )  # default centering at mean center(d) #>    a  b #> 1 -2 -2 #> 2 -1 -1 #> 3  0  0 #> 4  1  1 #> 5  2  2  # centering, using 0 as mean center(d, center = 0) #>    a b #> 1 -2 3 #> 2 -1 4 #> 3  0 5 #> 4  1 6 #> 5  2 7  # centering, using -5 as mean center(d, center = -5) #>   a  b #> 1 3  8 #> 2 4  9 #> 3 5 10 #> 4 6 11 #> 5 7 12"},{"path":"/reference/convert_data_to_numeric.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert data to numeric — convert_data_to_numeric","title":"Convert data to numeric — convert_data_to_numeric","text":"Convert data numeric converting characters factors factors either numeric levels dummy variables.","code":""},{"path":"/reference/convert_data_to_numeric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert data to numeric — convert_data_to_numeric","text":"","code":"convert_data_to_numeric(x, ...)  data_to_numeric(x, ...)  # S3 method for data.frame convert_data_to_numeric(x, dummy_factors = TRUE, ...)"},{"path":"/reference/convert_data_to_numeric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert data to numeric — convert_data_to_numeric","text":"x data frame vector. ... Arguments passed methods. dummy_factors Transform factors dummy factors (factor levels different columns filled binary 0-1 value).","code":""},{"path":"/reference/convert_data_to_numeric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert data to numeric — convert_data_to_numeric","text":"data frame numeric variables.","code":""},{"path":"/reference/convert_data_to_numeric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert data to numeric — convert_data_to_numeric","text":"","code":"convert_data_to_numeric(head(ToothGrowth)) #>    len supp.OJ supp.VC dose #> 1  4.2       0       1  0.5 #> 2 11.5       0       1  0.5 #> 3  7.3       0       1  0.5 #> 4  5.8       0       1  0.5 #> 5  6.4       0       1  0.5 #> 6 10.0       0       1  0.5 convert_data_to_numeric(head(ToothGrowth), dummy_factors = FALSE) #>    len supp dose #> 1  4.2    2  0.5 #> 2 11.5    2  0.5 #> 3  7.3    2  0.5 #> 4  5.8    2  0.5 #> 5  6.4    2  0.5 #> 6 10.0    2  0.5"},{"path":"/reference/convert_na_to.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace missing values in a variable or a dataframe. — convert_na_to","title":"Replace missing values in a variable or a dataframe. — convert_na_to","text":"Replace missing values variable dataframe.","code":""},{"path":"/reference/convert_na_to.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace missing values in a variable or a dataframe. — convert_na_to","text":"","code":"convert_na_to(x, ...)  # S3 method for numeric convert_na_to(x, replacement = NULL, verbose = TRUE, ...)  # S3 method for character convert_na_to(x, replacement = NULL, verbose = TRUE, ...)  # S3 method for data.frame convert_na_to(   x,   replace_num = NULL,   replace_char = NULL,   replace_fac = NULL,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   verbose = TRUE,   ... )"},{"path":"/reference/convert_na_to.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace missing values in a variable or a dataframe. — convert_na_to","text":"x numeric, factor, character vector, data frame. ... used. replacement Numeric character value used replace NA. verbose Toggle warnings. replace_num Value replace NA variable type numeric. replace_char Value replace NA variable type character. replace_fac Value replace NA variable type factor. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/convert_na_to.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace missing values in a variable or a dataframe. — convert_na_to","text":"x, NA values replaced replacement.","code":""},{"path":"/reference/convert_na_to.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Replace missing values in a variable or a dataframe. — convert_na_to","text":"","code":"# Convert NA to 0 in a numeric vector convert_na_to(   c(9, 3, NA, 2, 3, 1, NA, 8),   replacement = 0 ) #> [1] 9 3 0 2 3 1 0 8  # Convert NA to \"missing\" in a character vector convert_na_to(   c(\"a\", NA, \"d\", \"z\", NA, \"t\"),   replacement = \"missing\" ) #> [1] \"a\"       \"missing\" \"d\"       \"z\"       \"missing\" \"t\"        ### For dataframes  test_df <- data.frame(   x = c(1, 2, NA),   x2 = c(4, 5, NA),   y = c(\"a\", \"b\", NA) )  # Convert all NA to 0 in numeric variables, and all NA to \"missing\" in # character variables convert_na_to(   test_df,   replace_num = 0,   replace_char = \"missing\" ) #>   x x2       y #> 1 1  4       a #> 2 2  5       b #> 3 0  0 missing  # Convert a specific variable in the dataframe convert_na_to(   test_df,   replace_num = 0,   replace_char = \"missing\",   select = \"x\" ) #>   x x2    y #> 1 1  4    a #> 2 2  5    b #> 3 0 NA <NA>  # Convert all variables starting with \"x\" convert_na_to(   test_df,   replace_num = 0,   replace_char = \"missing\",   select = starts_with(\"x\") ) #>   x x2    y #> 1 1  4    a #> 2 2  5    b #> 3 0  0 <NA>  # Convert NA to 1 in variable 'x2' and to 0 in all other numeric # variables convert_na_to(   test_df,   replace_num = 0,   select = list(x2 = 1) ) #>   x x2    y #> 1 1  4    a #> 2 2  5    b #> 3 0  1 <NA>"},{"path":"/reference/convert_to_na.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert non-missing values in a variable into missing values. — convert_to_na","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"Convert non-missing values variable missing values.","code":""},{"path":"/reference/convert_to_na.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"","code":"convert_to_na(x, ...)  # S3 method for numeric convert_to_na(x, na = NULL, verbose = TRUE, ...)  # S3 method for data.frame convert_to_na(   x,   na = NULL,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   verbose = TRUE,   ... )"},{"path":"/reference/convert_to_na.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"x vector, factor data frame. ... used. na Numeric character vector (list numeric character vectors) values converted NA. verbose Toggle warnings. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/convert_to_na.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"x, values na converted NA.","code":""},{"path":"/reference/convert_to_na.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert non-missing values in a variable into missing values. — convert_to_na","text":"","code":"x <- sample(1:6, size = 30, replace = TRUE) x #>  [1] 4 2 2 1 1 3 5 5 1 3 4 6 2 1 3 2 1 4 4 2 4 4 2 6 6 1 2 4 6 1 # values 4 and 5 to NA convert_to_na(x, na = 4:5) #>  [1] NA  2  2  1  1  3 NA NA  1  3 NA  6  2  1  3  2  1 NA NA  2 NA NA  2  6  6 #> [26]  1  2 NA  6  1  # data frames set.seed(123) x <- data.frame(   a = sample(1:6, size = 20, replace = TRUE),   b = sample(letters[1:6], size = 20, replace = TRUE),   c = sample(c(30:33, 99), size = 20, replace = TRUE) ) # for all numerics, convert 5 to NA. Character/factor will be ignored. convert_to_na(x, na = 5) #> Warning: `na` needs to be a character vector. #>     a b  c #> 1   3 a 33 #> 2   6 e 99 #> 3   3 c 99 #> 4   2 b 32 #> 5   2 b 30 #> 6   6 a 31 #> 7   3 f 99 #> 8  NA c 99 #> 9   4 d 33 #> 10  6 f 99 #> 11  6 a 31 #> 12  1 c 30 #> 13  2 e 30 #> 14  3 d 32 #> 15 NA b 30 #> 16  3 e 99 #> 17  3 a 30 #> 18  1 a 31 #> 19  4 b 33 #> 20  1 c 33  # for numerics, 5 to NA, for character/factor, \"f\" to NA convert_to_na(x, na = list(6, \"f\")) #>     a    b  c #> 1   3    a 33 #> 2  NA    e 99 #> 3   3    c 99 #> 4   2    b 32 #> 5   2    b 30 #> 6  NA    a 31 #> 7   3 <NA> 99 #> 8   5    c 99 #> 9   4    d 33 #> 10 NA <NA> 99 #> 11 NA    a 31 #> 12  1    c 30 #> 13  2    e 30 #> 14  3    d 32 #> 15  5    b 30 #> 16  3    e 99 #> 17  3    a 30 #> 18  1    a 31 #> 19  4    b 33 #> 20  1    c 33  # select specific variables convert_to_na(x, select = c(\"a\", \"b\"), na = list(6, \"f\")) #>     a    b  c #> 1   3    a 33 #> 2  NA    e 99 #> 3   3    c 99 #> 4   2    b 32 #> 5   2    b 30 #> 6  NA    a 31 #> 7   3 <NA> 99 #> 8   5    c 99 #> 9   4    d 33 #> 10 NA <NA> 99 #> 11 NA    a 31 #> 12  1    c 30 #> 13  2    e 30 #> 14  3    d 32 #> 15  5    b 30 #> 16  3    e 99 #> 17  3    a 30 #> 18  1    a 31 #> 19  4    b 33 #> 20  1    c 33"},{"path":"/reference/data_cut.html","id":null,"dir":"Reference","previous_headings":"","what":"Recode (or ","title":"Recode (or ","text":"functions divides range variables intervals recodes values inside intervals according related interval. basically wrapper around base R's cut(), providing simplified accessible way define interval breaks (cut-values).","code":""},{"path":"/reference/data_cut.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recode (or ","text":"","code":"data_cut(x, ...)  # S3 method for numeric data_cut(   x,   split = \"median\",   n_groups = NULL,   range = NULL,   lowest = 1,   labels = NULL,   verbose = TRUE,   ... )  # S3 method for data.frame data_cut(   x,   split = \"median\",   n_groups = NULL,   range = NULL,   lowest = 1,   labels = NULL,   force = FALSE,   append = FALSE,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   verbose = TRUE,   ... )"},{"path":"/reference/data_cut.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recode (or ","text":"x (grouped) data frame, numeric vector factor. ... used. split Character vector, indicating breaks split variables, numeric values values indicating breaks. character, may one \"median\", \"mean\", \"quantile\", \"equal_length\", \"equal_range\". \"median\" \"mean\" return dichotomous variables, split mean median, respectively. \"quantile\" \"equal_length\" split variable n_groups groups, group refers interval specific range values. Thus, length interval based number groups. \"equal_range\" also splits variable multiple groups, however, length interval given, number resulting groups (hence, number breaks) determined many intervals can generated, based full range variable. n_groups split \"quantile\" \"equal_length\", defines number requested groups (.e. resulting number levels values) recoded variable(s). \"quantile\" define intervals based distribution variable, \"equal_length\" tries divide range variable pieces equal length. range split = \"equal_range\", defines range values recoded new value. lowest Minimum value recoded variable(s). NULL (default), numeric variables, minimum original input preserved. factors, default minimum 1. split = \"equal_range\", default minimum always 1, unless specified otherwise lowest. labels Character vector value labels. NULL, data_cut() returns factors instead numeric variables, labels used labelling factor levels. verbose Toggle warnings. force Logical, TRUE, forces recoding factors well. append Logical string. TRUE, recoded variables get new column names (suffix \"_r\") appended (column bind) x, thus returning original recoded variables. FALSE, original variables x overwritten recoded versions. character value, recoded variables appended new column names (using defined suffix) original data frame. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/data_cut.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recode (or ","text":"x, recoded groups. default x numeric, unless labelsis specified. case, factor returned, factor levels (.e. recoded groups labelled accordingly.","code":""},{"path":[]},{"path":"/reference/data_cut.html","id":"splits-and-breaks-cut-off-values-","dir":"Reference","previous_headings":"","what":"Splits and breaks (cut-off values)","title":"Recode (or ","text":"Breaks general exclusive, means values indicate lower bound next group interval begin. Take simple example, numeric variable values 1 9. median 5, thus first interval ranges 1-4 recoded 1, 5-9 turn 2 (compare cbind(1:9, data_cut(1:9))). variable, using split = \"quantile\" n_groups = 3 define breaks 3.67 6.33 (see quantile(1:9, probs = c(1/3, 2/3)), means values 1 3 belong first interval recoded 1 (next interval starts 3.67), 4 6 2 7 9 3.","code":""},{"path":"/reference/data_cut.html","id":"recoding-into-groups-with-equal-size-or-range","dir":"Reference","previous_headings":"","what":"Recoding into groups with equal size or range","title":"Recode (or ","text":"split = \"equal_length\" split = \"equal_range\" try divide range x intervals similar () length. difference split = \"equal_length\" divide range x n_groups pieces thereby defining intervals used breaks (hence, equivalent cut(x, breaks = n_groups)),  split = \"equal_range\" cut x intervals length range, first interval defaults starts 1. lowest (starting) value interval can defined using lowest argument.","code":""},{"path":"/reference/data_cut.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recode (or ","text":"","code":"set.seed(123) x <- sample(1:10, size = 50, replace = TRUE)  table(x) #> x #>  1  2  3  4  5  6  7  8  9 10  #>  2  3  5  3  7  5  5  2 11  7   # by default, at median table(data_cut(x)) #>  #>  1  2  #> 25 25   # into 3 groups, based on distribution (quantiles) table(data_cut(x, split = \"quantile\", n_groups = 3)) #>  #>  1  2  3  #> 13 19 18   # into 3 groups, user-defined break table(data_cut(x, split = c(3, 5))) #>  #>  1  2  3  #>  5  8 37   set.seed(123) x <- sample(1:100, size = 500, replace = TRUE)  # into 5 groups, try to recode into intervals of similar length, # i.e. the range within groups is the same for all groups table(data_cut(x, split = \"equal_length\", n_groups = 5)) #>  #>   1   2   3   4   5  #>  89 116  96  94 105   # into 5 groups, try to return same range within groups # i.e. 1-20, 21-40, 41-60, etc. Since the range of \"x\" is # 1-100, and we have a range of 20, this results into 5 # groups, and thus is for this particular case identical # to the previous result. table(data_cut(x, split = \"equal_range\", range = 20)) #>  #>   1   2   3   4   5  #>  89 116  96  94 105   # return factor with value labels instead of numeric value set.seed(123) x <- sample(1:10, size = 30, replace = TRUE) data_cut(x, \"equal_length\", n_groups = 3) #>  [1] 1 1 3 1 2 2 2 2 3 3 2 1 3 3 3 1 3 3 3 3 3 1 2 1 3 2 3 3 3 3 data_cut(x, \"equal_length\", n_groups = 3, labels = c(\"low\", \"mid\", \"high\")) #>  [1] low  low  high low  mid  mid  mid  mid  high high mid  low  high high high #> [16] low  high high high high high low  mid  low  high mid  high high high high #> Levels: low mid high"},{"path":"/reference/data_extract.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract one or more columns or elements from an object — data_extract","title":"Extract one or more columns or elements from an object — data_extract","text":"data_extract() (alias extract()) similar $. extracts either single column element object (e.g., data frame, list), multiple columns resp. elements.","code":""},{"path":"/reference/data_extract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract one or more columns or elements from an object — data_extract","text":"","code":"data_extract(data, select, ...)  extract(data, select, ...)  # S3 method for data.frame data_extract(   data,   select,   name = NULL,   extract = \"all\",   as_data_frame = FALSE,   ignore_case = FALSE,   verbose = TRUE,   ... )"},{"path":"/reference/data_extract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract one or more columns or elements from an object — data_extract","text":"data object subset. Methods currently available data frames data frame extensions (e.g., tibbles). select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. ... use future methods. name optional argument specifies column used names vector elements extraction. Must specified either literal variable name (e.g., column_name) string (\"column_name\"). name ignored data frame returned. extract String, indicating element extracted select matches multiple variables. Can \"\" (default) return matched variables, \"first\" \"last\" return first last match, \"odd\" \"even\" return odd-numbered even-numbered matches. Note \"first\" \"last\" return vector (unless as_data_frame = TRUE), \"\" can return vector (one match found) data frame (one match). Type safe return values possible extract \"first\" \"last\" (always return vector) as_data_frame = TRUE (always returns data frame). as_data_frame Logical, TRUE, always return data frame, even one variable matched. FALSE, either returns vector data frame. See extract details. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names. verbose Toggle warnings.","code":""},{"path":"/reference/data_extract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract one or more columns or elements from an object — data_extract","text":"vector (data frame) containing extracted element, NULL matching variable found.","code":""},{"path":"/reference/data_extract.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract one or more columns or elements from an object — data_extract","text":"data_extract() can used select multiple variables pull single variable data frame. Thus, return value default type safe - data_extract() either returns vector data frame.","code":""},{"path":"/reference/data_extract.html","id":"extracting-single-variables-vectors-","dir":"Reference","previous_headings":"","what":"Extracting single variables (vectors)","title":"Extract one or more columns or elements from an object — data_extract","text":"select name single column, select matches one column, vector returned. single variable also returned extract either \"first \"last\". Setting as_data_frame TRUE overrides behaviour always returns data frame.","code":""},{"path":"/reference/data_extract.html","id":"extracting-a-data-frame-of-variables","dir":"Reference","previous_headings":"","what":"Extracting a data frame of variables","title":"Extract one or more columns or elements from an object — data_extract","text":"select character vector containing one column name (numeric vector one valid column indices), select uses one supported select-helpers match multiple columns, data frame returned. Setting as_data_frame TRUE always returns data frame.","code":""},{"path":"/reference/data_extract.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract one or more columns or elements from an object — data_extract","text":"","code":"# single variable extract(mtcars, cyl, name = gear) #> 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4  #> 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4  extract(mtcars, \"cyl\", name = gear) #> 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4  #> 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4  extract(mtcars, -1, name = gear) #> 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4  #> 4 4 1 1 2 1 4 2 2 4 4 3 3 3 4 4 4 1 2 1 1 2 2 4 2 1 2 2 4 6 8 2  extract(mtcars, cyl, name = 0) #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>                   6                   6                   4                   6  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>                   8                   6                   8                   4  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>                   4                   6                   6                   8  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>                   8                   8                   8                   8  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>                   8                   4                   4                   4  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>                   4                   8                   8                   8  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>                   8                   4                   4                   4  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>                   8                   6                   8                   4  extract(mtcars, cyl, name = \"row.names\") #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>                   6                   6                   4                   6  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>                   8                   6                   8                   4  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>                   4                   6                   6                   8  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>                   8                   8                   8                   8  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>                   8                   4                   4                   4  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>                   4                   8                   8                   8  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>                   8                   4                   4                   4  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>                   8                   6                   8                   4   # selecting multiple variables head(extract(iris, starts_with(\"Sepal\"))) #>   Sepal.Length Sepal.Width #> 1          5.1         3.5 #> 2          4.9         3.0 #> 3          4.7         3.2 #> 4          4.6         3.1 #> 5          5.0         3.6 #> 6          5.4         3.9 head(extract(iris, ends_with(\"Width\"))) #>   Sepal.Width Petal.Width #> 1         3.5         0.2 #> 2         3.0         0.2 #> 3         3.2         0.2 #> 4         3.1         0.2 #> 5         3.6         0.2 #> 6         3.9         0.4 head(extract(iris, 2:4)) #>   Sepal.Width Petal.Length Petal.Width #> 1         3.5          1.4         0.2 #> 2         3.0          1.4         0.2 #> 3         3.2          1.3         0.2 #> 4         3.1          1.5         0.2 #> 5         3.6          1.4         0.2 #> 6         3.9          1.7         0.4  # select first of multiple variables extract(iris, starts_with(\"Sepal\"), extract = \"first\") #>   [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 #>  [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 #>  [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5 #>  [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 #>  [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 #>  [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 #> [109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 #> [127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 #> [145] 6.7 6.7 6.3 6.5 6.2 5.9 # select first of multiple variables, return as data frame head(extract(iris, starts_with(\"Sepal\"), extract = \"first\", as_data_frame = TRUE)) #>   Sepal.Length #> 1          5.1 #> 2          4.9 #> 3          4.7 #> 4          4.6 #> 5          5.0 #> 6          5.4"},{"path":"/reference/data_match.html","id":null,"dir":"Reference","previous_headings":"","what":"Find row indices of a data frame matching a specific condition — data_match","title":"Find row indices of a data frame matching a specific condition — data_match","text":"Find row indices data frame match specific condition.","code":""},{"path":"/reference/data_match.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find row indices of a data frame matching a specific condition — data_match","text":"","code":"data_match(x, to, match = \"and\", return_indices = FALSE, ...)"},{"path":"/reference/data_match.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find row indices of a data frame matching a specific condition — data_match","text":"x data frame. data frame matching specified conditions. match String, indicating logical operation matching conditions combined. Can \"\" (\"&\"), \"\" (\"|\") \"\" (\"!\"). return_indices Logical, FALSE, return vector rows can used filter original data frame. FALSE (default), returns directly filtered data frame instead row indices. ... used.","code":""},{"path":"/reference/data_match.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find row indices of a data frame matching a specific condition — data_match","text":"row indices match specified configuration.","code":""},{"path":[]},{"path":"/reference/data_match.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find row indices of a data frame matching a specific condition — data_match","text":"","code":"data_match(mtcars, data.frame(vs = 0, am = 1)) #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 #> Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 #> Maserati Bora  15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8 data_match(mtcars, data.frame(vs = 0, am = c(0, 1))) #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4 #> Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3 #> Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3 #> Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3 #> Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4 #> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4 #> Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4 #> Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 #> AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2 #> Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4 #> Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2 #> Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 #> Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 #> Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  # observations where \"vs\" is NOT 0 AND \"am\" is NOT 1 data_match(mtcars, data.frame(vs = 0, am = 1), match = \"not\") #>                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 #> Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2 #> Merc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4 #> Merc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4 #> Toyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  # observations where EITHER \"vs\" is 0 OR \"am\" is 1 data_match(mtcars, data.frame(vs = 0, am = 1), match = \"or\") #>                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4 #> Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3 #> Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3 #> Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3 #> Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4 #> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4 #> Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4 #> Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 #> AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2 #> Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4 #> Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2 #> Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 #> Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 #> Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8 #> Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 #> Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 #> Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 #> Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 #> Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 #> Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"},{"path":"/reference/data_merge.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge (join) two data frames, or a list of data frames — data_merge","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Merge (join) two data frames, list data frames. However, unlike base R's merge(), data_merge() offers methods join data frames, drop data frame column attributes.","code":""},{"path":"/reference/data_merge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"","code":"data_merge(x, ...)  data_join(x, ...)  # S3 method for data.frame data_merge(x, y, join = \"left\", by = NULL, id = NULL, verbose = TRUE, ...)  # S3 method for list data_merge(x, join = \"left\", by = NULL, id = NULL, verbose = TRUE, ...)"},{"path":"/reference/data_merge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"x, y data frame merge. x may also list data frames merged. Note list-method y argument. ... used. join Character vector, indicating method joining data frames. Can \"full\", \"left\" (default), \"right\", \"inner\", \"anti\", \"semi\" \"bind\". See details . Specifications columns used merging. id Optional name ID column created indicate source data frames appended rows. applies join = \"bind\". verbose Toggle warnings.","code":""},{"path":"/reference/data_merge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"merged data frame.","code":""},{"path":[]},{"path":"/reference/data_merge.html","id":"merging-data-frames","dir":"Reference","previous_headings":"","what":"Merging data frames","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Merging data frames performed adding rows (cases), columns (variables) source data frame (y) target data frame (x). usually requires one variables included data frames used merging, typically indicated argument. contains variable present data frames, cases matched filtered identical values x y.","code":""},{"path":"/reference/data_merge.html","id":"left-and-right-joins","dir":"Reference","previous_headings":"","what":"Left- and right-joins","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Left- right joins usually add new rows (cases), new columns (variables) existing cases x. join = \"left\" join = \"right\" work, must indicate one columns included data frames. join = \"left\", identifier variable, included x y, variables y copied x, cases y matching values identifier variable x (.e. cases x also found y get related values new columns y). match identifiers x y, copied variable y get NA value particular case. variables occur x y, used identifiers (), renamed avoid multiple identical variable names. Cases y values identifier match x's identifier removed. join = \"right\" works similar way join = \"left\", just cases x matching values identifier variable y chosen.  base R, equivalent merge(x, y, .x = TRUE) merge(x, y, .y = TRUE).","code":""},{"path":"/reference/data_merge.html","id":"full-joins","dir":"Reference","previous_headings":"","what":"Full joins","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Full joins copy cases y x. matching cases data frames, values new variables copied y x. cases y present x, added new rows x. Thus, full joins add new columns (variables), also might add new rows (cases).  base R, equivalent merge(x, y, = TRUE).","code":""},{"path":"/reference/data_merge.html","id":"inner-joins","dir":"Reference","previous_headings":"","what":"Inner joins","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"Inner joins merge two data frames, however, rows (cases) kept present data frames. Thus, inner joins usually add new columns (variables), also remove rows (cases) occur one data frame.  base R, equivalent merge(x, y).","code":""},{"path":"/reference/data_merge.html","id":"binds","dir":"Reference","previous_headings":"","what":"Binds","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"join = \"bind\" row-binds complete second data frame y x. Unlike simple rbind(), requires columns data frames, join = \"bind\" bind shared columns y x, add new columns y x.","code":""},{"path":[]},{"path":"/reference/data_merge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge (join) two data frames, or a list of data frames — data_merge","text":"","code":"x <- data.frame(a = 1:3, b = c(\"a\", \"b\", \"c\"), c = 5:7, id = 1:3) y <- data.frame(c = 6:8, d = c(\"f\", \"g\", \"h\"), e = 100:102, id = 2:4)  x #>   a b c id #> 1 1 a 5  1 #> 2 2 b 6  2 #> 3 3 c 7  3 y #>   c d   e id #> 1 6 f 100  2 #> 2 7 g 101  3 #> 3 8 h 102  4  # \"by\" will default to all shared columns, i.e. \"c\" and \"id\". new columns # \"d\" and \"e\" will be copied from \"y\" to \"x\", but there are only two cases # in \"x\" that have the same values for \"c\" and \"id\" in \"y\". only those cases # have values in the copied columns, the other case gets \"NA\". data_merge(x, y, join = \"left\") #>   a b c id    d   e #> 3 1 a 5  1 <NA>  NA #> 1 2 b 6  2    f 100 #> 2 3 c 7  3    g 101  # we change the id-value here x <- data.frame(a = 1:3, b = c(\"a\", \"b\", \"c\"), c = 5:7, id = 1:3) y <- data.frame(c = 6:8, d = c(\"f\", \"g\", \"h\"), e = 100:102, id = 3:5)  x #>   a b c id #> 1 1 a 5  1 #> 2 2 b 6  2 #> 3 3 c 7  3 y #>   c d   e id #> 1 6 f 100  3 #> 2 7 g 101  4 #> 3 8 h 102  5  # no cases in \"y\" have the same matching \"c\" and \"id\" as in \"x\", thus # copied variables from \"y\" to \"x\" copy no values, all get NA. data_merge(x, y, join = \"left\") #>   a b c id    d  e #> 1 1 a 5  1 <NA> NA #> 2 2 b 6  2 <NA> NA #> 3 3 c 7  3 <NA> NA  # one case in \"y\" has a match in \"id\" with \"x\", thus values for this # case from the remaining variables in \"y\" are copied to \"x\", all other # values (cases) in those remaining variables get NA data_merge(x, y, join = \"left\", by = \"id\") #>   a b id    d   e c.x c.y #> 2 1 a  1 <NA>  NA   5  NA #> 3 2 b  2 <NA>  NA   6  NA #> 1 3 c  3    f 100   7   6  data(mtcars) x <- mtcars[1:5, 1:3] y <- mtcars[28:32, 4:6]  # add ID common column x$id <- 1:5 y$id <- 3:7  # left-join, add new variables and copy values from y to x, # where \"id\" values match data_merge(x, y) #>    mpg cyl disp id  hp drat    wt #> 4 21.0   6  160  1  NA   NA    NA #> 5 21.0   6  160  2  NA   NA    NA #> 1 22.8   4  108  3 113 3.77 1.513 #> 2 21.4   6  258  4 264 4.22 3.170 #> 3 18.7   8  360  5 175 3.62 2.770  # right-join, add new variables and copy values from x to y, # where \"id\" values match data_merge(x, y, join = \"right\") #>    mpg cyl disp id  hp drat    wt #> 1 22.8   4  108  3 113 3.77 1.513 #> 2 21.4   6  258  4 264 4.22 3.170 #> 3 18.7   8  360  5 175 3.62 2.770 #> 4   NA  NA   NA  6 335 3.54 3.570 #> 5   NA  NA   NA  7 109 4.11 2.780  # full-join data_merge(x, y, join = \"full\") #>    mpg cyl disp id  hp drat    wt #> 4 21.0   6  160  1  NA   NA    NA #> 5 21.0   6  160  2  NA   NA    NA #> 1 22.8   4  108  3 113 3.77 1.513 #> 2 21.4   6  258  4 264 4.22 3.170 #> 3 18.7   8  360  5 175 3.62 2.770 #> 6   NA  NA   NA  6 335 3.54 3.570 #> 7   NA  NA   NA  7 109 4.11 2.780   data(mtcars) x <- mtcars[1:5, 1:3] y <- mtcars[28:32, c(1, 4:5)]  # add ID common column x$id <- 1:5 y$id <- 3:7  # left-join, no matching rows (because columns \"id\" and \"disp\" are used) # new variables get all NA values data_merge(x, y) #>    mpg cyl disp id hp drat #> 1 21.0   6  160  1 NA   NA #> 2 21.0   6  160  2 NA   NA #> 3 22.8   4  108  3 NA   NA #> 4 21.4   6  258  4 NA   NA #> 5 18.7   8  360  5 NA   NA  # one common value in \"mpg\", so one row from y is copied to x data_merge(x, y, by = \"mpg\") #>    mpg cyl disp  hp drat id.x id.y #> 2 21.0   6  160  NA   NA    1   NA #> 3 21.0   6  160  NA   NA    2   NA #> 4 22.8   4  108  NA   NA    3   NA #> 1 21.4   6  258 109 4.11    4    7 #> 5 18.7   8  360  NA   NA    5   NA  # only keep rows with matching values in by-column data_merge(x, y, join = \"semi\", by = \"mpg\") #>                 mpg cyl disp id #> Hornet 4 Drive 21.4   6  258  4  # only keep rows with non-matching values in by-column data_merge(x, y, join = \"anti\", by = \"mpg\") #>                    mpg cyl disp id #> Mazda RX4         21.0   6  160  1 #> Mazda RX4 Wag     21.0   6  160  2 #> Datsun 710        22.8   4  108  3 #> Hornet Sportabout 18.7   8  360  5  # merge list of data frames. can be of different rows x <- mtcars[1:5, 1:3] y <- mtcars[28:31, 3:5] z <- mtcars[11:18, c(1, 3:4, 6:8)] x$id <- 1:5 y$id <- 4:7 z$id <- 3:10 data_merge(list(x, y, z), join = \"bind\", by = \"id\", id = \"source\") #>     mpg cyl  disp id  hp drat    wt  qsec vs source #> 1  21.0   6 160.0  1  NA   NA    NA    NA NA      1 #> 2  21.0   6 160.0  2  NA   NA    NA    NA NA      1 #> 3  22.8   4 108.0  3  NA   NA    NA    NA NA      1 #> 4  21.4   6 258.0  4  NA   NA    NA    NA NA      1 #> 5  18.7   8 360.0  5  NA   NA    NA    NA NA      1 #> 6    NA  NA  95.1  4 113 3.77    NA    NA NA      2 #> 7    NA  NA 351.0  5 264 4.22    NA    NA NA      2 #> 8    NA  NA 145.0  6 175 3.62    NA    NA NA      2 #> 9    NA  NA 301.0  7 335 3.54    NA    NA NA      2 #> 10 17.8  NA 167.6  3 123   NA 3.440 18.90  1      3 #> 11 16.4  NA 275.8  4 180   NA 4.070 17.40  0      3 #> 12 17.3  NA 275.8  5 180   NA 3.730 17.60  0      3 #> 13 15.2  NA 275.8  6 180   NA 3.780 18.00  0      3 #> 14 10.4  NA 472.0  7 205   NA 5.250 17.98  0      3 #> 15 10.4  NA 460.0  8 215   NA 5.424 17.82  0      3 #> 16 14.7  NA 440.0  9 230   NA 5.345 17.42  0      3 #> 17 32.4  NA  78.7 10  66   NA 2.200 19.47  1      3"},{"path":"/reference/data_partition.html","id":null,"dir":"Reference","previous_headings":"","what":"Partition data into a test and a training set — data_partition","title":"Partition data into a test and a training set — data_partition","text":"Creates training test set based dataframe. Can also stratified (.e., evenly spread given factor) using group argument.","code":""},{"path":"/reference/data_partition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partition data into a test and a training set — data_partition","text":"","code":"data_partition(data, training_proportion = 0.7, group = NULL, seed = NULL, ...)"},{"path":"/reference/data_partition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partition data into a test and a training set — data_partition","text":"data data frame, object can coerced data frame. training_proportion proportion (0 1) training set. remaining part used test set. group character vector indicating name(s) column(s) used stratified partitioning. seed random number generator seed. Enter integer (e.g. 123) random sampling time run function. ... arguments passed functions.","code":""},{"path":"/reference/data_partition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partition data into a test and a training set — data_partition","text":"list two data frames, named test training.","code":""},{"path":[]},{"path":"/reference/data_partition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Partition data into a test and a training set — data_partition","text":"","code":"df <- iris df$Smell <- rep(c(\"Strong\", \"Light\"), 75)  data_partition(df) #> $training #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 74           6.1         2.8          4.7         1.2 versicolor  Light #> 23           4.6         3.6          1.0         0.2     setosa Strong #> 53           6.9         3.1          4.9         1.5 versicolor Strong #> 135          6.1         2.6          5.6         1.4  virginica Strong #> 148          6.5         3.0          5.2         2.0  virginica  Light #> 34           5.5         4.2          1.4         0.2     setosa  Light #> 69           6.2         2.2          4.5         1.5 versicolor Strong #> 72           6.1         2.8          4.0         1.3 versicolor  Light #> 76           6.6         3.0          4.4         1.4 versicolor  Light #> 63           6.0         2.2          4.0         1.0 versicolor Strong #> 97           5.7         2.9          4.2         1.3 versicolor Strong #> 91           5.5         2.6          4.4         1.2 versicolor Strong #> 38           4.9         3.6          1.4         0.1     setosa  Light #> 21           5.4         3.4          1.7         0.2     setosa Strong #> 41           5.0         3.5          1.3         0.3     setosa Strong #> 90           5.5         2.5          4.0         1.3 versicolor  Light #> 60           5.2         2.7          3.9         1.4 versicolor  Light #> 16           5.7         4.4          1.5         0.4     setosa  Light #> 116          6.4         3.2          5.3         2.3  virginica  Light #> 94           5.0         2.3          3.3         1.0 versicolor  Light #> 6            5.4         3.9          1.7         0.4     setosa  Light #> 86           6.0         3.4          4.5         1.6 versicolor  Light #> 129          6.4         2.8          5.6         2.1  virginica Strong #> 39           4.4         3.0          1.3         0.2     setosa Strong #> 31           4.8         3.1          1.6         0.2     setosa Strong #> 112          6.4         2.7          5.3         1.9  virginica  Light #> 81           5.5         2.4          3.8         1.1 versicolor Strong #> 118          7.7         3.8          6.7         2.2  virginica  Light #> 50           5.0         3.3          1.4         0.2     setosa  Light #> 113          6.8         3.0          5.5         2.1  virginica Strong #> 145          6.7         3.3          5.7         2.5  virginica Strong #> 4            4.6         3.1          1.5         0.2     setosa  Light #> 13           4.8         3.0          1.4         0.1     setosa Strong #> 144          6.8         3.2          5.9         2.3  virginica  Light #> 115          5.8         2.8          5.1         2.4  virginica Strong #> 25           4.8         3.4          1.9         0.2     setosa Strong #> 52           6.4         3.2          4.5         1.5 versicolor  Light #> 22           5.1         3.7          1.5         0.4     setosa  Light #> 89           5.6         3.0          4.1         1.3 versicolor Strong #> 32           5.4         3.4          1.5         0.4     setosa  Light #> 110          7.2         3.6          6.1         2.5  virginica  Light #> 132          7.9         3.8          6.4         2.0  virginica  Light #> 87           6.7         3.1          4.7         1.5 versicolor Strong #> 35           4.9         3.1          1.5         0.2     setosa Strong #> 40           5.1         3.4          1.5         0.2     setosa  Light #> 30           4.7         3.2          1.6         0.2     setosa  Light #> 12           4.8         3.4          1.6         0.2     setosa  Light #> 126          7.2         3.2          6.0         1.8  virginica  Light #> 105          6.5         3.0          5.8         2.2  virginica Strong #> 64           6.1         2.9          4.7         1.4 versicolor  Light #> 99           5.1         2.5          3.0         1.1 versicolor Strong #> 14           4.3         3.0          1.1         0.1     setosa  Light #> 93           5.8         2.6          4.0         1.2 versicolor Strong #> 96           5.7         3.0          4.2         1.2 versicolor  Light #> 71           5.9         3.2          4.8         1.8 versicolor Strong #> 67           5.6         3.0          4.5         1.5 versicolor Strong #> 149          6.2         3.4          5.4         2.3  virginica Strong #> 79           6.0         2.9          4.5         1.5 versicolor Strong #> 85           5.4         3.0          4.5         1.5 versicolor Strong #> 37           5.5         3.5          1.3         0.2     setosa Strong #> 8            5.0         3.4          1.5         0.2     setosa  Light #> 51           7.0         3.2          4.7         1.4 versicolor Strong #> 150          5.9         3.0          5.1         1.8  virginica  Light #> 122          5.6         2.8          4.9         2.0  virginica  Light #> 88           6.3         2.3          4.4         1.3 versicolor  Light #> 142          6.9         3.1          5.1         2.3  virginica  Light #> 84           6.0         2.7          5.1         1.6 versicolor  Light #> 46           4.8         3.0          1.4         0.3     setosa  Light #> 17           5.4         3.9          1.3         0.4     setosa Strong #> 62           5.9         3.0          4.2         1.5 versicolor  Light #> 83           5.8         2.7          3.9         1.2 versicolor Strong #> 54           5.5         2.3          4.0         1.3 versicolor  Light #> 107          4.9         2.5          4.5         1.7  virginica Strong #> 24           5.1         3.3          1.7         0.5     setosa  Light #> 7            4.6         3.4          1.4         0.3     setosa Strong #> 131          7.4         2.8          6.1         1.9  virginica Strong #> 26           5.0         3.0          1.6         0.2     setosa  Light #> 111          6.5         3.2          5.1         2.0  virginica Strong #> 92           6.1         3.0          4.6         1.4 versicolor  Light #> 27           5.0         3.4          1.6         0.4     setosa Strong #> 42           4.5         2.3          1.3         0.3     setosa  Light #> 5            5.0         3.6          1.4         0.2     setosa Strong #> 133          6.4         2.8          5.6         2.2  virginica Strong #> 77           6.8         2.8          4.8         1.4 versicolor Strong #> 73           6.3         2.5          4.9         1.5 versicolor Strong #> 137          6.3         3.4          5.6         2.4  virginica Strong #> 55           6.5         2.8          4.6         1.5 versicolor Strong #> 11           5.4         3.7          1.5         0.2     setosa Strong #> 36           5.0         3.2          1.2         0.2     setosa  Light #> 44           5.0         3.5          1.6         0.6     setosa  Light #> 80           5.7         2.6          3.5         1.0 versicolor  Light #> 19           5.7         3.8          1.7         0.3     setosa Strong #> 109          6.7         2.5          5.8         1.8  virginica Strong #> 127          6.2         2.8          4.8         1.8  virginica Strong #> 98           6.2         2.9          4.3         1.3 versicolor  Light #> 128          6.1         3.0          4.9         1.8  virginica  Light #> 9            4.4         2.9          1.4         0.2     setosa Strong #> 143          5.8         2.7          5.1         1.9  virginica Strong #> 120          6.0         2.2          5.0         1.5  virginica  Light #> 48           4.6         3.2          1.4         0.2     setosa  Light #> 123          7.7         2.8          6.7         2.0  virginica Strong #> 59           6.6         2.9          4.6         1.3 versicolor Strong #> 47           5.1         3.8          1.6         0.2     setosa Strong #> 57           6.3         3.3          4.7         1.6 versicolor Strong #> 119          7.7         2.6          6.9         2.3  virginica Strong #>  #> $test #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 1            5.1         3.5          1.4         0.2     setosa Strong #> 2            4.9         3.0          1.4         0.2     setosa  Light #> 3            4.7         3.2          1.3         0.2     setosa Strong #> 10           4.9         3.1          1.5         0.1     setosa  Light #> 15           5.8         4.0          1.2         0.2     setosa Strong #> 18           5.1         3.5          1.4         0.3     setosa  Light #> 20           5.1         3.8          1.5         0.3     setosa  Light #> 28           5.2         3.5          1.5         0.2     setosa  Light #> 29           5.2         3.4          1.4         0.2     setosa Strong #> 33           5.2         4.1          1.5         0.1     setosa Strong #> 43           4.4         3.2          1.3         0.2     setosa Strong #> 45           5.1         3.8          1.9         0.4     setosa Strong #> 49           5.3         3.7          1.5         0.2     setosa Strong #> 56           5.7         2.8          4.5         1.3 versicolor  Light #> 58           4.9         2.4          3.3         1.0 versicolor  Light #> 61           5.0         2.0          3.5         1.0 versicolor Strong #> 65           5.6         2.9          3.6         1.3 versicolor Strong #> 66           6.7         3.1          4.4         1.4 versicolor  Light #> 68           5.8         2.7          4.1         1.0 versicolor  Light #> 70           5.6         2.5          3.9         1.1 versicolor  Light #> 75           6.4         2.9          4.3         1.3 versicolor Strong #> 78           6.7         3.0          5.0         1.7 versicolor  Light #> 82           5.5         2.4          3.7         1.0 versicolor  Light #> 95           5.6         2.7          4.2         1.3 versicolor Strong #> 100          5.7         2.8          4.1         1.3 versicolor  Light #> 101          6.3         3.3          6.0         2.5  virginica Strong #> 102          5.8         2.7          5.1         1.9  virginica  Light #> 103          7.1         3.0          5.9         2.1  virginica Strong #> 104          6.3         2.9          5.6         1.8  virginica  Light #> 106          7.6         3.0          6.6         2.1  virginica  Light #> 108          7.3         2.9          6.3         1.8  virginica  Light #> 114          5.7         2.5          5.0         2.0  virginica  Light #> 117          6.5         3.0          5.5         1.8  virginica Strong #> 121          6.9         3.2          5.7         2.3  virginica Strong #> 124          6.3         2.7          4.9         1.8  virginica  Light #> 125          6.7         3.3          5.7         2.1  virginica Strong #> 130          7.2         3.0          5.8         1.6  virginica  Light #> 134          6.3         2.8          5.1         1.5  virginica  Light #> 136          7.7         3.0          6.1         2.3  virginica  Light #> 138          6.4         3.1          5.5         1.8  virginica  Light #> 139          6.0         3.0          4.8         1.8  virginica Strong #> 140          6.9         3.1          5.4         2.1  virginica  Light #> 141          6.7         3.1          5.6         2.4  virginica Strong #> 146          6.7         3.0          5.2         2.3  virginica  Light #> 147          6.3         2.5          5.0         1.9  virginica Strong #>  data_partition(df, group = \"Species\") #> $training #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 1            5.1         3.5          1.4         0.2     setosa Strong #> 40           5.1         3.4          1.5         0.2     setosa  Light #> 30           4.7         3.2          1.6         0.2     setosa  Light #> 48           4.6         3.2          1.4         0.2     setosa  Light #> 25           4.8         3.4          1.9         0.2     setosa Strong #> 16           5.7         4.4          1.5         0.4     setosa  Light #> 24           5.1         3.3          1.7         0.5     setosa  Light #> 11           5.4         3.7          1.5         0.2     setosa Strong #> 20           5.1         3.8          1.5         0.3     setosa  Light #> 49           5.3         3.7          1.5         0.2     setosa Strong #> 3            4.7         3.2          1.3         0.2     setosa Strong #> 29           5.2         3.4          1.4         0.2     setosa Strong #> 36           5.0         3.2          1.2         0.2     setosa  Light #> 22           5.1         3.7          1.5         0.4     setosa  Light #> 42           4.5         2.3          1.3         0.3     setosa  Light #> 43           4.4         3.2          1.3         0.2     setosa Strong #> 8            5.0         3.4          1.5         0.2     setosa  Light #> 21           5.4         3.4          1.7         0.2     setosa Strong #> 13           4.8         3.0          1.4         0.1     setosa Strong #> 2            4.9         3.0          1.4         0.2     setosa  Light #> 35           4.9         3.1          1.5         0.2     setosa Strong #> 32           5.4         3.4          1.5         0.4     setosa  Light #> 14           4.3         3.0          1.1         0.1     setosa  Light #> 6            5.4         3.9          1.7         0.4     setosa  Light #> 46           4.8         3.0          1.4         0.3     setosa  Light #> 34           5.5         4.2          1.4         0.2     setosa  Light #> 12           4.8         3.4          1.6         0.2     setosa  Light #> 4            4.6         3.1          1.5         0.2     setosa  Light #> 39           4.4         3.0          1.3         0.2     setosa Strong #> 28           5.2         3.5          1.5         0.2     setosa  Light #> 45           5.1         3.8          1.9         0.4     setosa Strong #> 50           5.0         3.3          1.4         0.2     setosa  Light #> 26           5.0         3.0          1.6         0.2     setosa  Light #> 18           5.1         3.5          1.4         0.3     setosa  Light #> 10           4.9         3.1          1.5         0.1     setosa  Light #> 75           6.4         2.9          4.3         1.3 versicolor Strong #> 58           4.9         2.4          3.3         1.0 versicolor  Light #> 68           5.8         2.7          4.1         1.0 versicolor  Light #> 59           6.6         2.9          4.6         1.3 versicolor Strong #> 57           6.3         3.3          4.7         1.6 versicolor Strong #> 96           5.7         3.0          4.2         1.2 versicolor  Light #> 60           5.2         2.7          3.9         1.4 versicolor  Light #> 74           6.1         2.8          4.7         1.2 versicolor  Light #> 73           6.3         2.5          4.9         1.5 versicolor Strong #> 76           6.6         3.0          4.4         1.4 versicolor  Light #> 83           5.8         2.7          3.9         1.2 versicolor Strong #> 79           6.0         2.9          4.5         1.5 versicolor Strong #> 94           5.0         2.3          3.3         1.0 versicolor  Light #> 63           6.0         2.2          4.0         1.0 versicolor Strong #> 61           5.0         2.0          3.5         1.0 versicolor Strong #> 100          5.7         2.8          4.1         1.3 versicolor  Light #> 91           5.5         2.6          4.4         1.2 versicolor Strong #> 95           5.6         2.7          4.2         1.3 versicolor Strong #> 85           5.4         3.0          4.5         1.5 versicolor Strong #> 92           6.1         3.0          4.6         1.4 versicolor  Light #> 84           6.0         2.7          5.1         1.6 versicolor  Light #> 70           5.6         2.5          3.9         1.1 versicolor  Light #> 93           5.8         2.6          4.0         1.2 versicolor Strong #> 82           5.5         2.4          3.7         1.0 versicolor  Light #> 97           5.7         2.9          4.2         1.3 versicolor Strong #> 80           5.7         2.6          3.5         1.0 versicolor  Light #> 55           6.5         2.8          4.6         1.5 versicolor Strong #> 81           5.5         2.4          3.8         1.1 versicolor Strong #> 64           6.1         2.9          4.7         1.4 versicolor  Light #> 72           6.1         2.8          4.0         1.3 versicolor  Light #> 56           5.7         2.8          4.5         1.3 versicolor  Light #> 51           7.0         3.2          4.7         1.4 versicolor Strong #> 88           6.3         2.3          4.4         1.3 versicolor  Light #> 67           5.6         3.0          4.5         1.5 versicolor Strong #> 98           6.2         2.9          4.3         1.3 versicolor  Light #> 117          6.5         3.0          5.5         1.8  virginica Strong #> 129          6.4         2.8          5.6         2.1  virginica Strong #> 126          7.2         3.2          6.0         1.8  virginica  Light #> 127          6.2         2.8          4.8         1.8  virginica Strong #> 121          6.9         3.2          5.7         2.3  virginica Strong #> 107          4.9         2.5          4.5         1.7  virginica Strong #> 148          6.5         3.0          5.2         2.0  virginica  Light #> 141          6.7         3.1          5.6         2.4  virginica Strong #> 120          6.0         2.2          5.0         1.5  virginica  Light #> 106          7.6         3.0          6.6         2.1  virginica  Light #> 130          7.2         3.0          5.8         1.6  virginica  Light #> 131          7.4         2.8          6.1         1.9  virginica Strong #> 149          6.2         3.4          5.4         2.3  virginica Strong #> 150          5.9         3.0          5.1         1.8  virginica  Light #> 142          6.9         3.1          5.1         2.3  virginica  Light #> 135          6.1         2.6          5.6         1.4  virginica Strong #> 125          6.7         3.3          5.7         2.1  virginica Strong #> 133          6.4         2.8          5.6         2.2  virginica Strong #> 102          5.8         2.7          5.1         1.9  virginica  Light #> 104          6.3         2.9          5.6         1.8  virginica  Light #> 110          7.2         3.6          6.1         2.5  virginica  Light #> 101          6.3         3.3          6.0         2.5  virginica Strong #> 105          6.5         3.0          5.8         2.2  virginica Strong #> 146          6.7         3.0          5.2         2.3  virginica  Light #> 134          6.3         2.8          5.1         1.5  virginica  Light #> 108          7.3         2.9          6.3         1.8  virginica  Light #> 123          7.7         2.8          6.7         2.0  virginica Strong #> 147          6.3         2.5          5.0         1.9  virginica Strong #> 113          6.8         3.0          5.5         2.1  virginica Strong #> 118          7.7         3.8          6.7         2.2  virginica  Light #> 140          6.9         3.1          5.4         2.1  virginica  Light #> 143          5.8         2.7          5.1         1.9  virginica Strong #> 145          6.7         3.3          5.7         2.5  virginica Strong #> 109          6.7         2.5          5.8         1.8  virginica Strong #> 116          6.4         3.2          5.3         2.3  virginica  Light #>  #> $test #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 5            5.0         3.6          1.4         0.2     setosa Strong #> 7            4.6         3.4          1.4         0.3     setosa Strong #> 9            4.4         2.9          1.4         0.2     setosa Strong #> 15           5.8         4.0          1.2         0.2     setosa Strong #> 17           5.4         3.9          1.3         0.4     setosa Strong #> 19           5.7         3.8          1.7         0.3     setosa Strong #> 23           4.6         3.6          1.0         0.2     setosa Strong #> 27           5.0         3.4          1.6         0.4     setosa Strong #> 31           4.8         3.1          1.6         0.2     setosa Strong #> 33           5.2         4.1          1.5         0.1     setosa Strong #> 37           5.5         3.5          1.3         0.2     setosa Strong #> 38           4.9         3.6          1.4         0.1     setosa  Light #> 41           5.0         3.5          1.3         0.3     setosa Strong #> 44           5.0         3.5          1.6         0.6     setosa  Light #> 47           5.1         3.8          1.6         0.2     setosa Strong #> 52           6.4         3.2          4.5         1.5 versicolor  Light #> 53           6.9         3.1          4.9         1.5 versicolor Strong #> 54           5.5         2.3          4.0         1.3 versicolor  Light #> 62           5.9         3.0          4.2         1.5 versicolor  Light #> 65           5.6         2.9          3.6         1.3 versicolor Strong #> 66           6.7         3.1          4.4         1.4 versicolor  Light #> 69           6.2         2.2          4.5         1.5 versicolor Strong #> 71           5.9         3.2          4.8         1.8 versicolor Strong #> 77           6.8         2.8          4.8         1.4 versicolor Strong #> 78           6.7         3.0          5.0         1.7 versicolor  Light #> 86           6.0         3.4          4.5         1.6 versicolor  Light #> 87           6.7         3.1          4.7         1.5 versicolor Strong #> 89           5.6         3.0          4.1         1.3 versicolor Strong #> 90           5.5         2.5          4.0         1.3 versicolor  Light #> 99           5.1         2.5          3.0         1.1 versicolor Strong #> 103          7.1         3.0          5.9         2.1  virginica Strong #> 111          6.5         3.2          5.1         2.0  virginica Strong #> 112          6.4         2.7          5.3         1.9  virginica  Light #> 114          5.7         2.5          5.0         2.0  virginica  Light #> 115          5.8         2.8          5.1         2.4  virginica Strong #> 119          7.7         2.6          6.9         2.3  virginica Strong #> 122          5.6         2.8          4.9         2.0  virginica  Light #> 124          6.3         2.7          4.9         1.8  virginica  Light #> 128          6.1         3.0          4.9         1.8  virginica  Light #> 132          7.9         3.8          6.4         2.0  virginica  Light #> 136          7.7         3.0          6.1         2.3  virginica  Light #> 137          6.3         3.4          5.6         2.4  virginica Strong #> 138          6.4         3.1          5.5         1.8  virginica  Light #> 139          6.0         3.0          4.8         1.8  virginica Strong #> 144          6.8         3.2          5.9         2.3  virginica  Light #>  data_partition(df, group = c(\"Species\", \"Smell\")) #> $training #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 34           5.5         4.2          1.4         0.2     setosa  Light #> 42           4.5         2.3          1.3         0.3     setosa  Light #> 26           5.0         3.0          1.6         0.2     setosa  Light #> 48           4.6         3.2          1.4         0.2     setosa  Light #> 16           5.7         4.4          1.5         0.4     setosa  Light #> 14           4.3         3.0          1.1         0.1     setosa  Light #> 36           5.0         3.2          1.2         0.2     setosa  Light #> 50           5.0         3.3          1.4         0.2     setosa  Light #> 2            4.9         3.0          1.4         0.2     setosa  Light #> 4            4.6         3.1          1.5         0.2     setosa  Light #> 38           4.9         3.6          1.4         0.1     setosa  Light #> 32           5.4         3.4          1.5         0.4     setosa  Light #> 46           4.8         3.0          1.4         0.3     setosa  Light #> 44           5.0         3.5          1.6         0.6     setosa  Light #> 30           4.7         3.2          1.6         0.2     setosa  Light #> 6            5.4         3.9          1.7         0.4     setosa  Light #> 28           5.2         3.5          1.5         0.2     setosa  Light #> 54           5.5         2.3          4.0         1.3 versicolor  Light #> 60           5.2         2.7          3.9         1.4 versicolor  Light #> 74           6.1         2.8          4.7         1.2 versicolor  Light #> 76           6.6         3.0          4.4         1.4 versicolor  Light #> 70           5.6         2.5          3.9         1.1 versicolor  Light #> 62           5.9         3.0          4.2         1.5 versicolor  Light #> 92           6.1         3.0          4.6         1.4 versicolor  Light #> 88           6.3         2.3          4.4         1.3 versicolor  Light #> 90           5.5         2.5          4.0         1.3 versicolor  Light #> 96           5.7         3.0          4.2         1.2 versicolor  Light #> 56           5.7         2.8          4.5         1.3 versicolor  Light #> 58           4.9         2.4          3.3         1.0 versicolor  Light #> 72           6.1         2.8          4.0         1.3 versicolor  Light #> 94           5.0         2.3          3.3         1.0 versicolor  Light #> 80           5.7         2.6          3.5         1.0 versicolor  Light #> 64           6.1         2.9          4.7         1.4 versicolor  Light #> 82           5.5         2.4          3.7         1.0 versicolor  Light #> 132          7.9         3.8          6.4         2.0  virginica  Light #> 104          6.3         2.9          5.6         1.8  virginica  Light #> 130          7.2         3.0          5.8         1.6  virginica  Light #> 106          7.6         3.0          6.6         2.1  virginica  Light #> 118          7.7         3.8          6.7         2.2  virginica  Light #> 114          5.7         2.5          5.0         2.0  virginica  Light #> 142          6.9         3.1          5.1         2.3  virginica  Light #> 108          7.3         2.9          6.3         1.8  virginica  Light #> 148          6.5         3.0          5.2         2.0  virginica  Light #> 112          6.4         2.7          5.3         1.9  virginica  Light #> 120          6.0         2.2          5.0         1.5  virginica  Light #> 124          6.3         2.7          4.9         1.8  virginica  Light #> 138          6.4         3.1          5.5         1.8  virginica  Light #> 128          6.1         3.0          4.9         1.8  virginica  Light #> 122          5.6         2.8          4.9         2.0  virginica  Light #> 140          6.9         3.1          5.4         2.1  virginica  Light #> 110          7.2         3.6          6.1         2.5  virginica  Light #> 21           5.4         3.4          1.7         0.2     setosa Strong #> 1            5.1         3.5          1.4         0.2     setosa Strong #> 37           5.5         3.5          1.3         0.2     setosa Strong #> 23           4.6         3.6          1.0         0.2     setosa Strong #> 3            4.7         3.2          1.3         0.2     setosa Strong #> 33           5.2         4.1          1.5         0.1     setosa Strong #> 43           4.4         3.2          1.3         0.2     setosa Strong #> 5            5.0         3.6          1.4         0.2     setosa Strong #> 29           5.2         3.4          1.4         0.2     setosa Strong #> 11           5.4         3.7          1.5         0.2     setosa Strong #> 35           4.9         3.1          1.5         0.2     setosa Strong #> 39           4.4         3.0          1.3         0.2     setosa Strong #> 17           5.4         3.9          1.3         0.4     setosa Strong #> 41           5.0         3.5          1.3         0.3     setosa Strong #> 47           5.1         3.8          1.6         0.2     setosa Strong #> 31           4.8         3.1          1.6         0.2     setosa Strong #> 15           5.8         4.0          1.2         0.2     setosa Strong #> 69           6.2         2.2          4.5         1.5 versicolor Strong #> 75           6.4         2.9          4.3         1.3 versicolor Strong #> 89           5.6         3.0          4.1         1.3 versicolor Strong #> 61           5.0         2.0          3.5         1.0 versicolor Strong #> 95           5.6         2.7          4.2         1.3 versicolor Strong #> 91           5.5         2.6          4.4         1.2 versicolor Strong #> 67           5.6         3.0          4.5         1.5 versicolor Strong #> 81           5.5         2.4          3.8         1.1 versicolor Strong #> 65           5.6         2.9          3.6         1.3 versicolor Strong #> 63           6.0         2.2          4.0         1.0 versicolor Strong #> 99           5.1         2.5          3.0         1.1 versicolor Strong #> 51           7.0         3.2          4.7         1.4 versicolor Strong #> 53           6.9         3.1          4.9         1.5 versicolor Strong #> 83           5.8         2.7          3.9         1.2 versicolor Strong #> 55           6.5         2.8          4.6         1.5 versicolor Strong #> 79           6.0         2.9          4.5         1.5 versicolor Strong #> 85           5.4         3.0          4.5         1.5 versicolor Strong #> 121          6.9         3.2          5.7         2.3  virginica Strong #> 133          6.4         2.8          5.6         2.2  virginica Strong #> 123          7.7         2.8          6.7         2.0  virginica Strong #> 103          7.1         3.0          5.9         2.1  virginica Strong #> 137          6.3         3.4          5.6         2.4  virginica Strong #> 125          6.7         3.3          5.7         2.1  virginica Strong #> 139          6.0         3.0          4.8         1.8  virginica Strong #> 119          7.7         2.6          6.9         2.3  virginica Strong #> 131          7.4         2.8          6.1         1.9  virginica Strong #> 109          6.7         2.5          5.8         1.8  virginica Strong #> 107          4.9         2.5          4.5         1.7  virginica Strong #> 143          5.8         2.7          5.1         1.9  virginica Strong #> 101          6.3         3.3          6.0         2.5  virginica Strong #> 145          6.7         3.3          5.7         2.5  virginica Strong #> 141          6.7         3.1          5.6         2.4  virginica Strong #> 147          6.3         2.5          5.0         1.9  virginica Strong #> 127          6.2         2.8          4.8         1.8  virginica Strong #>  #> $test #>     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species  Smell #> 8            5.0         3.4          1.5         0.2     setosa  Light #> 10           4.9         3.1          1.5         0.1     setosa  Light #> 12           4.8         3.4          1.6         0.2     setosa  Light #> 18           5.1         3.5          1.4         0.3     setosa  Light #> 20           5.1         3.8          1.5         0.3     setosa  Light #> 22           5.1         3.7          1.5         0.4     setosa  Light #> 24           5.1         3.3          1.7         0.5     setosa  Light #> 40           5.1         3.4          1.5         0.2     setosa  Light #> 52           6.4         3.2          4.5         1.5 versicolor  Light #> 66           6.7         3.1          4.4         1.4 versicolor  Light #> 68           5.8         2.7          4.1         1.0 versicolor  Light #> 78           6.7         3.0          5.0         1.7 versicolor  Light #> 84           6.0         2.7          5.1         1.6 versicolor  Light #> 86           6.0         3.4          4.5         1.6 versicolor  Light #> 98           6.2         2.9          4.3         1.3 versicolor  Light #> 100          5.7         2.8          4.1         1.3 versicolor  Light #> 102          5.8         2.7          5.1         1.9  virginica  Light #> 116          6.4         3.2          5.3         2.3  virginica  Light #> 126          7.2         3.2          6.0         1.8  virginica  Light #> 134          6.3         2.8          5.1         1.5  virginica  Light #> 136          7.7         3.0          6.1         2.3  virginica  Light #> 144          6.8         3.2          5.9         2.3  virginica  Light #> 146          6.7         3.0          5.2         2.3  virginica  Light #> 150          5.9         3.0          5.1         1.8  virginica  Light #> 7            4.6         3.4          1.4         0.3     setosa Strong #> 9            4.4         2.9          1.4         0.2     setosa Strong #> 13           4.8         3.0          1.4         0.1     setosa Strong #> 19           5.7         3.8          1.7         0.3     setosa Strong #> 25           4.8         3.4          1.9         0.2     setosa Strong #> 27           5.0         3.4          1.6         0.4     setosa Strong #> 45           5.1         3.8          1.9         0.4     setosa Strong #> 49           5.3         3.7          1.5         0.2     setosa Strong #> 57           6.3         3.3          4.7         1.6 versicolor Strong #> 59           6.6         2.9          4.6         1.3 versicolor Strong #> 71           5.9         3.2          4.8         1.8 versicolor Strong #> 73           6.3         2.5          4.9         1.5 versicolor Strong #> 77           6.8         2.8          4.8         1.4 versicolor Strong #> 87           6.7         3.1          4.7         1.5 versicolor Strong #> 93           5.8         2.6          4.0         1.2 versicolor Strong #> 97           5.7         2.9          4.2         1.3 versicolor Strong #> 105          6.5         3.0          5.8         2.2  virginica Strong #> 111          6.5         3.2          5.1         2.0  virginica Strong #> 113          6.8         3.0          5.5         2.1  virginica Strong #> 115          5.8         2.8          5.1         2.4  virginica Strong #> 117          6.5         3.0          5.5         1.8  virginica Strong #> 129          6.4         2.8          5.6         2.1  virginica Strong #> 135          6.1         2.6          5.6         1.4  virginica Strong #> 149          6.2         3.4          5.4         2.3  virginica Strong #>"},{"path":"/reference/data_relocate.html","id":null,"dir":"Reference","previous_headings":"","what":"Relocate (reorder) columns of a data frame — data_relocate","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"Relocate (reorder) columns data frame","code":""},{"path":"/reference/data_relocate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"","code":"data_relocate(   data,   select,   before = NULL,   after = NULL,   ignore_case = FALSE,   verbose = TRUE,   ... )  data_reorder(data, select, ignore_case = FALSE, verbose = TRUE, ...)  data_remove(data, select, ignore_case = FALSE, verbose = FALSE, ...)"},{"path":"/reference/data_relocate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"data data frame. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. , Destination columns. Supplying neither move columns left-hand side; specifying error. Can character vector, indicating name destination column, numeric value, indicating index number destination column. -1, added last column. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names. verbose Toggle warnings. ... Arguments passed functions. Mostly used yet.","code":""},{"path":"/reference/data_relocate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"data frame reordered columns.","code":""},{"path":"/reference/data_relocate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"data_relocate() reorder columns specific positions, indicated . data_reorder() instead move selected columns beginning data frame .","code":""},{"path":[]},{"path":"/reference/data_relocate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Relocate (reorder) columns of a data frame — data_relocate","text":"","code":"# Reorder columns head(data_relocate(iris, select = \"Species\", before = \"Sepal.Length\")) #>   Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1  setosa          5.1         3.5          1.4         0.2 #> 2  setosa          4.9         3.0          1.4         0.2 #> 3  setosa          4.7         3.2          1.3         0.2 #> 4  setosa          4.6         3.1          1.5         0.2 #> 5  setosa          5.0         3.6          1.4         0.2 #> 6  setosa          5.4         3.9          1.7         0.4 head(data_relocate(iris, select = \"Species\", before = \"Sepal.Width\")) #>   Sepal.Length Species Sepal.Width Petal.Length Petal.Width #> 1          5.1  setosa         3.5          1.4         0.2 #> 2          4.9  setosa         3.0          1.4         0.2 #> 3          4.7  setosa         3.2          1.3         0.2 #> 4          4.6  setosa         3.1          1.5         0.2 #> 5          5.0  setosa         3.6          1.4         0.2 #> 6          5.4  setosa         3.9          1.7         0.4 head(data_relocate(iris, select = \"Sepal.Width\", after = \"Species\")) #>   Sepal.Length Petal.Length Petal.Width Species Sepal.Width #> 1          5.1          1.4         0.2  setosa         3.5 #> 2          4.9          1.4         0.2  setosa         3.0 #> 3          4.7          1.3         0.2  setosa         3.2 #> 4          4.6          1.5         0.2  setosa         3.1 #> 5          5.0          1.4         0.2  setosa         3.6 #> 6          5.4          1.7         0.4  setosa         3.9 # same as head(data_relocate(iris, select = \"Sepal.Width\", after = -1)) #>   Sepal.Length Petal.Length Petal.Width Species Sepal.Width #> 1          5.1          1.4         0.2  setosa         3.5 #> 2          4.9          1.4         0.2  setosa         3.0 #> 3          4.7          1.3         0.2  setosa         3.2 #> 4          4.6          1.5         0.2  setosa         3.1 #> 5          5.0          1.4         0.2  setosa         3.6 #> 6          5.4          1.7         0.4  setosa         3.9  # reorder multiple columns head(data_relocate(iris, select = c(\"Species\", \"Petal.Length\"), after = \"Sepal.Width\")) #>   Sepal.Length Sepal.Width Species Petal.Length Petal.Width #> 1          5.1         3.5  setosa          1.4         0.2 #> 2          4.9         3.0  setosa          1.4         0.2 #> 3          4.7         3.2  setosa          1.3         0.2 #> 4          4.6         3.1  setosa          1.5         0.2 #> 5          5.0         3.6  setosa          1.4         0.2 #> 6          5.4         3.9  setosa          1.7         0.4 # same as head(data_relocate(iris, select = c(\"Species\", \"Petal.Length\"), after = 2)) #>   Sepal.Length Sepal.Width Species Petal.Length Petal.Width #> 1          5.1         3.5  setosa          1.4         0.2 #> 2          4.9         3.0  setosa          1.4         0.2 #> 3          4.7         3.2  setosa          1.3         0.2 #> 4          4.6         3.1  setosa          1.5         0.2 #> 5          5.0         3.6  setosa          1.4         0.2 #> 6          5.4         3.9  setosa          1.7         0.4  # Reorder columns head(data_reorder(iris, c(\"Species\", \"Sepal.Length\"))) #>   Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1  setosa          5.1         3.5          1.4         0.2 #> 2  setosa          4.9         3.0          1.4         0.2 #> 3  setosa          4.7         3.2          1.3         0.2 #> 4  setosa          4.6         3.1          1.5         0.2 #> 5  setosa          5.0         3.6          1.4         0.2 #> 6  setosa          5.4         3.9          1.7         0.4 head(data_reorder(iris, c(\"Species\", \"dupa\"))) # Safe for non-existing cols #> Warning: Following variable(s) were not found: dupa #>   Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1  setosa          5.1         3.5          1.4         0.2 #> 2  setosa          4.9         3.0          1.4         0.2 #> 3  setosa          4.7         3.2          1.3         0.2 #> 4  setosa          4.6         3.1          1.5         0.2 #> 5  setosa          5.0         3.6          1.4         0.2 #> 6  setosa          5.4         3.9          1.7         0.4  # Remove columns head(data_remove(iris, \"Sepal.Length\")) #>   Sepal.Width Petal.Length Petal.Width Species #> 1         3.5          1.4         0.2  setosa #> 2         3.0          1.4         0.2  setosa #> 3         3.2          1.3         0.2  setosa #> 4         3.1          1.5         0.2  setosa #> 5         3.6          1.4         0.2  setosa #> 6         3.9          1.7         0.4  setosa head(data_remove(iris, starts_with(\"Sepal\"))) #>   Petal.Length Petal.Width Species #> 1          1.4         0.2  setosa #> 2          1.4         0.2  setosa #> 3          1.3         0.2  setosa #> 4          1.5         0.2  setosa #> 5          1.4         0.2  setosa #> 6          1.7         0.4  setosa"},{"path":"/reference/data_rename.html","id":null,"dir":"Reference","previous_headings":"","what":"Rename columns and variable names — data_addprefix","title":"Rename columns and variable names — data_addprefix","text":"Safe intuitive functions rename variables rows dataframes.","code":""},{"path":"/reference/data_rename.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rename columns and variable names — data_addprefix","text":"","code":"data_addprefix(   data,   pattern,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   ... )  data_addsuffix(   data,   pattern,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   ... )  data_rename(data, pattern = NULL, replacement = NULL, safe = TRUE, ...)  data_rename_rows(data, rows = NULL)"},{"path":"/reference/data_rename.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rename columns and variable names — data_addprefix","text":"data data frame, object can coerced data frame. pattern Character vector. data_rename(), indicates columns selected renaming. Can NULL (case columns selected). data_addprefix() data_addsuffix(), character string, added prefix suffix column names. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names. ... arguments passed functions. replacement Character vector. Indicates new name columns selected pattern. Can NULL (case column numbered sequential order). NULL, pattern replacement must length. safe throw error instance variable renamed/removed exist. rows Vector row names.","code":""},{"path":"/reference/data_rename.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rename columns and variable names — data_addprefix","text":"modified data frame.","code":""},{"path":[]},{"path":"/reference/data_rename.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rename columns and variable names — data_addprefix","text":"","code":"# Add prefix / suffix to all columns head(data_addprefix(iris, \"NEW_\")) #>   NEW_Sepal.Length NEW_Sepal.Width NEW_Petal.Length NEW_Petal.Width NEW_Species #> 1              5.1             3.5              1.4             0.2      setosa #> 2              4.9             3.0              1.4             0.2      setosa #> 3              4.7             3.2              1.3             0.2      setosa #> 4              4.6             3.1              1.5             0.2      setosa #> 5              5.0             3.6              1.4             0.2      setosa #> 6              5.4             3.9              1.7             0.4      setosa head(data_addsuffix(iris, \"_OLD\")) #>   Sepal.Length_OLD Sepal.Width_OLD Petal.Length_OLD Petal.Width_OLD Species_OLD #> 1              5.1             3.5              1.4             0.2      setosa #> 2              4.9             3.0              1.4             0.2      setosa #> 3              4.7             3.2              1.3             0.2      setosa #> 4              4.6             3.1              1.5             0.2      setosa #> 5              5.0             3.6              1.4             0.2      setosa #> 6              5.4             3.9              1.7             0.4      setosa  # Rename columns head(data_rename(iris, \"Sepal.Length\", \"length\")) #>   length Sepal.Width Petal.Length Petal.Width Species #> 1    5.1         3.5          1.4         0.2  setosa #> 2    4.9         3.0          1.4         0.2  setosa #> 3    4.7         3.2          1.3         0.2  setosa #> 4    4.6         3.1          1.5         0.2  setosa #> 5    5.0         3.6          1.4         0.2  setosa #> 6    5.4         3.9          1.7         0.4  setosa # data_rename(iris, \"FakeCol\", \"length\", safe=FALSE)  # This fails head(data_rename(iris, \"FakeCol\", \"length\")) # This doesn't #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa head(data_rename(iris, c(\"Sepal.Length\", \"Sepal.Width\"), c(\"length\", \"width\"))) #>   length width Petal.Length Petal.Width Species #> 1    5.1   3.5          1.4         0.2  setosa #> 2    4.9   3.0          1.4         0.2  setosa #> 3    4.7   3.2          1.3         0.2  setosa #> 4    4.6   3.1          1.5         0.2  setosa #> 5    5.0   3.6          1.4         0.2  setosa #> 6    5.4   3.9          1.7         0.4  setosa  # Reset names head(data_rename(iris, NULL)) #>     1   2   3   4      5 #> 1 5.1 3.5 1.4 0.2 setosa #> 2 4.9 3.0 1.4 0.2 setosa #> 3 4.7 3.2 1.3 0.2 setosa #> 4 4.6 3.1 1.5 0.2 setosa #> 5 5.0 3.6 1.4 0.2 setosa #> 6 5.4 3.9 1.7 0.4 setosa  # Change all head(data_rename(iris, paste0(\"Var\", 1:5))) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa"},{"path":"/reference/data_rescale.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescale Variables to a New Range — data_rescale","title":"Rescale Variables to a New Range — data_rescale","text":"Rescale variables new range. Can also used reverse-score variables (change keying/scoring direction).","code":""},{"path":"/reference/data_rescale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescale Variables to a New Range — data_rescale","text":"","code":"data_rescale(x, ...)  change_scale(x, ...)  # S3 method for numeric data_rescale(x, to = c(0, 100), range = NULL, verbose = TRUE, ...)  # S3 method for data.frame data_rescale(   x,   to = c(0, 100),   range = NULL,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   ... )"},{"path":"/reference/data_rescale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescale Variables to a New Range — data_rescale","text":"x (grouped) data frame, numeric vector factor. ... Arguments passed methods. Numeric vector length 2 giving new range variable rescaling. reverse-score variable, range given maximum value first. See examples. range Initial (old) range values. NULL, take range input vector (range(x)). verbose Toggle warnings. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/data_rescale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescale Variables to a New Range — data_rescale","text":"rescaled object.","code":""},{"path":[]},{"path":"/reference/data_rescale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rescale Variables to a New Range — data_rescale","text":"","code":"data_rescale(c(0, 1, 5, -5, -2)) #> [1]  50  60 100   0  30 data_rescale(c(0, 1, 5, -5, -2), to = c(-5, 5)) #> [1]  0  1  5 -5 -2 data_rescale(c(1, 2, 3, 4, 5), to = c(-2, 2)) #> [1] -2 -1  0  1  2  # Specify the \"theoretical\" range of the input vector data_rescale(c(1, 3, 4), to = c(0, 40), range = c(0, 4)) #> [1] 10 30 40  # Reverse-score a variable data_rescale(c(1, 2, 3, 4, 5), to = c(5, 1)) #> [1] 5 4 3 2 1 data_rescale(c(1, 2, 3, 4, 5), to = c(2, -2)) #> [1]  2  1  0 -1 -2  # Data frames head(data_rescale(iris, to = c(0, 1))) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   0.22222222   0.6250000   0.06779661  0.04166667  setosa #> 2   0.16666667   0.4166667   0.06779661  0.04166667  setosa #> 3   0.11111111   0.5000000   0.05084746  0.04166667  setosa #> 4   0.08333333   0.4583333   0.08474576  0.04166667  setosa #> 5   0.19444444   0.6666667   0.06779661  0.04166667  setosa #> 6   0.30555556   0.7916667   0.11864407  0.12500000  setosa head(data_rescale(iris, to = c(0, 1), select = \"Sepal.Length\")) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   0.22222222         3.5          1.4         0.2  setosa #> 2   0.16666667         3.0          1.4         0.2  setosa #> 3   0.11111111         3.2          1.3         0.2  setosa #> 4   0.08333333         3.1          1.5         0.2  setosa #> 5   0.19444444         3.6          1.4         0.2  setosa #> 6   0.30555556         3.9          1.7         0.4  setosa  # One can specify a list of ranges head(data_rescale(iris, to = list(   \"Sepal.Length\" = c(0, 1),   \"Petal.Length\" = c(-1, 0) ))) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1   0.22222222         3.5   -0.9322034         0.2  setosa #> 2   0.16666667         3.0   -0.9322034         0.2  setosa #> 3   0.11111111         3.2   -0.9491525         0.2  setosa #> 4   0.08333333         3.1   -0.9152542         0.2  setosa #> 5   0.19444444         3.6   -0.9322034         0.2  setosa #> 6   0.30555556         3.9   -0.8813559         0.4  setosa"},{"path":"/reference/data_restoretype.html","id":null,"dir":"Reference","previous_headings":"","what":"Restore the type of columns according to a reference data frame — data_restoretype","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"Restore type columns according reference data frame","code":""},{"path":"/reference/data_restoretype.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"","code":"data_restoretype(data, reference = NULL, ...)"},{"path":"/reference/data_restoretype.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"data data frame pivot. reference reference data frame find correct column types. ... Additional arguments passed methods.","code":""},{"path":"/reference/data_restoretype.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"dataframe columns whose types restored based reference dataframe.","code":""},{"path":"/reference/data_restoretype.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Restore the type of columns according to a reference data frame — data_restoretype","text":"","code":"data <- data.frame(   Sepal.Length = c(\"1\", \"3\", \"2\"),   Species = c(\"setosa\", \"versicolor\", \"setosa\"),   New = c(\"1\", \"3\", \"4\") )  fixed <- data_restoretype(data, reference = iris) summary(fixed) #>   Sepal.Length       Species      New            #>  Min.   :1.0   setosa    :2   Length:3           #>  1st Qu.:1.5   versicolor:1   Class :character   #>  Median :2.0   virginica :0   Mode  :character   #>  Mean   :2.0                                     #>  3rd Qu.:2.5                                     #>  Max.   :3.0"},{"path":"/reference/data_reverse.html","id":null,"dir":"Reference","previous_headings":"","what":"Reverse-Score Variables — data_reverse","title":"Reverse-Score Variables — data_reverse","text":"Reverse-score variables (change keying/scoring direction).","code":""},{"path":"/reference/data_reverse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reverse-Score Variables — data_reverse","text":"","code":"data_reverse(x, ...)  reverse_scale(x, ...)  # S3 method for numeric data_reverse(x, range = NULL, verbose = TRUE, ...)  # S3 method for data.frame data_reverse(   x,   range = NULL,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   ... )"},{"path":"/reference/data_reverse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reverse-Score Variables — data_reverse","text":"x (grouped) data frame, numeric vector factor. ... Arguments passed methods. range Initial (old) range values. NULL, take range input vector (range(x)). verbose Toggle warnings. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/data_reverse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reverse-Score Variables — data_reverse","text":"reverse-scored object.","code":""},{"path":[]},{"path":"/reference/data_reverse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reverse-Score Variables — data_reverse","text":"","code":"data_reverse(c(1, 2, 3, 4, 5)) #> [1] 5 4 3 2 1 data_reverse(c(-2, -1, 0, 2, 1)) #> [1]  2  1  0 -2 -1  # Specify the \"theoretical\" range of the input vector data_reverse(c(1, 3, 4), range = c(0, 4)) #> [1] 3 1 0  # Factor variables data_reverse(factor(c(1, 2, 3, 4, 5))) #> [1] 5 4 3 2 1 #> Levels: 1 2 3 4 5 data_reverse(factor(c(1, 2, 3, 4, 5)), range = 0:10) #> [1] 9 8 7 6 5 #> Levels: 0 1 2 3 4 5 6 7 8 9 10  # Data frames head(data_reverse(iris)) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species #> 1          7.1         2.9          6.5         2.4 virginica #> 2          7.3         3.4          6.5         2.4 virginica #> 3          7.5         3.2          6.6         2.4 virginica #> 4          7.6         3.3          6.4         2.4 virginica #> 5          7.2         2.8          6.5         2.4 virginica #> 6          6.8         2.5          6.2         2.2 virginica head(data_reverse(iris, select = \"Sepal.Length\")) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1          7.1         3.5          1.4         0.2  setosa #> 2          7.3         3.0          1.4         0.2  setosa #> 3          7.5         3.2          1.3         0.2  setosa #> 4          7.6         3.1          1.5         0.2  setosa #> 5          7.2         3.6          1.4         0.2  setosa #> 6          6.8         3.9          1.7         0.4  setosa"},{"path":"/reference/data_rotate.html","id":null,"dir":"Reference","previous_headings":"","what":"Rotate a data frame — data_rotate","title":"Rotate a data frame — data_rotate","text":"function rotates data frame, .e. columns become rows vice versa. equivalent using t() restores data.frame class, preserves attributes prints warning data type modified (see example).","code":""},{"path":"/reference/data_rotate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rotate a data frame — data_rotate","text":"","code":"data_rotate(data, rownames = NULL, colnames = FALSE, verbose = TRUE)  data_transpose(data, rownames = NULL, colnames = FALSE, verbose = TRUE)"},{"path":"/reference/data_rotate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rotate a data frame — data_rotate","text":"data data frame. rownames Character vector (optional). NULL, data frame's rownames added (first) column output, rownames name column. colnames Logical character vector (optional). TRUE, values first column x used column names rotated data frame. character vector, values column used column names. verbose Toggle warnings.","code":""},{"path":"/reference/data_rotate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rotate a data frame — data_rotate","text":"(rotated) data frame.","code":""},{"path":[]},{"path":"/reference/data_rotate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rotate a data frame — data_rotate","text":"","code":"x <- mtcars[1:3, 1:4]  x #>                mpg cyl disp  hp #> Mazda RX4     21.0   6  160 110 #> Mazda RX4 Wag 21.0   6  160 110 #> Datsun 710    22.8   4  108  93  data_rotate(x) #>      Mazda RX4 Mazda RX4 Wag Datsun 710 #> mpg         21            21       22.8 #> cyl          6             6        4.0 #> disp       160           160      108.0 #> hp         110           110       93.0 data_rotate(x, rownames = \"property\") #>   property Mazda RX4 Mazda RX4 Wag Datsun 710 #> 1      mpg        21            21       22.8 #> 2      cyl         6             6        4.0 #> 3     disp       160           160      108.0 #> 4       hp       110           110       93.0  # use values in 1. column as column name data_rotate(x, colnames = TRUE) #>       21  21 22.8 #> cyl    6   6    4 #> disp 160 160  108 #> hp   110 110   93 data_rotate(x, rownames = \"property\", colnames = TRUE) #>   property  21  21 22.8 #> 1      cyl   6   6    4 #> 2     disp 160 160  108 #> 3       hp 110 110   93  # warn that data types are changed str(data_rotate(iris[1:4, ])) #> Warning: Your data frame contains mixed types of data. After transposition, all variables #>   will be transformed into characters. #> 'data.frame':\t5 obs. of  4 variables: #>  $ 1: chr  \"5.1\" \"3.5\" \"1.4\" \"0.2\" ... #>  $ 2: chr  \"4.9\" \"3.0\" \"1.4\" \"0.2\" ... #>  $ 3: chr  \"4.7\" \"3.2\" \"1.3\" \"0.2\" ... #>  $ 4: chr  \"4.6\" \"3.1\" \"1.5\" \"0.2\" ...  # use either first column or specific column for column names x <- data.frame(a = 1:5, b = 11:15, c = letters[1:5], d = rnorm(5)) data_rotate(x, colnames = TRUE) #> Warning: Your data frame contains mixed types of data. After transposition, all variables #>   will be transformed into characters. #>            1          2          3          4          5 #> b         11         12         13         14         15 #> c          a          b          c          d          e #> d  2.4544004 -1.6493455  0.7958052 -0.3412932 -1.4052538 data_rotate(x, colnames = \"c\") #> Warning: Your data frame contains mixed types of data. After transposition, all variables #>   will be transformed into characters. #>         a         b          c          d         e #> a  1.0000  2.000000  3.0000000  4.0000000  5.000000 #> b 11.0000 12.000000 13.0000000 14.0000000 15.000000 #> d  2.4544 -1.649345  0.7958052 -0.3412932 -1.405254"},{"path":"/reference/data_to_long.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape (pivot) data from wide to long — data_to_long","title":"Reshape (pivot) data from wide to long — data_to_long","text":"function \"lengthens\" data, increasing number rows decreasing number columns. dependency-free base-R equivalent tidyr::pivot_longer().","code":""},{"path":"/reference/data_to_long.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape (pivot) data from wide to long — data_to_long","text":"","code":"data_to_long(   data,   select = \"all\",   colnames_to = \"Name\",   values_to = \"Value\",   rows_to = NULL,   ignore_case = FALSE,   cols = select,   ...,   names_to = colnames_to )  data_to_wide(   data,   values_from = \"Value\",   colnames_from = \"Name\",   rows_from = NULL,   sep = \"_\",   ...,   names_from = colnames_from )  reshape_longer(   data,   select = \"all\",   colnames_to = \"Name\",   values_to = \"Value\",   rows_to = NULL,   ignore_case = FALSE,   cols = select,   ...,   names_to = colnames_to )  reshape_wider(   data,   values_from = \"Value\",   colnames_from = \"Name\",   rows_from = NULL,   sep = \"_\",   ...,   names_from = colnames_from )"},{"path":"/reference/data_to_long.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape (pivot) data from wide to long — data_to_long","text":"data data frame pivot. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. colnames_to name new column contain column names. values_to name new column contain values pivoted variables. rows_to name column contain row-number original data. NULL, removed. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names. cols Deprecated. Please use select. ... Additional arguments passed methods. names_to, names_from colnames_to, compatibility tidyr::pivot_longer(). values_from name column contains values put columns. colnames_from name column contains levels used future columns. rows_from name column identifies rows. NULL, use unique rows. sep indicating separating character variable names wide format.","code":""},{"path":"/reference/data_to_long.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape (pivot) data from wide to long — data_to_long","text":"data.frame","code":""},{"path":[]},{"path":"/reference/data_to_long.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reshape (pivot) data from wide to long — data_to_long","text":"","code":"wide_data <- data.frame(replicate(5, rnorm(10)))  # From wide to long # ------------------ # Default behaviour (equivalent to tidyr::pivot_longer(wide_data, cols = 1:5)) data_to_long(wide_data) #>    Name       Value #> 1    X1 -0.92296082 #> 2    X2  0.46494175 #> 3    X3  1.80105086 #> 4    X4  0.39550987 #> 5    X5 -1.03071105 #> 6    X1  0.00989813 #> 7    X2 -1.95103061 #> 8    X3  1.05533859 #> 9    X4  0.88642168 #> 10   X5  0.43083317 #> 11   X1 -0.40885977 #> 12   X2 -0.51611334 #> 13   X3 -0.29192076 #> 14   X4  1.20104255 #> 15   X5  0.20890740 #> 16   X1 -1.38824529 #> 17   X2  0.48908032 #> 18   X3 -0.82410938 #> 19   X4  1.31680568 #> 20   X5 -1.54824026 #> 21   X1 -0.26459524 #> 22   X2  0.90202473 #> 23   X3  1.21625024 #> 24   X4 -0.19304042 #> 25   X5 -1.17380222 #> 26   X1 -0.94729828 #> 27   X2  0.64036299 #> 28   X3  0.21972840 #> 29   X4  0.55057625 #> 30   X5  1.17881520 #> 31   X1  0.73952133 #> 32   X2  0.95120888 #> 33   X3  0.07499953 #> 34   X4 -2.08877294 #> 35   X5 -0.41930944 #> 36   X1  0.89677871 #> 37   X2 -0.59912321 #> 38   X3 -2.14041848 #> 39   X4 -0.02490979 #> 40   X5 -1.85257530 #> 41   X1 -0.34600089 #> 42   X2 -1.33069499 #> 43   X3  1.47097224 #> 44   X4  0.88845765 #> 45   X5  0.48592647 #> 46   X1 -1.78205707 #> 47   X2 -0.59220975 #> 48   X3 -0.83776761 #> 49   X4 -0.33743115 #> 50   X5 -0.47191259  # Customizing the names data_to_long(wide_data,   select = c(1, 2),   colnames_to = \"Column\",   values_to = \"Numbers\",   rows_to = \"Row\" ) #>             X3          X4         X5 Row Column     Numbers #> 1   1.80105086  0.39550987 -1.0307111   1     X1 -0.92296082 #> 2   1.80105086  0.39550987 -1.0307111   1     X2  0.46494175 #> 3   1.05533859  0.88642168  0.4308332   2     X1  0.00989813 #> 4   1.05533859  0.88642168  0.4308332   2     X2 -1.95103061 #> 5  -0.29192076  1.20104255  0.2089074   3     X1 -0.40885977 #> 6  -0.29192076  1.20104255  0.2089074   3     X2 -0.51611334 #> 7  -0.82410938  1.31680568 -1.5482403   4     X1 -1.38824529 #> 8  -0.82410938  1.31680568 -1.5482403   4     X2  0.48908032 #> 9   1.21625024 -0.19304042 -1.1738022   5     X1 -0.26459524 #> 10  1.21625024 -0.19304042 -1.1738022   5     X2  0.90202473 #> 11  0.21972840  0.55057625  1.1788152   6     X1 -0.94729828 #> 12  0.21972840  0.55057625  1.1788152   6     X2  0.64036299 #> 13  0.07499953 -2.08877294 -0.4193094   7     X1  0.73952133 #> 14  0.07499953 -2.08877294 -0.4193094   7     X2  0.95120888 #> 15 -2.14041848 -0.02490979 -1.8525753   8     X1  0.89677871 #> 16 -2.14041848 -0.02490979 -1.8525753   8     X2 -0.59912321 #> 17  1.47097224  0.88845765  0.4859265   9     X1 -0.34600089 #> 18  1.47097224  0.88845765  0.4859265   9     X2 -1.33069499 #> 19 -0.83776761 -0.33743115 -0.4719126  10     X1 -1.78205707 #> 20 -0.83776761 -0.33743115 -0.4719126  10     X2 -0.59220975  # From long to wide # ----------------- long_data <- data_to_long(wide_data, rows_to = \"Row_ID\") # Save row number data_to_wide(long_data,   colnames_from = \"Name\",   values_from = \"Value\",   rows_from = \"Row_ID\" ) #>    Row_ID    Value_X1   Value_X2    Value_X3    Value_X4   Value_X5 #> 1       1 -0.92296082  0.4649418  1.80105086  0.39550987 -1.0307111 #> 2       2  0.00989813 -1.9510306  1.05533859  0.88642168  0.4308332 #> 3       3 -0.40885977 -0.5161133 -0.29192076  1.20104255  0.2089074 #> 4       4 -1.38824529  0.4890803 -0.82410938  1.31680568 -1.5482403 #> 5       5 -0.26459524  0.9020247  1.21625024 -0.19304042 -1.1738022 #> 6       6 -0.94729828  0.6403630  0.21972840  0.55057625  1.1788152 #> 7       7  0.73952133  0.9512089  0.07499953 -2.08877294 -0.4193094 #> 8       8  0.89677871 -0.5991232 -2.14041848 -0.02490979 -1.8525753 #> 9       9 -0.34600089 -1.3306950  1.47097224  0.88845765  0.4859265 #> 10     10 -1.78205707 -0.5922097 -0.83776761 -0.33743115 -0.4719126  # Full example # ------------------ if (require(\"psych\")) {   data <- psych::bfi # Wide format with one row per participant's personality test    # Pivot long format   long <- data_to_long(data,     select = regex(\"\\\\d\"), # Select all columns that contain a digit     colnames_to = \"Item\",     values_to = \"Score\",     rows_to = \"Participant\"   )    # Separate facet and question number   long$Facet <- gsub(\"\\\\d\", \"\", long$Item)   long$Item <- gsub(\"[A-Z]\", \"\", long$Item)   long$Item <- paste0(\"I\", long$Item)    wide <- data_to_wide(long,     colnames_from = \"Item\",     values_from = \"Score\"   )   head(wide) } #> Loading required package: psych #>   gender education age Participant Facet Score_I1 Score_I2 Score_I3 Score_I4 #> 1      1        NA  16       61617     A        2        4        3        4 #> 2      1        NA  16       61617     C        2        3        3        4 #> 3      1        NA  16       61617     E        3        3        3        4 #> 4      1        NA  16       61617     N        3        4        2        2 #> 5      1        NA  16       61617     O        3        6        3        4 #> 6      2        NA  18       61618     A        2        4        5        2 #>   Score_I5 #> 1        4 #> 2        4 #> 3        4 #> 4        3 #> 5        3 #> 6        5"},{"path":"/reference/demean.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute group-meaned and de-meaned variables — demean","title":"Compute group-meaned and de-meaned variables — demean","text":"demean() computes group- de-meaned versions variable can used regression analysis model - within-subject effect. degroup() generic terms centering-operation. demean() always uses mean-centering, degroup() can also use mode median centering.","code":""},{"path":"/reference/demean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute group-meaned and de-meaned variables — demean","text":"","code":"demean(   x,   select,   group,   suffix_demean = \"_within\",   suffix_groupmean = \"_between\",   add_attributes = TRUE,   verbose = TRUE )  degroup(   x,   select,   group,   center = \"mean\",   suffix_demean = \"_within\",   suffix_groupmean = \"_between\",   add_attributes = TRUE,   verbose = TRUE )  detrend(   x,   select,   group,   center = \"mean\",   suffix_demean = \"_within\",   suffix_groupmean = \"_between\",   add_attributes = TRUE,   verbose = TRUE )"},{"path":"/reference/demean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute group-meaned and de-meaned variables — demean","text":"x data frame. select Character vector (formula) names variables select group- de-meaned. group Character vector (formula) name variable indicates group- cluster-ID. suffix_demean, suffix_groupmean String value, appended names group-meaned de-meaned variables x. default, de-meaned variables suffixed \"_within\" grouped-meaned variables \"_between\". add_attributes Logical, TRUE, returned variables gain attributes indicate within- -effects. relevant printing model_parameters() - cases, within- -effects printed separated blocks. verbose Toggle warnings messages. center Method centering. demean() always performs mean-centering, degroup() can use center = \"median\" center = \"mode\" median- mode-centering, also \"min\" \"max\".","code":""},{"path":"/reference/demean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute group-meaned and de-meaned variables — demean","text":"data frame group-/de-meaned variables, get suffix \"_between\" (group-meaned variable) \"_within\" (de-meaned variable) default.","code":""},{"path":[]},{"path":"/reference/demean.html","id":"heterogeneity-bias","dir":"Reference","previous_headings":"","what":"Heterogeneity Bias","title":"Compute group-meaned and de-meaned variables — demean","text":"Mixed models include different levels sources variability, .e. error terms level. macro-indicators (level-2 predictors, higher-level units, general: group-level predictors vary within across groups) included fixed effects (.e. treated covariate level-1), variance left unaccounted covariate absorbed error terms level-1 level-2 (Bafumi Gelman 2006; Gelman Hill 2007, Chapter 12.6.): “covariates contain two parts: one specific higher-level entity vary occasions, one represents difference occasions, within higher-level entities” (Bell et al. 2015). Hence, error terms correlated covariate, violates one assumptions mixed models (iid, independent identically distributed error terms). bias also called heterogeneity bias (Bell et al. 2015). resolve problem, level-2 predictors used (level-1) covariates separated \"within\" \"\" effects \"de-meaning\" \"group-meaning\": demeaning time-varying predictors, “higher level, mean term longer constrained Level 1 effects, free account higher-level variance associated variable” (Bell et al. 2015).","code":""},{"path":"/reference/demean.html","id":"panel-data-and-correlating-fixed-and-group-effects","dir":"Reference","previous_headings":"","what":"Panel data and correlating fixed and group effects","title":"Compute group-meaned and de-meaned variables — demean","text":"demean() intended create group- de-meaned variables panel regression models (fixed effects models), complex random-effect-within-models (see Bell et al. 2015, 2018), group-effects (random effects) fixed effects correlate (see Bafumi Gelman 2006). can happen, instance, analyzing panel data, can lead Heterogeneity Bias. control correlating predictors group effects, recommended include group-meaned de-meaned version time-varying covariates (group-meaned version time-invariant covariates higher level, e.g. level-2 predictors) model. , one can fit complex multilevel models panel data, including time-varying predictors, time-invariant predictors random effects.","code":""},{"path":"/reference/demean.html","id":"why-mixed-models-are-preferred-over-fixed-effects-models","dir":"Reference","previous_headings":"","what":"Why mixed models are preferred over fixed effects models","title":"Compute group-meaned and de-meaned variables — demean","text":"mixed models approach can model causes endogeneity explicitly including (separated) within- -effects time-varying fixed effects including time-constant fixed effects. Furthermore, mixed models also include random effects, thus mixed models approach superior classic fixed-effects models, lack information variation group-effects -subject effects. Furthermore, fixed effects regression include random slopes, means fixed effects regressions neglecting “cross-cluster differences effects lower-level controls () reduces precision estimated context effects, resulting unnecessarily wide confidence intervals low statistical power” (Heisig et al. 2017).","code":""},{"path":"/reference/demean.html","id":"terminology","dir":"Reference","previous_headings":"","what":"Terminology","title":"Compute group-meaned and de-meaned variables — demean","text":"group-meaned variable simply mean independent variable within group (id-level cluster) represented group. represents cluster-mean independent variable. regression coefficient group-meaned variable -subject-effect. de-meaned variable centered version group-meaned variable. De-meaning sometimes also called person-mean centering centering within clusters. regression coefficient de-meaned variable represents within-subject-effect.","code":""},{"path":"/reference/demean.html","id":"de-meaning-with-continuous-predictors","dir":"Reference","previous_headings":"","what":"De-meaning with continuous predictors","title":"Compute group-meaned and de-meaned variables — demean","text":"continuous time-varying predictors, recommendation include de-meaned group-meaned versions fixed effects, raw (untransformed) time-varying predictors . de-meaned predictor also included random effect (random slope). regression models, coefficient de-meaned predictors indicates within-subject effect, coefficient group-meaned predictor indicates -subject effect.","code":""},{"path":"/reference/demean.html","id":"de-meaning-with-binary-predictors","dir":"Reference","previous_headings":"","what":"De-meaning with binary predictors","title":"Compute group-meaned and de-meaned variables — demean","text":"binary time-varying predictors, two recommendations. First include raw (untransformed) binary predictor fixed effect de-meaned variable random effect (random slope). alternative add de-meaned version(s) binary time-varying covariates additional fixed effect well (instead adding random slope). Centering time-varying binary variables obtain within-effects (level 1) necessary. sensible interpretation left typical 0/1 format (Hoffmann 2015, chapter 8-2.). demean() thus coerce categorical time-varying predictors numeric compute de- group-meaned versions variables, raw (untransformed) binary predictor de-meaned version added model.","code":""},{"path":"/reference/demean.html","id":"de-meaning-of-factors-with-more-than-levels","dir":"Reference","previous_headings":"","what":"De-meaning of factors with more than 2 levels","title":"Compute group-meaned and de-meaned variables — demean","text":"Factors two levels demeaned two ways: first, also converted numeric de-meaned; second, dummy variables created (binary, 0/1 coding level) binary dummy-variables de-meaned way (described ). Packages like panelr internally convert factors dummies demeaning, behaviour can mimicked .","code":""},{"path":"/reference/demean.html","id":"de-meaning-interaction-terms","dir":"Reference","previous_headings":"","what":"De-meaning interaction terms","title":"Compute group-meaned and de-meaned variables — demean","text":"multiple ways deal interaction terms within- -effects. classical approach simply use product term de-meaned variables (.e. introducing de-meaned variables interaction term model formula, e.g. y ~ x_within * time_within). approach, however, might subject bias (see Giesselmann & Schmidt-Catran 2020).  Another option first calculate product term apply de-meaning . approach produces estimator “reflects unit-level differences interacted variables whose moderators vary within units”, desirable within interaction two time-dependent variables required.  third option, interaction result genuine within estimator, \"double de-mean\" interaction terms (Giesselmann & Schmidt-Catran 2018), however, currently supported demean(). required, wmb() function panelr package used.  de-mean interaction terms within-models, simply specify term interaction select-argument, e.g. select = \"*b\" (see 'Examples').","code":""},{"path":"/reference/demean.html","id":"analysing-panel-data-with-mixed-models-using-lme-","dir":"Reference","previous_headings":"","what":"Analysing panel data with mixed models using lme4","title":"Compute group-meaned and de-meaned variables — demean","text":"description translate formulas described Bell et al. 2018 R using lmer() lme4 can found vignette.","code":""},{"path":"/reference/demean.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute group-meaned and de-meaned variables — demean","text":"Bafumi J, Gelman . 2006. Fitting Multilevel Models Predictors Group Effects Correlate. . Philadelphia, PA: Annual meeting American Political Science Association. Bell , Fairbrother M, Jones K. 2019. Fixed Random Effects Models: Making Informed Choice. Quality & Quantity (53); 1051-1074 Bell , Jones K. 2015. Explaining Fixed Effects: Random Effects Modeling Time-Series Cross-Sectional Panel Data. Political Science Research Methods, 3(1), 133–153. Gelman , Hill J. 2007. Data Analysis Using Regression Multilevel/Hierarchical Models. Analytical Methods Social Research. Cambridge, New York: Cambridge University Press Giesselmann M, Schmidt-Catran, AW. 2020. Interactions fixed effects regression models. Sociological Methods & Research, 1–28. https://doi.org/10.1177/0049124120914934 Heisig JP, Schaeffer M, Giesecke J. 2017. Costs Simplicity: Multilevel Models May Benefit Accounting Cross-Cluster Differences Effects Controls. American Sociological Review 82 (4): 796–827. Hoffman L. 2015. Longitudinal analysis: modeling within-person fluctuation change. New York: Routledge","code":""},{"path":[]},{"path":"/reference/demean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute group-meaned and de-meaned variables — demean","text":"","code":"data(iris) iris$ID <- sample(1:4, nrow(iris), replace = TRUE) # fake-ID iris$binary <- as.factor(rbinom(150, 1, .35)) # binary variable  x <- demean(iris, select = c(\"Sepal.Length\", \"Petal.Length\"), group = \"ID\") head(x) #>   Sepal.Length_between Petal.Length_between Sepal.Length_within #> 1             5.900000             3.802273          -0.8000000 #> 2             5.900000             3.802273          -1.0000000 #> 3             5.743590             3.579487          -1.0435897 #> 4             5.734375             3.812500          -1.1343750 #> 5             5.743590             3.579487          -0.7435897 #> 6             5.900000             3.802273          -0.5000000 #>   Petal.Length_within #> 1           -2.402273 #> 2           -2.402273 #> 3           -2.279487 #> 4           -2.312500 #> 5           -2.179487 #> 6           -2.102273  x <- demean(iris, select = c(\"Sepal.Length\", \"binary\", \"Species\"), group = \"ID\") #> Categorical predictors (Species, binary) have been coerced to numeric values to compute de- and group-meaned variables. #>  head(x) #>   Sepal.Length_between Species_between binary_between Species_setosa_between #> 1             5.900000       0.9772727      0.3636364              0.3409091 #> 2             5.900000       0.9772727      0.3636364              0.3409091 #> 3             5.743590       0.9230769      0.4102564              0.3333333 #> 4             5.734375       1.1250000      0.2812500              0.3125000 #> 5             5.743590       0.9230769      0.4102564              0.3333333 #> 6             5.900000       0.9772727      0.3636364              0.3409091 #>   Species_versicolor_between Species_virginica_between Sepal.Length_within #> 1                  0.3409091                 0.3181818          -0.8000000 #> 2                  0.3409091                 0.3181818          -1.0000000 #> 3                  0.4102564                 0.2564103          -1.0435897 #> 4                  0.2500000                 0.4375000          -1.1343750 #> 5                  0.4102564                 0.2564103          -0.7435897 #> 6                  0.3409091                 0.3181818          -0.5000000 #>   Species_within binary_within Species_setosa_within Species_versicolor_within #> 1     -0.9772727    -0.3636364             0.6590909                -0.3409091 #> 2     -0.9772727    -0.3636364             0.6590909                -0.3409091 #> 3     -0.9230769     0.5897436             0.6666667                -0.4102564 #> 4     -1.1250000    -0.2812500             0.6875000                -0.2500000 #> 5     -0.9230769    -0.4102564             0.6666667                -0.4102564 #> 6     -0.9772727    -0.3636364             0.6590909                -0.3409091 #>   Species_virginica_within #> 1               -0.3181818 #> 2               -0.3181818 #> 3               -0.2564103 #> 4               -0.4375000 #> 5               -0.2564103 #> 6               -0.3181818   # demean interaction term x*y dat <- data.frame(   a = c(1, 2, 3, 4, 1, 2, 3, 4),   x = c(4, 3, 3, 4, 1, 2, 1, 2),   y = c(1, 2, 1, 2, 4, 3, 2, 1),   ID = c(1, 2, 3, 1, 2, 3, 1, 2) ) demean(dat, select = c(\"a\", \"x*y\"), group = \"ID\") #>   a_between x_y_between   a_within x_y_within #> 1  2.666667    4.666667 -1.6666667 -0.6666667 #> 2  2.333333    4.000000 -0.3333333  2.0000000 #> 3  2.500000    4.500000  0.5000000 -1.5000000 #> 4  2.666667    4.666667  1.3333333  3.3333333 #> 5  2.333333    4.000000 -1.3333333  0.0000000 #> 6  2.500000    4.500000 -0.5000000  1.5000000 #> 7  2.666667    4.666667  0.3333333 -2.6666667 #> 8  2.333333    4.000000  1.6666667 -2.0000000  # or in formula-notation demean(dat, select = ~ a + x * y, group = ~ID) #>   a_between x_y_between   a_within x_y_within #> 1  2.666667    4.666667 -1.6666667 -0.6666667 #> 2  2.333333    4.000000 -0.3333333  2.0000000 #> 3  2.500000    4.500000  0.5000000 -1.5000000 #> 4  2.666667    4.666667  1.3333333  3.3333333 #> 5  2.333333    4.000000 -1.3333333  0.0000000 #> 6  2.500000    4.500000 -0.5000000  1.5000000 #> 7  2.666667    4.666667  0.3333333 -2.6666667 #> 8  2.333333    4.000000  1.6666667 -2.0000000"},{"path":"/reference/describe_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Describe a distribution — describe_distribution","title":"Describe a distribution — describe_distribution","text":"function describes distribution set indices (e.g., measures centrality, dispersion, range, skewness, kurtosis).","code":""},{"path":"/reference/describe_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Describe a distribution — describe_distribution","text":"","code":"describe_distribution(x, ...)  # S3 method for list describe_distribution(   x,   centrality = \"mean\",   dispersion = TRUE,   iqr = TRUE,   range = TRUE,   quartiles = FALSE,   ci = NULL,   include_factors = FALSE,   iterations = 100,   threshold = 0.1,   verbose = TRUE,   ... )  # S3 method for numeric describe_distribution(   x,   centrality = \"mean\",   dispersion = TRUE,   iqr = TRUE,   range = TRUE,   quartiles = FALSE,   ci = NULL,   iterations = 100,   threshold = 0.1,   verbose = TRUE,   ... )  # S3 method for factor describe_distribution(x, dispersion = TRUE, range = TRUE, verbose = TRUE, ...)  # S3 method for character describe_distribution(x, dispersion = TRUE, range = TRUE, verbose = TRUE, ...)  # S3 method for data.frame describe_distribution(   x,   centrality = \"mean\",   dispersion = TRUE,   iqr = TRUE,   range = TRUE,   quartiles = FALSE,   include_factors = FALSE,   ci = NULL,   iterations = 100,   threshold = 0.1,   verbose = TRUE,   ... )"},{"path":"/reference/describe_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Describe a distribution — describe_distribution","text":"x numeric vector, character vector, dataframe, list. See Details. ... Additional arguments passed methods. centrality point-estimates (centrality indices) compute.  Character (vector) list one options: \"median\", \"mean\", \"MAP\" \"\". dispersion Logical, TRUE, computes indices dispersion related estimate(s) (SD MAD mean median, respectively). iqr Logical, TRUE, interquartile range calculated (based stats::IQR(), using type = 6). range Return range (min max). quartiles Return first third quartiles (25th 75pth percentiles). ci Confidence Interval (CI) level. Default NULL, .e. confidence intervals computed. NULL, confidence intervals based bootstrap replicates (see iterations). centrality = \"\", bootstrapped confidence interval refers first centrality index (typically median). include_factors Logical, TRUE, factors included output, however, columns range (first last factor levels) well n missing contain information. iterations number bootstrap replicates computing confidence intervals. applies ci NULL. threshold centrality = \"trimmed\" (.e. trimmed mean), indicates fraction (0 0.5) observations trimmed end vector mean computed. verbose Toggle warnings messages.","code":""},{"path":"/reference/describe_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Describe a distribution — describe_distribution","text":"data frame columns describe properties variables.","code":""},{"path":"/reference/describe_distribution.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Describe a distribution — describe_distribution","text":"x dataframe, numeric variables kept displayed summary. x list, behavior different whether x stored list. x stored (example, describe_distribution(mylist) mylist created ), artificial variable names used summary (Var_1, Var_2, etc.). x unstored list (example, describe_distribution(list(mtcars$mpg))), \"mtcars$mpg\" used variable name.","code":""},{"path":"/reference/describe_distribution.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Describe a distribution — describe_distribution","text":"also plot()-method implemented see-package.","code":""},{"path":"/reference/describe_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Describe a distribution — describe_distribution","text":"","code":"describe_distribution(rnorm(100)) #> Mean  |   SD | IQR |  Min | Max | Skewness | Kurtosis |   n | n_Missing #> ----------------------------------------------------------------------- #> -0.22 | 0.92 | 1.3 | -2.5 | 2.4 |   -0.041 |    0.072 | 100 |         0  data(iris) describe_distribution(iris) #> Variable     | Mean |   SD |  IQR | Min | Max | Skewness | Kurtosis |   n | n_Missing #> ------------------------------------------------------------------------------------- #> Sepal.Length |  5.8 | 0.83 | 1.30 | 4.3 | 7.9 |     0.31 |    -0.55 | 150 |         0 #> Sepal.Width  |  3.1 | 0.44 | 0.52 | 2.0 | 4.4 |     0.32 |     0.23 | 150 |         0 #> Petal.Length |  3.8 | 1.77 | 3.52 | 1.0 | 6.9 |    -0.27 |    -1.40 | 150 |         0 #> Petal.Width  |  1.2 | 0.76 | 1.50 | 0.1 | 2.5 |    -0.10 |    -1.34 | 150 |         0 describe_distribution(iris, include_factors = TRUE, quartiles = TRUE) #> Variable     | Mean |   SD |  IQR |    Min |       Max |  Q1 |  Q3 | Skewness | Kurtosis |   n | n_Missing #> ---------------------------------------------------------------------------------------------------------- #> Sepal.Length |  5.8 | 0.83 | 1.30 |    4.3 |       7.9 | 5.1 | 6.4 |     0.31 |    -0.55 | 150 |         0 #> Sepal.Width  |  3.1 | 0.44 | 0.52 |      2 |       4.4 | 2.8 | 3.3 |     0.32 |     0.23 | 150 |         0 #> Petal.Length |  3.8 | 1.77 | 3.52 |      1 |       6.9 | 1.6 | 5.1 |    -0.27 |    -1.40 | 150 |         0 #> Petal.Width  |  1.2 | 0.76 | 1.50 |    0.1 |       2.5 | 0.3 | 1.8 |    -0.10 |    -1.34 | 150 |         0 #> Species      |   NA |   NA |   NA | setosa | virginica |  NA |  NA |     0.00 |    -1.51 | 150 |         0 describe_distribution(list(mtcars$mpg, mtcars$cyl)) #> Variable   | Mean |  SD | IQR | Min | Max | Skewness | Kurtosis |  n | n_Missing #> -------------------------------------------------------------------------------- #> mtcars$mpg | 20.1 | 6.0 | 7.5 |  10 |  34 |     0.67 |   -0.022 | 32 |         0 #> mtcars$cyl |  6.2 | 1.8 | 4.0 |   4 |   8 |    -0.19 |   -1.763 | 32 |         0"},{"path":"/reference/efc.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample dataset from the EFC Survey — efc","title":"Sample dataset from the EFC Survey — efc","text":"Selected variables EUROFAMCARE survey. Useful testing \"real-life\" data sets, including random missing values. data set also value variable label attributes.","code":""},{"path":"/reference/find_columns.html","id":null,"dir":"Reference","previous_headings":"","what":"Find or get columns in a data frame based on search patterns — find_columns","title":"Find or get columns in a data frame based on search patterns — find_columns","text":"find_columns() returns colum names data set match certain search pattern, get_columns() returns found data.","code":""},{"path":"/reference/find_columns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find or get columns in a data frame based on search patterns — find_columns","text":"","code":"find_columns(   data,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   partial_match = FALSE,   verbose = TRUE,   ... )  get_columns(   data,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   partial_match = FALSE,   verbose = TRUE,   ... )  data_findcols(   data,   pattern = NULL,   starts_with = NULL,   ends_with = NULL,   ignore_case = FALSE,   ... )"},{"path":"/reference/find_columns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find or get columns in a data frame based on search patterns — find_columns","text":"data data frame. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names. partial_match Logical, TRUE, also find columns select exactly match column names. using contains(\"\") select-helper, however, since select-helpers may work called inside functions (see 'Details'), argument may used workaround. verbose Toggle warnings. ... Arguments passed functions. Mostly used yet. pattern regular expression (character string), representing pattern matched column names. Can also one following select-helpers: starts_with(\"\"), end_with(\"\"), regex(\"\"), contains(\"\"), range using :. starts_with, ends_with Character string, containing string matched column names. starts_with finds matches beginning column names, ends_with finds matches end column names.","code":""},{"path":"/reference/find_columns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find or get columns in a data frame based on search patterns — find_columns","text":"find_columns() returns character vector column names matched pattern select exclude, NULL matching column name found. get_columns() returns data frame matching columns.","code":""},{"path":"/reference/find_columns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find or get columns in a data frame based on search patterns — find_columns","text":"Note limitations calling inside functions. following work expected, returning columns start \"Sep\": However, example work expected! One workaround use partial_match argument, provides least bit flexibility exact matching. partial_match behaves like contains(\"\") select-helper:","code":"foo <- function(data) {   find_columns(data, select = starts_with(\"Sep\")) } foo(iris) foo <- function(data) {   i <- \"Sep\"   find_columns(data, select = starts_with(i)) } foo(iris) foo <- function(data) {   i <- \"Sep\"   find_columns(data, select = i, partial_match = TRUE) } foo(iris)"},{"path":"/reference/find_columns.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Find or get columns in a data frame based on search patterns — find_columns","text":"data_findcols() deprecated fully replaced find_columns() future update.","code":""},{"path":[]},{"path":"/reference/find_columns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find or get columns in a data frame based on search patterns — find_columns","text":"","code":"# Find columns names by pattern find_columns(iris, starts_with(\"Sepal\")) #> [1] \"Sepal.Length\" \"Sepal.Width\"  find_columns(iris, ends_with(\"Width\")) #> [1] \"Sepal.Width\" \"Petal.Width\" find_columns(iris, regex(\"\\\\.\")) #> [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  find_columns(iris, c(\"Petal.Width\", \"Sepal.Length\")) #> [1] \"Petal.Width\"  \"Sepal.Length\"  # starts with \"Sepal\", but not allowed to end with \"width\" find_columns(iris, starts_with(\"Sepal\"), exclude = contains(\"Width\")) #> [1] \"Sepal.Length\""},{"path":"/reference/format_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Convenient text formatting functionalities — format_text","title":"Convenient text formatting functionalities — format_text","text":"Convenience functions manipulate format text.","code":""},{"path":"/reference/format_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convenient text formatting functionalities — format_text","text":"","code":"format_text(text, sep = \", \", last = \" and \", width = NULL, ...)  text_fullstop(text)  text_lastchar(text, n = 1)  text_concatenate(text, sep = \", \", last = \" and \")  text_paste(text, text2 = NULL, sep = \", \", ...)  text_remove(text, pattern = \"\", ...)  text_wrap(text, width = NULL, ...)"},{"path":"/reference/format_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convenient text formatting functionalities — format_text","text":"text, text2 character string. sep Separator. last Last separator. width Positive integer giving target column width wrapping lines output. Can \"auto\", case select 90\\ default width. ... arguments passed functions. n number characters find. pattern Character vector. data_rename(), indicates columns selected renaming. Can NULL (case columns selected). data_addprefix() data_addsuffix(), character string, added prefix suffix column names.","code":""},{"path":"/reference/format_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convenient text formatting functionalities — format_text","text":"character string.","code":""},{"path":"/reference/format_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convenient text formatting functionalities — format_text","text":"","code":"# Add full stop if missing text_fullstop(c(\"something\", \"something else.\")) #> [1] \"something.\"      \"something else.\"  # Find last characters text_lastchar(c(\"ABC\", \"DEF\"), n = 2) #>  ABC  DEF  #> \"BC\" \"EF\"   # Smart concatenation text_concatenate(c(\"First\", \"Second\", \"Last\")) #> [1] \"First, Second and Last\"  # Remove parts of string text_remove(c(\"one!\", \"two\", \"three!\"), \"!\") #> [1] \"one\"   \"two\"   \"three\"  # Wrap text long_text <- paste(rep(\"abc \", 100), collapse = \"\") cat(text_wrap(long_text, width = 50)) #>  abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc abc abc abc abc abc abc abc abc #> abc abc abc abc  # Paste with optional separator text_paste(c(\"A\", \"\", \"B\"), c(\"42\", \"42\", \"42\")) #> [1] \"A, 42\" \"42\"    \"B, 42\""},{"path":"/reference/nhanes_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample dataset from the National Health and Nutrition Examination Survey — nhanes_sample","title":"Sample dataset from the National Health and Nutrition Examination Survey — nhanes_sample","text":"Selected variables National Health Nutrition Examination Survey used example Lumley (2010), Appendix E.","code":""},{"path":"/reference/nhanes_sample.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sample dataset from the National Health and Nutrition Examination Survey — nhanes_sample","text":"Lumley T (2010). Complex Surveys: guide analysis using R. Wiley","code":""},{"path":"/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize numeric variable to 0-1 range — normalize","title":"Normalize numeric variable to 0-1 range — normalize","text":"Performs normalization data, .e., scales variables range 0 - special case data_rescale().","code":""},{"path":"/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize numeric variable to 0-1 range — normalize","text":"","code":"normalize(x, ...)  # S3 method for numeric normalize(x, include_bounds = TRUE, verbose = TRUE, ...)  # S3 method for data.frame normalize(   x,   include_bounds = TRUE,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   verbose = TRUE,   ... )"},{"path":"/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize numeric variable to 0-1 range — normalize","text":"x numeric vector, (grouped) data frame, matrix. See 'Details'. ... Arguments passed methods. include_bounds Logical, TRUE, return value may include 0 1. FALSE, return value compressed, using Smithson Verkuilen's (2006) formula (x * (n - 1) + 0.5) / n, avoid zeros ones normalized variables. can useful case beta-regression, response variable allowed include zeros ones. verbose Toggle warnings messages . select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize numeric variable to 0-1 range — normalize","text":"normalized object.","code":""},{"path":"/reference/normalize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Normalize numeric variable to 0-1 range — normalize","text":"x matrix, normalization performed across values (column- row-wise). column-wise normalization, convert matrix data.frame. x grouped data frame (grouped_df), normalization performed separately group.","code":""},{"path":"/reference/normalize.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Normalize numeric variable to 0-1 range — normalize","text":"Smithson M, Verkuilen J (2006). Better Lemon Squeezer? Maximum-Likelihood Regression Beta-Distributed Dependent Variables. Psychological Methods, 11(1), 54–71.","code":""},{"path":[]},{"path":"/reference/normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize numeric variable to 0-1 range — normalize","text":"","code":"normalize(c(0, 1, 5, -5, -2)) #> [1] 0.5 0.6 1.0 0.0 0.3 normalize(c(0, 1, 5, -5, -2), include_bounds = FALSE) #> [1] 0.50 0.58 0.90 0.10 0.34  head(normalize(trees)) #>        Girth     Height      Volume #> 1 0.00000000 0.29166667 0.001497006 #> 2 0.02439024 0.08333333 0.001497006 #> 3 0.04065041 0.00000000 0.000000000 #> 4 0.17886179 0.37500000 0.092814371 #> 5 0.19512195 0.75000000 0.128742515 #> 6 0.20325203 0.83333333 0.142215569"},{"path":"/reference/ranktransform.html","id":null,"dir":"Reference","previous_headings":"","what":"(Signed) rank transformation — ranktransform","title":"(Signed) rank transformation — ranktransform","text":"Transform numeric values integers rank (.e., 1st smallest, 2nd smallest, 3rd smallest, etc.). Setting sign argument TRUE give signed ranks, ranking done according absolute size sign preserved (.e., 2, 1, -3, 4).","code":""},{"path":"/reference/ranktransform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Signed) rank transformation — ranktransform","text":"","code":"ranktransform(x, ...)  # S3 method for numeric ranktransform(x, sign = FALSE, method = \"average\", verbose = TRUE, ...)  # S3 method for data.frame ranktransform(   x,   sign = FALSE,   method = \"average\",   select = NULL,   exclude = NULL,   ignore_case = FALSE,   ... )"},{"path":"/reference/ranktransform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Signed) rank transformation — ranktransform","text":"x Object. ... Arguments passed methods. sign Logical, TRUE, return signed ranks. method Treatment ties. Can one \"average\" (default), \"first\", \"last\", \"random\", \"max\" \"min\". See rank() details. verbose Toggle warnings. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/ranktransform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Signed) rank transformation — ranktransform","text":"rank-transformed object.","code":""},{"path":[]},{"path":"/reference/ranktransform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"(Signed) rank transformation — ranktransform","text":"","code":"ranktransform(c(0, 1, 5, -5, -2)) #> [1] 3 4 5 1 2 ranktransform(c(0, 1, 5, -5, -2), sign = TRUE) #> Warning: Zeros detected. These cannot be sign-rank transformed. #> [1]   NA  1.0  3.5 -3.5 -2.0  head(ranktransform(trees)) #>   Girth Height Volume #> 1     1    6.0    2.5 #> 2     2    3.0    2.5 #> 3     3    1.0    1.0 #> 4     4    8.5    5.0 #> 5     5   25.5    7.0 #> 6     6   28.0    9.0"},{"path":"/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. insight compact_character, compact_list, is_empty_object, object_has_names, object_has_rownames","code":""},{"path":"/reference/remove_empty.html","id":null,"dir":"Reference","previous_headings":"","what":"Return or remove variables or observations that are completely missing — remove_empty","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"functions check rows columns data frame completely contain missing values, .e. observations variables completely missing values, either (1) returns indices; (2) removes data frame.","code":""},{"path":"/reference/remove_empty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"","code":"empty_columns(x)  empty_rows(x)  remove_empty_columns(x)  remove_empty_rows(x)  remove_empty(x)"},{"path":"/reference/remove_empty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"x data frame.","code":""},{"path":"/reference/remove_empty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"empty_columns() empty_rows(), numeric (named) vector row column indices variables completely missing values. remove_empty_columns() remove_empty_rows(), data frame \"empty\" columns rows removed, respectively. remove_empty, empty rows columns removed.","code":""},{"path":"/reference/remove_empty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return or remove variables or observations that are completely missing — remove_empty","text":"","code":"tmp <- data.frame(   a = c(1, 2, 3, NA, 5),   b = c(1, NA, 3, NA, 5),   c = c(NA, NA, NA, NA, NA),   d = c(1, NA, 3, NA, 5) )  tmp #>    a  b  c  d #> 1  1  1 NA  1 #> 2  2 NA NA NA #> 3  3  3 NA  3 #> 4 NA NA NA NA #> 5  5  5 NA  5  # indices of empty columns or rows empty_columns(tmp) #> c  #> 3  empty_rows(tmp) #> [1] 4  # remove empty columns or rows remove_empty_columns(tmp) #>    a  b  d #> 1  1  1  1 #> 2  2 NA NA #> 3  3  3  3 #> 4 NA NA NA #> 5  5  5  5 remove_empty_rows(tmp) #>   a  b  c  d #> 1 1  1 NA  1 #> 2 2 NA NA NA #> 3 3  3 NA  3 #> 5 5  5 NA  5  # remove empty columns and rows remove_empty(tmp) #>   a  b  d #> 1 1  1  1 #> 2 2 NA NA #> 3 3  3  3 #> 5 5  5  5"},{"path":"/reference/replace_nan_inf.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert infinite or NaN values into NA — replace_nan_inf","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"Replaces infinite (Inf -Inf) NaN values NA.","code":""},{"path":"/reference/replace_nan_inf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"","code":"replace_nan_inf(data)"},{"path":"/reference/replace_nan_inf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"data vector data frame.","code":""},{"path":"/reference/replace_nan_inf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"Data Inf, -Inf, NaN converted NA.","code":""},{"path":"/reference/replace_nan_inf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert infinite or NaN values into NA — replace_nan_inf","text":"","code":"# a vector x <- c(1, 2, NA, 3, NaN, 4, NA, 5, Inf, -Inf, 6, 7) replace_nan_inf(x) #>  [1]  1  2 NA  3 NA  4 NA  5 NA NA  6  7  # a dataframe df <- data.frame(   x = c(1, NA, 5, Inf, 2, NA),   y = c(3, NaN, 4, -Inf, 6, 7),   stringsAsFactors = FALSE ) replace_nan_inf(df) #>    x  y #> 1  1  3 #> 2 NA NA #> 3  5  4 #> 4 NA NA #> 5  2  6 #> 6 NA  7"},{"path":"/reference/rescale_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescale design weights for multilevel analysis — rescale_weights","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"functions fit multilevel mixed effects models allow specify frequency weights, design (.e. sampling probability) weights, used analyzing complex samples survey data. rescale_weights() implements algorithm proposed Asparouhov (2006) Carle (2009) rescale design weights survey data account grouping structure multilevel models, can used multilevel modelling.","code":""},{"path":"/reference/rescale_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"","code":"rescale_weights(data, group, probability_weights, nest = FALSE)"},{"path":"/reference/rescale_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"data data frame. group Variable names (character vector, formula), indicating grouping structure (strata) survey data (level-2-cluster variable). also possible create weights multiple group variables; cases, created weighting variable suffixed name group variable. probability_weights Variable indicating probability (design sampling) weights survey data (level-1-weight). nest Logical, TRUE group indicates least two group variables, groups \"nested\", .e. groups now combination group level variables group.","code":""},{"path":"/reference/rescale_weights.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"data, including new weighting variables: pweights_aand pweights_b, represent rescaled design weights use multilevel models (use variables weights argument).","code":""},{"path":"/reference/rescale_weights.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"Rescaling based two methods: pweights_a, sample weights probability_weights adjusted factor represents proportion group size divided sum sampling weights within group. adjustment factor pweights_b sum sample weights within group divided sum squared sample weights within group (see Carle (2009), Appendix B). words, pweights_a \"scales weights new weights sum cluster sample size\" pweights_b \"scales weights new weights sum effective cluster size\". Regarding choice scaling methods B, Carle suggests \"analysts wish discuss point estimates report results based weighting method . analysts interested residual -group variance, method B may generally provide least biased estimates\". general, recommended fit non-weighted model weighted models scaling methods comparing models, see whether \"inferential decisions converge\", gain confidence results. Though bias scaled weights decreases increasing group size, method preferred insufficient low group size concern. group ID probably PSU may used random effects (e.g. nested design, group PSU varying intercepts), depending survey design mimicked.","code":""},{"path":"/reference/rescale_weights.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"Carle .C. (2009). Fitting multilevel models complex survey data design weights: Recommendations. BMC Medical Research Methodology 9(49): 1-13 Asparouhov T. (2006). General Multi-Level Modeling Sampling Weights. Communications Statistics - Theory Methods 35: 439-460","code":""},{"path":"/reference/rescale_weights.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rescale design weights for multilevel analysis — rescale_weights","text":"","code":"if (require(\"lme4\")) {   data(nhanes_sample)   head(rescale_weights(nhanes_sample, \"SDMVSTRA\", \"WTINT2YR\"))    # also works with multiple group-variables   head(rescale_weights(nhanes_sample, c(\"SDMVSTRA\", \"SDMVPSU\"), \"WTINT2YR\"))    # or nested structures.   x <- rescale_weights(     data = nhanes_sample,     group = c(\"SDMVSTRA\", \"SDMVPSU\"),     probability_weights = \"WTINT2YR\",     nest = TRUE   )   head(x)    nhanes_sample <- rescale_weights(nhanes_sample, \"SDMVSTRA\", \"WTINT2YR\")    glmer(     total ~ factor(RIAGENDR) * (log(age) + factor(RIDRETH1)) + (1 | SDMVPSU),     family = poisson(),     data = nhanes_sample,     weights = pweights_a   ) } #> Loading required package: lme4 #> Loading required package: Matrix #> Generalized linear mixed model fit by maximum likelihood (Laplace #>   Approximation) [glmerMod] #>  Family: poisson  ( log ) #> Formula: total ~ factor(RIAGENDR) * (log(age) + factor(RIDRETH1)) + (1 |   #>     SDMVPSU) #>    Data: nhanes_sample #> Weights: pweights_a #>       AIC       BIC    logLik  deviance  df.resid  #>  78844.27  78920.47 -39409.14  78818.27      2582  #> Random effects: #>  Groups  Name        Std.Dev. #>  SDMVPSU (Intercept) 0.1018   #> Number of obs: 2595, groups:  SDMVPSU, 2 #> Fixed Effects: #>                         (Intercept)                    factor(RIAGENDR)2   #>                            2.491801                            -1.021308   #>                            log(age)                    factor(RIDRETH1)2   #>                            0.838726                            -0.088627   #>                   factor(RIDRETH1)3                    factor(RIDRETH1)4   #>                           -0.013333                             0.722511   #>                   factor(RIDRETH1)5           factor(RIAGENDR)2:log(age)   #>                           -0.106521                            -1.012695   #> factor(RIAGENDR)2:factor(RIDRETH1)2  factor(RIAGENDR)2:factor(RIDRETH1)3   #>                           -0.009086                             0.732985   #> factor(RIAGENDR)2:factor(RIDRETH1)4  factor(RIAGENDR)2:factor(RIDRETH1)5   #>                            0.275967                             0.542074"},{"path":"/reference/reshape_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape CI between wide/long formats — reshape_ci","title":"Reshape CI between wide/long formats — reshape_ci","text":"Reshape CI wide/long formats.","code":""},{"path":"/reference/reshape_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape CI between wide/long formats — reshape_ci","text":"","code":"reshape_ci(x, ci_type = \"CI\")"},{"path":"/reference/reshape_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape CI between wide/long formats — reshape_ci","text":"x data frame containing columns named CI_low CI_high (similar, see ci_type). ci_type String indicating \"type\" (.e. prefix) interval columns. Per easystats convention, confidence credible intervals named CI_low CI_high, related ci_type \"CI\". column names intervals differ, ci_type can used indicate name, e.g. ci_type = \"SI\" can used support intervals, column names data frame SI_low SI_high.","code":""},{"path":"/reference/reshape_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape CI between wide/long formats — reshape_ci","text":"dataframe columns corresponding confidence intervals reshaped either wide long format.","code":""},{"path":"/reference/reshape_ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reshape CI between wide/long formats — reshape_ci","text":"","code":"x <- data.frame(   Parameter = c(\"Term 1\", \"Term 2\", \"Term 1\", \"Term 2\"),   CI = c(.8, .8, .9, .9),   CI_low = c(.2, .3, .1, .15),   CI_high = c(.5, .6, .8, .85),   stringsAsFactors = FALSE )  reshape_ci(x) #>   Parameter CI_low_0.8 CI_high_0.8 CI_low_0.9 CI_high_0.9 #> 1    Term 1        0.2         0.5       0.10        0.80 #> 2    Term 2        0.3         0.6       0.15        0.85 reshape_ci(reshape_ci(x)) #>   Parameter  CI CI_low CI_high #> 1    Term 1 0.8   0.20    0.50 #> 2    Term 1 0.9   0.10    0.80 #> 3    Term 2 0.8   0.30    0.60 #> 4    Term 2 0.9   0.15    0.85"},{"path":"/reference/rownames.html","id":null,"dir":"Reference","previous_headings":"","what":"Tools for working with row names — rownames_as_column","title":"Tools for working with row names — rownames_as_column","text":"Tools working row names","code":""},{"path":"/reference/rownames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tools for working with row names — rownames_as_column","text":"","code":"rownames_as_column(x, var = \"rowname\")  column_as_rownames(x, var = \"rowname\")"},{"path":"/reference/rownames.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tools for working with row names — rownames_as_column","text":"x data frame. var Name column use rownames. column_as_rownames(), argument can variable name column number.","code":""},{"path":"/reference/rownames.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tools for working with row names — rownames_as_column","text":"rownames_as_column() column_as_rownames() return data frame.","code":""},{"path":"/reference/rownames.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tools for working with row names — rownames_as_column","text":"","code":"# Convert between row names and column -------------------------------- test <- rownames_as_column(mtcars, var = \"car\") test #>                    car  mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> 1            Mazda RX4 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> 2        Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> 3           Datsun 710 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> 4       Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> 5    Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> 6              Valiant 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> 7           Duster 360 14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4 #> 8            Merc 240D 24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 #> 9             Merc 230 22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2 #> 10            Merc 280 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4 #> 11           Merc 280C 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4 #> 12          Merc 450SE 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3 #> 13          Merc 450SL 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3 #> 14         Merc 450SLC 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3 #> 15  Cadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4 #> 16 Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4 #> 17   Chrysler Imperial 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4 #> 18            Fiat 128 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 #> 19         Honda Civic 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 #> 20      Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 #> 21       Toyota Corona 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1 #> 22    Dodge Challenger 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 #> 23         AMC Javelin 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2 #> 24          Camaro Z28 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4 #> 25    Pontiac Firebird 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2 #> 26           Fiat X1-9 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 #> 27       Porsche 914-2 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> 28        Lotus Europa 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 #> 29      Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 #> 30        Ferrari Dino 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 #> 31       Maserati Bora 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8 #> 32          Volvo 142E 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2 head(column_as_rownames(test, var = \"car\")) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"},{"path":"/reference/skewness.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Skewness and (Excess) Kurtosis — skewness","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"Compute Skewness (Excess) Kurtosis","code":""},{"path":"/reference/skewness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"","code":"skewness(x, na.rm = TRUE, type = \"2\", iterations = NULL, verbose = TRUE, ...)  kurtosis(x, na.rm = TRUE, type = \"2\", iterations = NULL, verbose = TRUE, ...)  # S3 method for parameters_kurtosis print(x, digits = 3, test = FALSE, ...)  # S3 method for parameters_skewness print(x, digits = 3, test = FALSE, ...)  # S3 method for parameters_skewness summary(object, test = FALSE, ...)  # S3 method for parameters_kurtosis summary(object, test = FALSE, ...)"},{"path":"/reference/skewness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"x numeric vector data.frame. na.rm Remove missing values. type Type algorithm computing skewness. May one 1 (\"1\", \"\" \"classic\"), 2 (\"2\", \"II\" \"SPSS\" \"SAS\") 3 ( \"3\", \"III\" \"Minitab\"). See 'Details'. iterations number bootstrap replicates computing standard errors. NULL (default), parametric standard errors computed. verbose Toggle warnings messages. ... Arguments passed methods. digits Number decimal places. test Logical, TRUE, tests skewness kurtosis significantly different zero. object object returned skewness() kurtosis().","code":""},{"path":"/reference/skewness.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"Values skewness kurtosis.","code":""},{"path":[]},{"path":"/reference/skewness.html","id":"skewness","dir":"Reference","previous_headings":"","what":"Skewness","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"Symmetric distributions skewness around zero, negative skewness values indicates \"left-skewed\" distribution, positive skewness values indicates \"right-skewed\" distribution. Examples relationship skewness distributions : Normal distribution (symmetric distribution) skewness 0 Half-normal distribution skewness just 1 Exponential distribution skewness 2 Lognormal distribution can skewness positive value, depending parameters (https://en.wikipedia.org/wiki/Skewness)","code":""},{"path":"/reference/skewness.html","id":"types-of-skewness","dir":"Reference","previous_headings":"","what":"Types of Skewness","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"skewness() supports three different methods estimating skewness, discussed Joanes Gill (1988): Type \"1\" \"classical\" method, g1 = (sum((x - mean(x))^3) / n) / (sum((x - mean(x))^2) / n)^1.5 Type \"2\" first calculates type-1 skewness, adjusts result: G1 = g1 * sqrt(n * (n - 1)) / (n - 2). SAS SPSS usually return Type \"3\" first calculates type-1 skewness, adjusts result: b1 = g1 * ((1 - 1 / n))^1.5. Minitab usually returns.","code":""},{"path":"/reference/skewness.html","id":"kurtosis","dir":"Reference","previous_headings":"","what":"Kurtosis","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"kurtosis measure \"tailedness\" distribution. distribution kurtosis values zero called \"mesokurtic\". kurtosis value larger zero indicates \"leptokurtic\" distribution fatter tails. kurtosis value zero indicates \"platykurtic\" distribution thinner tails (https://en.wikipedia.org/wiki/Kurtosis).","code":""},{"path":"/reference/skewness.html","id":"types-of-kurtosis","dir":"Reference","previous_headings":"","what":"Types of Kurtosis","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"kurtosis() supports three different methods estimating kurtosis, discussed Joanes Gill (1988): Type \"1\" \"classical\" method, g2 = n * sum((x - mean(x))^4) / (sum((x - mean(x))^2)^2) - 3. Type \"2\" first calculates type-1 kurtosis, adjusts result: G2 = ((n + 1) * g2 + 6) * (n - 1)/((n - 2) * (n - 3)). SAS SPSS usually return Type \"3\" first calculates type-1 kurtosis, adjusts result: b2 = (g2 + 3) * (1 - 1 / n)^2 - 3. Minitab usually returns.","code":""},{"path":"/reference/skewness.html","id":"standard-errors","dir":"Reference","previous_headings":"","what":"Standard Errors","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"recommended compute empirical (bootstrapped) standard errors (via iterations argument) relying analytic standard errors (Wright & Herrington, 2011).","code":""},{"path":"/reference/skewness.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"D. N. Joanes C. . Gill (1998). Comparing measures sample skewness kurtosis. Statistician, 47, 183–189. Wright, D. B., & Herrington, J. . (2011). Problematic standard errors confidence intervals skewness kurtosis. Behavior research methods, 43(1), 8-17.","code":""},{"path":"/reference/skewness.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Skewness and (Excess) Kurtosis — skewness","text":"","code":"skewness(rnorm(1000)) #> Skewness |    SE #> ---------------- #>    0.029 | 0.077 kurtosis(rnorm(1000)) #> Kurtosis |    SE #> ---------------- #>    0.163 | 0.154"},{"path":"/reference/smoothness.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantify the smoothness of a vector — smoothness","title":"Quantify the smoothness of a vector — smoothness","text":"Quantify smoothness vector","code":""},{"path":"/reference/smoothness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantify the smoothness of a vector — smoothness","text":"","code":"smoothness(x, method = \"cor\", lag = 1, iterations = NULL, ...)"},{"path":"/reference/smoothness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantify the smoothness of a vector — smoothness","text":"x Numeric vector (similar time series). method Can \"diff\" (standard deviation standardized differences) \"cor\" (default, lag-one autocorrelation). lag integer indicating lag use. less 1, interpreted expressed percentage length vector. iterations number bootstrap replicates computing standard errors. NULL (default), parametric standard errors computed. ... Arguments passed methods.","code":""},{"path":"/reference/smoothness.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quantify the smoothness of a vector — smoothness","text":"Value smoothness.","code":""},{"path":"/reference/smoothness.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Quantify the smoothness of a vector — smoothness","text":"https://stats.stackexchange.com/questions/24607/--measure-smoothness---time-series--r","code":""},{"path":"/reference/smoothness.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantify the smoothness of a vector — smoothness","text":"","code":"x <- (-10:10)^3 + rnorm(21, 0, 100) plot(x)  smoothness(x, method = \"cor\") #> [1] 0.9203155 #> attr(,\"class\") #> [1] \"parameters_smoothness\" \"numeric\"               smoothness(x, method = \"diff\") #> [1] 1.708527 #> attr(,\"class\") #> [1] \"parameters_smoothness\" \"numeric\""},{"path":"/reference/standardize.html","id":null,"dir":"Reference","previous_headings":"","what":"Standardization (Z-scoring) — standardize","title":"Standardization (Z-scoring) — standardize","text":"Performs standardization data (z-scoring), .e., centering scaling, data expressed terms standard deviation (.e., mean = 0, SD = 1) Median Absolute Deviance (median = 0, MAD = 1). applied statistical model, function extracts dataset, standardizes , refits model standardized version dataset. normalize() function can also used scale numeric variables within 0 - 1 range.  model standardization, see effectsize::standardize.default()","code":""},{"path":"/reference/standardize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standardization (Z-scoring) — standardize","text":"","code":"standardize(x, ...)  standardise(x, ...)  # S3 method for numeric standardize(   x,   robust = FALSE,   two_sd = FALSE,   weights = NULL,   reference = NULL,   center = NULL,   scale = NULL,   verbose = TRUE,   ... )  # S3 method for factor standardize(   x,   robust = FALSE,   two_sd = FALSE,   weights = NULL,   force = FALSE,   verbose = TRUE,   ... )  # S3 method for data.frame standardize(   x,   robust = FALSE,   two_sd = FALSE,   weights = NULL,   reference = NULL,   center = NULL,   scale = NULL,   remove_na = c(\"none\", \"selected\", \"all\"),   force = FALSE,   append = FALSE,   select = NULL,   exclude = NULL,   ignore_case = FALSE,   verbose = TRUE,   ... )  unstandardize(x, ...)  unstandardise(x, ...)  # S3 method for numeric unstandardize(   x,   center = NULL,   scale = NULL,   reference = NULL,   robust = FALSE,   two_sd = FALSE,   ... )  # S3 method for data.frame unstandardize(   x,   center = NULL,   scale = NULL,   reference = NULL,   robust = FALSE,   two_sd = FALSE,   select = NULL,   exclude = NULL,   ... )"},{"path":"/reference/standardize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standardization (Z-scoring) — standardize","text":"x (grouped) data frame, vector statistical model (unstandardize() model). ... Arguments passed methods. robust Logical, TRUE, centering done subtracting median variables dividing median absolute deviation (MAD). FALSE, variables standardized subtracting mean dividing standard deviation (SD). two_sd TRUE, variables scaled two times deviation (SD MAD depending robust). method can useful obtain model coefficients continuous parameters comparable coefficients related binary predictors, applied predictors (outcome) (Gelman, 2008). weights Can NULL (weighting), : model: TRUE (default), weighted-standardization carried . data.frames: numeric vector weights, character name column data.frame contains weights. numeric vectors: numeric vector weights. reference data frame variable centrality deviation computed instead input variable. Useful standardizing subset new data according another data frame. center, scale standardize():  Numeric values, can used alternative reference define reference centrality deviation. scale center length 1, recycled match length selected variables standardization. Else, center scale must length number selected variables. Values center scale matched selected variables provided order, unless named vector given. case, names matched names selected variables. unstandardize(): center scale correspond center (mean / median) scale (SD / MAD) original non-standardized data (data frames, named, column order correspond numeric column). However, one can also directly provide original data reference, center scale computed (according robust two_sd). Alternatively, input contains attributes center scale (output standardize()), take rest arguments absent. verbose Toggle warnings messages . force Logical, TRUE, forces standardization factors dates well. Factors converted numerical values, lowest level value 1 (unless factor numeric levels, converted corresponding numeric value). remove_na missing values (NA) treated: \"none\" (default): column's standardization done separately, ignoring NAs. Else, rows NA columns selected select / exclude (\"selected\") columns (\"\") dropped standardization, resulting data frame include cases. append Logical string. TRUE, standardized variables get new column names (suffix \"_z\") appended (column bind) x, thus returning original standardized variables. FALSE, original variables x overwritten standardized versions. character value, standardized variables appended new column names (using defined suffix) original data frame. select Variables included performing required tasks. Can either variable specified literal variable name (e.g., column_name), string variable name (e.g., \"column_name\"), character vector variable names (e.g., c(\"col1\", \"col2\", \"col3\")), formula variable names (e.g., ~column_1 + column_2), vector positive integers, giving positions counting left (e.g. 1 c(1, 3, 5)), vector negative integers, giving positions counting right (e.g., -1 -1:-3), one following select-helpers: starts_with(\"\"), ends_with(\"\"), contains(\"\"), range using : regex(\"\"). NULL, selects columns. exclude See select, however, column names matched pattern exclude excluded instead selected. NULL (default), excludes columns. ignore_case Logical, TRUE one select-helpers regular expression used select, ignores lower/upper case search pattern matching variable names.","code":""},{"path":"/reference/standardize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Standardization (Z-scoring) — standardize","text":"standardized object (either standardize data frame statistical model fitted standardized data).","code":""},{"path":"/reference/standardize.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Standardization (Z-scoring) — standardize","text":"x vector data frame remove_na = \"none\"), missing values preserved, return value length / number rows original input.","code":""},{"path":[]},{"path":"/reference/standardize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Standardization (Z-scoring) — standardize","text":"","code":"d <- iris[1:4, ]  # vectors standardise(d$Petal.Length) #> [1]  0.000000  0.000000 -1.224745  1.224745 #> attr(,\"center\") #> [1] 1.4 #> attr(,\"scale\") #> [1] 0.08164966 #> attr(,\"robust\") #> [1] FALSE  # Data frames # overwrite standardise(d, select = c(\"Sepal.Length\", \"Sepal.Width\")) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1    1.2402159   1.3887301          1.4         0.2  setosa #> 2    0.3382407  -0.9258201          1.4         0.2  setosa #> 3   -0.5637345   0.0000000          1.3         0.2  setosa #> 4   -1.0147221  -0.4629100          1.5         0.2  setosa  # append standardise(d, select = c(\"Sepal.Length\", \"Sepal.Width\"), append = TRUE) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length_z #> 1          5.1         3.5          1.4         0.2  setosa      1.2402159 #> 2          4.9         3.0          1.4         0.2  setosa      0.3382407 #> 3          4.7         3.2          1.3         0.2  setosa     -0.5637345 #> 4          4.6         3.1          1.5         0.2  setosa     -1.0147221 #>   Sepal.Width_z #> 1     1.3887301 #> 2    -0.9258201 #> 3     0.0000000 #> 4    -0.4629100  # append, suffix standardise(d, select = c(\"Sepal.Length\", \"Sepal.Width\"), append = \"_std\") #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length_std #> 1          5.1         3.5          1.4         0.2  setosa        1.2402159 #> 2          4.9         3.0          1.4         0.2  setosa        0.3382407 #> 3          4.7         3.2          1.3         0.2  setosa       -0.5637345 #> 4          4.6         3.1          1.5         0.2  setosa       -1.0147221 #>   Sepal.Width_std #> 1       1.3887301 #> 2      -0.9258201 #> 3       0.0000000 #> 4      -0.4629100  # standardizing with reference center and scale d <- data.frame(   a = c(-2, -1, 0, 1, 2),   b = c(3, 4, 5, 6, 7) )  # default standardization, based on mean and sd of each variable standardize(d) # means are 0 and 5, sd ~ 1.581139 #>            a          b #> 1 -1.2649111 -1.2649111 #> 2 -0.6324555 -0.6324555 #> 3  0.0000000  0.0000000 #> 4  0.6324555  0.6324555 #> 5  1.2649111  1.2649111  # standardization, based on mean and sd set to the same values standardize(d, center = c(0, 5), scale = c(1.581, 1.581)) #>            a          b #> 1 -1.2650221 -1.2650221 #> 2 -0.6325111 -0.6325111 #> 3  0.0000000  0.0000000 #> 4  0.6325111  0.6325111 #> 5  1.2650221  1.2650221  # standardization, mean and sd for each variable newly defined standardize(d, center = c(3, 4), scale = c(2, 4)) #>      a     b #> 1 -2.5 -0.25 #> 2 -2.0  0.00 #> 3 -1.5  0.25 #> 4 -1.0  0.50 #> 5 -0.5  0.75  # standardization, taking same mean and sd for each variable standardize(d, center = 1, scale = 3) #>            a         b #> 1 -1.0000000 0.6666667 #> 2 -0.6666667 1.0000000 #> 3 -0.3333333 1.3333333 #> 4  0.0000000 1.6666667 #> 5  0.3333333 2.0000000"},{"path":"/reference/to_numeric.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to Numeric (if possible) — to_numeric","title":"Convert to Numeric (if possible) — to_numeric","text":"Tries convert vector numeric possible (warnings errors). Otherwise, leaves .","code":""},{"path":"/reference/to_numeric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to Numeric (if possible) — to_numeric","text":"","code":"to_numeric(x)"},{"path":"/reference/to_numeric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to Numeric (if possible) — to_numeric","text":"x vector converted.","code":""},{"path":"/reference/to_numeric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert to Numeric (if possible) — to_numeric","text":"Numeric vector (possible)","code":""},{"path":"/reference/to_numeric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert to Numeric (if possible) — to_numeric","text":"","code":"to_numeric(c(\"1\", \"2\")) #> [1] 1 2 to_numeric(c(\"1\", \"2\", \"A\")) #> [1] \"1\" \"2\" \"A\""},{"path":"/reference/visualisation_recipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare objects for visualisation — visualisation_recipe","title":"Prepare objects for visualisation — visualisation_recipe","text":"function prepares objects visualisation returning list layers data geoms can easily plotted using instance ggplot2. See documentation object's class:","code":""},{"path":"/reference/visualisation_recipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare objects for visualisation — visualisation_recipe","text":"","code":"visualisation_recipe(x, ...)"},{"path":"/reference/visualisation_recipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare objects for visualisation — visualisation_recipe","text":"x easystats object. ... arguments passed functions.","code":""},{"path":"/reference/visualisation_recipe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare objects for visualisation — visualisation_recipe","text":"modelbased (estimate_means, estimate_contrasts, estimate_slopes, estimate_predicted, estimate_grouplevel)","code":""},{"path":"/reference/weighted_mean.html","id":null,"dir":"Reference","previous_headings":"","what":"Weighted Mean, Median, SD, and MAD — weighted_mean","title":"Weighted Mean, Median, SD, and MAD — weighted_mean","text":"Weighted Mean, Median, SD, MAD","code":""},{"path":"/reference/weighted_mean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weighted Mean, Median, SD, and MAD — weighted_mean","text":"","code":"weighted_mean(x, weights = NULL, verbose = TRUE, ...)  weighted_median(x, weights = NULL, verbose = TRUE, ...)  weighted_sd(x, weights = NULL, verbose = TRUE, ...)  weighted_mad(x, weights = NULL, constant = 1.4826, verbose = TRUE, ...)"},{"path":"/reference/weighted_mean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weighted Mean, Median, SD, and MAD — weighted_mean","text":"x object containing values whose weighted mean     computed. weights numerical vector weights length x giving weights use elements x. verbose Show warning weights negative? weights = NULL, x passed non-weighted function. ... arguments passed methods. constant scale factor.","code":""},{"path":"/reference/weighted_mean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weighted Mean, Median, SD, and MAD — weighted_mean","text":"","code":"## GPA from Siegel 1994 x <- c(3.7, 3.3, 3.5, 2.8) wt <- c(5, 5, 4, 1) / 15  weighted_mean(x, wt) #> [1] 3.453333 weighted_median(x, wt) #> [1] 3.653333  weighted_sd(x, wt) #> [1] 0.2852935 weighted_mad(x, wt) #> [1] 0.1594619"},{"path":"/reference/winsorize.html","id":null,"dir":"Reference","previous_headings":"","what":"Winsorize data — winsorize","title":"Winsorize data — winsorize","text":"Winsorize data","code":""},{"path":"/reference/winsorize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Winsorize data — winsorize","text":"","code":"winsorize(data, ...)  # S3 method for numeric winsorize(data, threshold = 0.2, verbose = TRUE, ...)"},{"path":"/reference/winsorize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Winsorize data — winsorize","text":"data Dataframe vector. ... Currently used. threshold amount winsorization. verbose Toggle warnings.","code":""},{"path":"/reference/winsorize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Winsorize data — winsorize","text":"dataframe winsorized columns winsorized vector.","code":""},{"path":"/reference/winsorize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Winsorize data — winsorize","text":"Winsorizing winsorization transformation statistics limiting extreme values statistical data reduce effect possibly spurious outliers. distribution many statistics can heavily influenced outliers. typical strategy set outliers (values beyond certain threshold) specified percentile data; example, 90\\ 5th percentile, data 95th percentile set 95th percentile. Winsorized estimators usually robust outliers standard forms.","code":""},{"path":[]},{"path":"/reference/winsorize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Winsorize data — winsorize","text":"","code":"winsorize(iris$Sepal.Length, threshold = 0.2) #>   [1] 5.1 5.0 5.0 5.0 5.0 5.4 5.0 5.0 5.0 5.0 5.4 5.0 5.0 5.0 5.8 5.7 5.4 5.1 #>  [19] 5.7 5.1 5.4 5.1 5.0 5.1 5.0 5.0 5.0 5.2 5.2 5.0 5.0 5.4 5.2 5.5 5.0 5.0 #>  [37] 5.5 5.0 5.0 5.1 5.0 5.0 5.0 5.0 5.1 5.0 5.1 5.0 5.3 5.0 6.5 6.4 6.5 5.5 #>  [55] 6.5 5.7 6.3 5.0 6.5 5.2 5.0 5.9 6.0 6.1 5.6 6.5 5.6 5.8 6.2 5.6 5.9 6.1 #>  [73] 6.3 6.1 6.4 6.5 6.5 6.5 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.5 6.3 5.6 5.5 #>  [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 6.5 6.3 6.5 6.5 5.0 6.5 #> [109] 6.5 6.5 6.5 6.4 6.5 5.7 5.8 6.4 6.5 6.5 6.5 6.0 6.5 5.6 6.5 6.3 6.5 6.5 #> [127] 6.2 6.1 6.4 6.5 6.5 6.5 6.4 6.3 6.1 6.5 6.3 6.4 6.0 6.5 6.5 6.5 5.8 6.5 #> [145] 6.5 6.5 6.3 6.5 6.2 5.9 winsorize(iris, threshold = 0.2) #>        Sepal.Length Sepal.Width Petal.Length Petal.Width Species #>   [1,]          5.1         3.4          1.5         0.2       1 #>   [2,]          5.0         3.0          1.5         0.2       1 #>   [3,]          5.0         3.2          1.5         0.2       1 #>   [4,]          5.0         3.1          1.5         0.2       1 #>   [5,]          5.0         3.4          1.5         0.2       1 #>   [6,]          5.4         3.4          1.7         0.4       1 #>   [7,]          5.0         3.4          1.5         0.3       1 #>   [8,]          5.0         3.4          1.5         0.2       1 #>   [9,]          5.0         2.9          1.5         0.2       1 #>  [10,]          5.0         3.1          1.5         0.2       1 #>  [11,]          5.4         3.4          1.5         0.2       1 #>  [12,]          5.0         3.4          1.6         0.2       1 #>  [13,]          5.0         3.0          1.5         0.2       1 #>  [14,]          5.0         3.0          1.5         0.2       1 #>  [15,]          5.8         3.4          1.5         0.2       1 #>  [16,]          5.7         3.4          1.5         0.4       1 #>  [17,]          5.4         3.4          1.5         0.4       1 #>  [18,]          5.1         3.4          1.5         0.3       1 #>  [19,]          5.7         3.4          1.7         0.3       1 #>  [20,]          5.1         3.4          1.5         0.3       1 #>  [21,]          5.4         3.4          1.7         0.2       1 #>  [22,]          5.1         3.4          1.5         0.4       1 #>  [23,]          5.0         3.4          1.5         0.2       1 #>  [24,]          5.1         3.3          1.7         0.5       1 #>  [25,]          5.0         3.4          1.9         0.2       1 #>  [26,]          5.0         3.0          1.6         0.2       1 #>  [27,]          5.0         3.4          1.6         0.4       1 #>  [28,]          5.2         3.4          1.5         0.2       1 #>  [29,]          5.2         3.4          1.5         0.2       1 #>  [30,]          5.0         3.2          1.6         0.2       1 #>  [31,]          5.0         3.1          1.6         0.2       1 #>  [32,]          5.4         3.4          1.5         0.4       1 #>  [33,]          5.2         3.4          1.5         0.2       1 #>  [34,]          5.5         3.4          1.5         0.2       1 #>  [35,]          5.0         3.1          1.5         0.2       1 #>  [36,]          5.0         3.2          1.5         0.2       1 #>  [37,]          5.5         3.4          1.5         0.2       1 #>  [38,]          5.0         3.4          1.5         0.2       1 #>  [39,]          5.0         3.0          1.5         0.2       1 #>  [40,]          5.1         3.4          1.5         0.2       1 #>  [41,]          5.0         3.4          1.5         0.3       1 #>  [42,]          5.0         2.7          1.5         0.3       1 #>  [43,]          5.0         3.2          1.5         0.2       1 #>  [44,]          5.0         3.4          1.6         0.6       1 #>  [45,]          5.1         3.4          1.9         0.4       1 #>  [46,]          5.0         3.0          1.5         0.3       1 #>  [47,]          5.1         3.4          1.6         0.2       1 #>  [48,]          5.0         3.2          1.5         0.2       1 #>  [49,]          5.3         3.4          1.5         0.2       1 #>  [50,]          5.0         3.3          1.5         0.2       1 #>  [51,]          6.5         3.2          4.7         1.4       2 #>  [52,]          6.4         3.2          4.5         1.5       2 #>  [53,]          6.5         3.1          4.9         1.5       2 #>  [54,]          5.5         2.7          4.0         1.3       2 #>  [55,]          6.5         2.8          4.6         1.5       2 #>  [56,]          5.7         2.8          4.5         1.3       2 #>  [57,]          6.3         3.3          4.7         1.6       2 #>  [58,]          5.0         2.7          3.3         1.0       2 #>  [59,]          6.5         2.9          4.6         1.3       2 #>  [60,]          5.2         2.7          3.9         1.4       2 #>  [61,]          5.0         2.7          3.5         1.0       2 #>  [62,]          5.9         3.0          4.2         1.5       2 #>  [63,]          6.0         2.7          4.0         1.0       2 #>  [64,]          6.1         2.9          4.7         1.4       2 #>  [65,]          5.6         2.9          3.6         1.3       2 #>  [66,]          6.5         3.1          4.4         1.4       2 #>  [67,]          5.6         3.0          4.5         1.5       2 #>  [68,]          5.8         2.7          4.1         1.0       2 #>  [69,]          6.2         2.7          4.5         1.5       2 #>  [70,]          5.6         2.7          3.9         1.1       2 #>  [71,]          5.9         3.2          4.8         1.8       2 #>  [72,]          6.1         2.8          4.0         1.3       2 #>  [73,]          6.3         2.7          4.9         1.5       2 #>  [74,]          6.1         2.8          4.7         1.2       2 #>  [75,]          6.4         2.9          4.3         1.3       2 #>  [76,]          6.5         3.0          4.4         1.4       2 #>  [77,]          6.5         2.8          4.8         1.4       2 #>  [78,]          6.5         3.0          5.0         1.7       2 #>  [79,]          6.0         2.9          4.5         1.5       2 #>  [80,]          5.7         2.7          3.5         1.0       2 #>  [81,]          5.5         2.7          3.8         1.1       2 #>  [82,]          5.5         2.7          3.7         1.0       2 #>  [83,]          5.8         2.7          3.9         1.2       2 #>  [84,]          6.0         2.7          5.1         1.6       2 #>  [85,]          5.4         3.0          4.5         1.5       2 #>  [86,]          6.0         3.4          4.5         1.6       2 #>  [87,]          6.5         3.1          4.7         1.5       2 #>  [88,]          6.3         2.7          4.4         1.3       2 #>  [89,]          5.6         3.0          4.1         1.3       2 #>  [90,]          5.5         2.7          4.0         1.3       2 #>  [91,]          5.5         2.7          4.4         1.2       2 #>  [92,]          6.1         3.0          4.6         1.4       2 #>  [93,]          5.8         2.7          4.0         1.2       2 #>  [94,]          5.0         2.7          3.3         1.0       2 #>  [95,]          5.6         2.7          4.2         1.3       2 #>  [96,]          5.7         3.0          4.2         1.2       2 #>  [97,]          5.7         2.9          4.2         1.3       2 #>  [98,]          6.2         2.9          4.3         1.3       2 #>  [99,]          5.1         2.7          3.0         1.1       2 #> [100,]          5.7         2.8          4.1         1.3       2 #> [101,]          6.3         3.3          5.3         1.9       3 #> [102,]          5.8         2.7          5.1         1.9       3 #> [103,]          6.5         3.0          5.3         1.9       3 #> [104,]          6.3         2.9          5.3         1.8       3 #> [105,]          6.5         3.0          5.3         1.9       3 #> [106,]          6.5         3.0          5.3         1.9       3 #> [107,]          5.0         2.7          4.5         1.7       3 #> [108,]          6.5         2.9          5.3         1.8       3 #> [109,]          6.5         2.7          5.3         1.8       3 #> [110,]          6.5         3.4          5.3         1.9       3 #> [111,]          6.5         3.2          5.1         1.9       3 #> [112,]          6.4         2.7          5.3         1.9       3 #> [113,]          6.5         3.0          5.3         1.9       3 #> [114,]          5.7         2.7          5.0         1.9       3 #> [115,]          5.8         2.8          5.1         1.9       3 #> [116,]          6.4         3.2          5.3         1.9       3 #> [117,]          6.5         3.0          5.3         1.8       3 #> [118,]          6.5         3.4          5.3         1.9       3 #> [119,]          6.5         2.7          5.3         1.9       3 #> [120,]          6.0         2.7          5.0         1.5       3 #> [121,]          6.5         3.2          5.3         1.9       3 #> [122,]          5.6         2.8          4.9         1.9       3 #> [123,]          6.5         2.8          5.3         1.9       3 #> [124,]          6.3         2.7          4.9         1.8       3 #> [125,]          6.5         3.3          5.3         1.9       3 #> [126,]          6.5         3.2          5.3         1.8       3 #> [127,]          6.2         2.8          4.8         1.8       3 #> [128,]          6.1         3.0          4.9         1.8       3 #> [129,]          6.4         2.8          5.3         1.9       3 #> [130,]          6.5         3.0          5.3         1.6       3 #> [131,]          6.5         2.8          5.3         1.9       3 #> [132,]          6.5         3.4          5.3         1.9       3 #> [133,]          6.4         2.8          5.3         1.9       3 #> [134,]          6.3         2.8          5.1         1.5       3 #> [135,]          6.1         2.7          5.3         1.4       3 #> [136,]          6.5         3.0          5.3         1.9       3 #> [137,]          6.3         3.4          5.3         1.9       3 #> [138,]          6.4         3.1          5.3         1.8       3 #> [139,]          6.0         3.0          4.8         1.8       3 #> [140,]          6.5         3.1          5.3         1.9       3 #> [141,]          6.5         3.1          5.3         1.9       3 #> [142,]          6.5         3.1          5.1         1.9       3 #> [143,]          5.8         2.7          5.1         1.9       3 #> [144,]          6.5         3.2          5.3         1.9       3 #> [145,]          6.5         3.3          5.3         1.9       3 #> [146,]          6.5         3.0          5.2         1.9       3 #> [147,]          6.3         2.7          5.0         1.9       3 #> [148,]          6.5         3.0          5.2         1.9       3 #> [149,]          6.2         3.4          5.3         1.9       3 #> [150,]          5.9         3.0          5.1         1.8       3"},{"path":"/news/index.html","id":"datawizard-0309000","dir":"Changelog","previous_headings":"","what":"datawizard 0.3.0.9000","title":"datawizard 0.3.0.9000","text":"MAJOR CHANGES data_match() now returns filtered data default. Old behavior (returning rows indices) can set setting return_indices = TRUE. following functions now re-exported insight package: object_has_names(), object_has_rownames(), is_empty_object(), compact_list(), compact_character() data_findcols() become deprecated future updates. Please use new replacements find_columns() get_columns(). vignette Analysing Longitudinal Panel Data now moved parameters package. NEW FUNCTIONS convert rownames column, vice versa: rownames_as_column() column_as_rownames() (@etiennebacher, #80). find_columns() get_columns() find column names retrieve subsets data frames, based various select-methods (including select-helpers). function supersede data_findcols() future. computing weighted centrality measures dispersion: weighted_mean(), weighted_median(), weighted_sd() weighted_mad(). replace NA vectors dataframes: convert_na_to() (@etiennebacher, #111). MINOR CHANGES select argument several functions (like data_remove(), reshape_longer(), data_extract()) now allow use select-helpers selecting variables based specific patterns. data_extract() gains new arguments allow type-safe return values, .e. always return vector data frame. Thus, data_extract() can now used select multiple variables pull single variable data frames. data_match() gains match argument, indicate logical operation matching results combined. Improved support labelled data many functions, .e. returned data frame preserve value variable label attributes, possible applicable. describe_distribution() now works lists (@etiennebacher, #105). data_rename() doesn’t use pattern anymore rename columns replacement provided (@etiennebacher, #103). data_rename() now adds suffix duplicated names replacement (@etiennebacher, #103). BUG FIXES data_to_numeric() produced wrong results factors dummy_factors = TRUE factor contained missing values. data_match() produced wrong results data contained missing values. Fixed CRAN check issues data_extract() one variable extracted data frame.","code":""},{"path":"/news/index.html","id":"datawizard-030","dir":"Changelog","previous_headings":"","what":"datawizard 0.3.0","title":"datawizard 0.3.0","text":"CRAN release: 2022-03-02 NEW FUNCTIONS find remove empty rows columns data frame: empty_rows(), empty_columns(), remove_empty_rows(), remove_empty_columns(), remove_empty. check names: object_has_names() object_has_rownames(). rotate data frames: data_rotate(). reverse score variables: data_reverse(). merge/join multiple data frames: data_merge() (alias data_join()). cut (recode) data groups: data_cut(). replace specific values NAs: convert_to_na(). replace Inf NaN values NAs: replace_nan_inf(). Arguments cols, data_relocate() can now also numeric values, indicating position destination column.","code":""},{"path":"/news/index.html","id":"datawizard-023","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.3","title":"datawizard 0.2.3","text":"CRAN release: 2022-01-26 New functions: work lists: is_empty_object() compact_list() work strings: compact_character()","code":""},{"path":"/news/index.html","id":"datawizard-022","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.2","title":"datawizard 0.2.2","text":"CRAN release: 2022-01-04 New function data_extract() (alias extract()) pull single variables data frame, possibly naming value row names data frame. reshape_ci() gains ci_type argument, reshape data frames CI-columns prefixes \"CI\". standardize() center() gain arguments center scale, define references centrality deviation used centering standardizing variables. center() gains arguments force reference, similar standardize(). functionality append argument center() standardize() revised. made suffix argument redundant, thus removed. Fixed issue standardize(). Fixed issue data_findcols().","code":""},{"path":"/news/index.html","id":"datawizard-021","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.1","title":"datawizard 0.2.1","text":"CRAN release: 2021-10-04 Exports plot method visualisation_recipe() objects see package. centre(), standardise(), unstandardise() exported aliases center(), standardize(), unstandardize(), respectively.","code":""},{"path":"/news/index.html","id":"datawizard-0201","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.0.1","title":"datawizard 0.2.0.1","text":"CRAN release: 2021-09-02 mainly maintenance release addresses issues conflicting namespaces.","code":""},{"path":"/news/index.html","id":"datawizard-020","dir":"Changelog","previous_headings":"","what":"datawizard 0.2.0","title":"datawizard 0.2.0","text":"CRAN release: 2021-08-17 New function: visualisation_recipe(). following function now moved performance package: check_multimodal(). Minor updates documentation, including new vignette demean().","code":""},{"path":"/news/index.html","id":"datawizard-010","dir":"Changelog","previous_headings":"","what":"datawizard 0.1.0","title":"datawizard 0.1.0","text":"CRAN release: 2021-06-18 First release.","code":""}]
